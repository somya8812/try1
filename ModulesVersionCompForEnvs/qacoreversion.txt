root@as-dl380-r10-12:~/cluster# kubectl describe pods -n np2-qa
Name:         np2-qa-helm-af-78b8598d95-2jnzm
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-af
              pod-template-hash=78b8598d95
Annotations:  cni.projectcalico.org/containerID: 656b065e9134d3cc88ed55c29bcbaacd3f9c0ae84ab173d98f737205fdf831e9
              cni.projectcalico.org/podIP: 192.168.163.146/32
              cni.projectcalico.org/podIPs: 192.168.163.146/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.146
IPs:
  IP:           192.168.163.146
Controlled By:  ReplicaSet/np2-qa-helm-af-78b8598d95
Containers:
  af-chart:
    Container ID:  containerd://62ec899d72a7152a1c38e64aed53d03b5cd411a292fc92cfa3ebf56620f89b7f
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/af:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/af@sha256:8d165883b9f546265580b29204ceb188f5b0fb158b5fdbfb3c81bb059afbc63e
    Port:          8035/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_af.sh; echo af TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:00 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from af-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tc7qh (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      af-configmap
    Optional:  false
  af-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  af-pv-claim
    ReadOnly:   false
  kube-api-access-tc7qh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-amf-998bcfcb4-cn8gj
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 11 Oct 2023 08:00:01 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-amf
              pod-template-hash=998bcfcb4
Annotations:  cni.projectcalico.org/containerID: 2f20e17046bb2baeb88b9578c0eb2ec4c6d461f27d4177ebafe1fa090365a2bf
              cni.projectcalico.org/podIP: 192.168.163.185/32
              cni.projectcalico.org/podIPs: 192.168.163.185/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.185
IPs:
  IP:           192.168.163.185
Controlled By:  ReplicaSet/np2-qa-helm-amf-998bcfcb4
Containers:
  amf-chart:
    Container ID:  containerd://3038fe42fa9348f8a69f06814f45daf4340c8badf068883ebb3573ec550207b8
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/amf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/amf@sha256:95512263d4ac18f3eef0340f1ca9e8216a3c85950a6457e2767aa4b2160e8041
    Port:          29518/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_amf.sh; echo amf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 11 Oct 2023 08:00:02 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/log/ from amf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-529bl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      amf-configmap
    Optional:  false
  amf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  amf-pv-claim
    ReadOnly:   false
  kube-api-access-529bl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-ausf-65c4c5588-886hn
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-ausf
              pod-template-hash=65c4c5588
Annotations:  cni.projectcalico.org/containerID: 75a3c3d4a89c5ab510bc25dcb2dcdef074e3c2eb90c30294595bb3845d41354f
              cni.projectcalico.org/podIP: 192.168.163.159/32
              cni.projectcalico.org/podIPs: 192.168.163.159/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.159
IPs:
  IP:           192.168.163.159
Controlled By:  ReplicaSet/np2-qa-helm-ausf-65c4c5588
Init Containers:
  init-postgres:
    Container ID:  containerd://5ecdaf15b08d4c8331049121f8890cd68eb239aaa2a4cbd703c56fba6abc44b5
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf@sha256:5d2d61ab18a548a909ff37f8cec61da9dcdb9aeded079a0db9052e9bbb146b8d
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 06 Oct 2023 09:58:00 -0400
      Finished:     Fri, 06 Oct 2023 09:58:00 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jr4rv (ro)
Containers:
  ausf-chart:
    Container ID:  containerd://850018e11db968f7406ffea2ab3dc3eb44206d282df35cef18fd8ff5c3d936a0
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf@sha256:5d2d61ab18a548a909ff37f8cec61da9dcdb9aeded079a0db9052e9bbb146b8d
    Port:          8000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_ausf.sh; echo ausf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from ausf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jr4rv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      ausf-configmap
    Optional:  false
  ausf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  ausf-pv-claim
    ReadOnly:   false
  kube-api-access-jr4rv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-bsf-7c49c964db-srgm7
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-bsf
              pod-template-hash=7c49c964db
Annotations:  cni.projectcalico.org/containerID: 3845a32f31242d19d8c1805b605a87a5a61a371766044ec270975b77485a4d69
              cni.projectcalico.org/podIP: 192.168.163.190/32
              cni.projectcalico.org/podIPs: 192.168.163.190/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.190
IPs:
  IP:           192.168.163.190
Controlled By:  ReplicaSet/np2-qa-helm-bsf-7c49c964db
Init Containers:
  init-postgres:
    Container ID:  containerd://c67fbfbbcf7f2ddbe7af46fb0d7d1d1ff07a0fe45d968595cc4d97f11fe1f4f1
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf@sha256:0187142e3ec679fb4b97b70f17c79bf3420f3720735fecb62101d7089bf03938
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 06 Oct 2023 09:58:00 -0400
      Finished:     Fri, 06 Oct 2023 09:58:00 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-n4vd4 (ro)
Containers:
  bsf-chart:
    Container ID:  containerd://59a3bbcdad9282c1003339eb5ca139aac4fc740900b7a362ada2c0503fd6b837
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf@sha256:0187142e3ec679fb4b97b70f17c79bf3420f3720735fecb62101d7089bf03938
    Port:          11000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_bsf.sh; echo bsf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from bsf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-n4vd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      bsf-configmap
    Optional:  false
  bsf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  bsf-pv-claim
    ReadOnly:   false
  kube-api-access-n4vd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-nef-6d5df896d8-58lmq
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-nef
              pod-template-hash=6d5df896d8
Annotations:  cni.projectcalico.org/containerID: d5d2f42f52225c09a72c3aca56c7a0f72c168b9d394197cc5a41248b08b6f288
              cni.projectcalico.org/podIP: 192.168.163.147/32
              cni.projectcalico.org/podIPs: 192.168.163.147/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.147
IPs:
  IP:           192.168.163.147
Controlled By:  ReplicaSet/np2-qa-helm-nef-6d5df896d8
Containers:
  nef-chart:
    Container ID:  containerd://699ac1ea50f32aa055f7833b1490288af3d9048b7653a7ee0f0ff2a1cbc69c67
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nef:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nef@sha256:296c084777712c5c13354ad44f69fd4e85f47c9a354c79d8d431da5a8f6f099f
    Port:          10000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nef.sh; echo nef TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:00 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nef-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kgjpr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nef-configmap
    Optional:  false
  nef-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nef-pv-claim
    ReadOnly:   false
  kube-api-access-kgjpr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-nrf-7f48b47b96-zgnqx
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-nrf
              pod-template-hash=7f48b47b96
Annotations:  cni.projectcalico.org/containerID: cd792bc8c0d85b5476637043036f9e9a853ee7213f9be10bd46b4f78310046f2
              cni.projectcalico.org/podIP: 192.168.163.141/32
              cni.projectcalico.org/podIPs: 192.168.163.141/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.141
IPs:
  IP:           192.168.163.141
Controlled By:  ReplicaSet/np2-qa-helm-nrf-7f48b47b96
Init Containers:
  init-postgres:
    Container ID:  containerd://1539e7eee41d4fa5e2dfffbb6fca825999be0987ee58773cce0746237fd7101a
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf@sha256:3a842d17256f894c17a881b3df18533e669a9053b0da18682cc31cf9c0eaadfd
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 06 Oct 2023 09:58:00 -0400
      Finished:     Fri, 06 Oct 2023 09:58:00 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rcp7g (ro)
Containers:
  nrf-chart:
    Container ID:  containerd://08a720b156e311c8721c3d2136669055ae272db90a035b689c13e29b2cad5e2b
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf@sha256:3a842d17256f894c17a881b3df18533e669a9053b0da18682cc31cf9c0eaadfd
    Port:          10500/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nrf.sh; echo nrf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nrf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rcp7g (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nrf-configmap
    Optional:  false
  nrf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nrf-pv-claim
    ReadOnly:   false
  kube-api-access-rcp7g:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-nsacf-55797458bf-kpv8v
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-nsacf
              pod-template-hash=55797458bf
Annotations:  cni.projectcalico.org/containerID: 147fc71c6f7c3d4912f61808b87782d73cba98376b6ce9c07adc5b96bde5bd92
              cni.projectcalico.org/podIP: 192.168.163.139/32
              cni.projectcalico.org/podIPs: 192.168.163.139/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.139
IPs:
  IP:           192.168.163.139
Controlled By:  ReplicaSet/np2-qa-helm-nsacf-55797458bf
Init Containers:
  init-postgres:
    Container ID:  containerd://2c313d252f8fb0663cbce4a03819ac2b8027b4da1143532ec517c6eee66dd75b
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf@sha256:7fde39b7fb051f51aeaf6a98f6143e58d1d2e2cd7276d38d29189ed7a6d1b1f0
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 06 Oct 2023 09:58:00 -0400
      Finished:     Fri, 06 Oct 2023 09:58:00 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-grbjc (ro)
Containers:
  nsacf-chart:
    Container ID:  containerd://301c6e42778b4b492387491821dbbed7a5c246c5958197d2226277adc38b6717
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf@sha256:7fde39b7fb051f51aeaf6a98f6143e58d1d2e2cd7276d38d29189ed7a6d1b1f0
    Port:          8009/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nsacf.sh; echo nsacf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:02 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nsacf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-grbjc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nsacf-configmap
    Optional:  false
  nsacf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nsacf-pv-claim
    ReadOnly:   false
  kube-api-access-grbjc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-nssaaf-869d47c478-2lcqb
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-nssaaf
              pod-template-hash=869d47c478
Annotations:  cni.projectcalico.org/containerID: 322f9276a78866cf63039f016ec986fe12c3d54716757575b3691df887d159d2
              cni.projectcalico.org/podIP: 192.168.163.150/32
              cni.projectcalico.org/podIPs: 192.168.163.150/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.150
IPs:
  IP:           192.168.163.150
Controlled By:  ReplicaSet/np2-qa-helm-nssaaf-869d47c478
Containers:
  nssaaf-chart:
    Container ID:  containerd://9cd0bf3cc8908cfc36d525805484d453806a73f565ea67c9439d4a21dcba3a98
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssaaf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssaaf@sha256:0a27fc0914434ab7b4f70fbe138f6baa78878e0abac665768c6e6f34268f07b6
    Port:          80/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nssaaf.sh; echo nssaaf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Tue, 07 Nov 2023 01:01:45 -0500
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Mon, 06 Nov 2023 15:01:41 -0500
      Finished:     Tue, 07 Nov 2023 01:01:43 -0500
    Ready:          True
    Restart Count:  76
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nssaaf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v8s6x (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nssaaf-configmap
    Optional:  false
  nssaaf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nssaaf-pv-claim
    ReadOnly:   false
  kube-api-access-v8s6x:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-nssf-768ff4cdd4-2f56h
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-nssf
              pod-template-hash=768ff4cdd4
Annotations:  cni.projectcalico.org/containerID: bda5f024d1299502af22464f02ed82ef6a649f1569159fc0a6c89e76d1b63340
              cni.projectcalico.org/podIP: 192.168.163.144/32
              cni.projectcalico.org/podIPs: 192.168.163.144/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.144
IPs:
  IP:           192.168.163.144
Controlled By:  ReplicaSet/np2-qa-helm-nssf-768ff4cdd4
Init Containers:
  init-postgres:
    Container ID:  containerd://bf7f586cda38ceb09b56964e151a0417b7ab721f9bd1aa2adc30844727a9a515
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf@sha256:1e8f54691b1a09f98ce720a96fbe516dec5607baed45d0ae84ff671c79a10b31
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
      Finished:     Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tqc2t (ro)
Containers:
  nssf-chart:
    Container ID:  containerd://fc9e8db6e156c8ecda992f9cc0c0d6006ed4e952a729e04538e8e7dbb399d672
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf@sha256:1e8f54691b1a09f98ce720a96fbe516dec5607baed45d0ae84ff671c79a10b31
    Port:          29520/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nssf.sh; echo nssf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:02 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nssf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tqc2t (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nssf-configmap
    Optional:  false
  nssf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nssf-pv-claim
    ReadOnly:   false
  kube-api-access-tqc2t:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-pcf-687b4d5977-njfhh
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-pcf
              pod-template-hash=687b4d5977
Annotations:  cni.projectcalico.org/containerID: 5bb4f5bdc784acade7b5d1ecaf2e200a00ddf8fb46d9ac10871b922ac2938582
              cni.projectcalico.org/podIP: 192.168.163.187/32
              cni.projectcalico.org/podIPs: 192.168.163.187/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.187
IPs:
  IP:           192.168.163.187
Controlled By:  ReplicaSet/np2-qa-helm-pcf-687b4d5977
Containers:
  pcf-chart:
    Container ID:  containerd://f8c11126c5b47a1d06202c3b7d9f0a4224aea076d9624bf7c9d5bbbb25a0f7d1
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/pcf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/pcf@sha256:a0e1187d13485b20b1c27dfae4008fd42624b762db95274c15882ff2b0e8d3b4
    Port:          9001/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_pcf.sh; echo pcf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from pcf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h78g4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      pcf-configmap
    Optional:  false
  pcf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  pcf-pv-claim
    ReadOnly:   false
  kube-api-access-h78g4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-rx-c8f646774-kkg5r
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:58:00 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-rx
              pod-template-hash=c8f646774
Annotations:  cni.projectcalico.org/containerID: ba2bb1a16ae02f92ddc1329a6070c230df1dd070d5e17c96aebc50b3acef0c85
              cni.projectcalico.org/podIP: 192.168.163.154/32
              cni.projectcalico.org/podIPs: 192.168.163.154/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.154
IPs:
  IP:           192.168.163.154
Controlled By:  ReplicaSet/np2-qa-helm-rx-c8f646774
Containers:
  rx-chart:
    Container ID:   containerd://01f995d8fa3b542821322ed153e4d714322880f7528c71aad598a5cd685aa4aa
    Image:          registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/rx:23.2.6.1
    Image ID:       registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/rx@sha256:de2ed664a21fb5f0b761004c855a895363e18eb0ac7f89194a0c452513bc4fd4
    Port:           3868/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qz6pw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rx-configmap
    Optional:  false
  rx-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  rx-pv-claim1
    ReadOnly:   false
  kube-api-access-qz6pw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-smf-86bbbf97db-wmjdm
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 11 Oct 2023 08:00:01 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-smf
              pod-template-hash=86bbbf97db
Annotations:  cni.projectcalico.org/containerID: 00d98653a53e5679691d7b9d30ae9db6ae16ec835015ba47add4bb348ec9b91c
              cni.projectcalico.org/podIP: 192.168.163.184/32
              cni.projectcalico.org/podIPs: 192.168.163.184/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.184
IPs:
  IP:           192.168.163.184
Controlled By:  ReplicaSet/np2-qa-helm-smf-86bbbf97db
Init Containers:
  init-redis:
    Container ID:  containerd://ac124639cdcbbb46f6109f66d19caa54ff6b7a479ac7394506dc5d051af4f41c
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf@sha256:dfe3ad0af411fc8a1ffe1cba3ca10d4f30add3d64e40831470b5636529986e01
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until redis-cli -h redis-service ping;do echo waiting for redis database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 11 Oct 2023 08:00:02 -0400
      Finished:     Wed, 11 Oct 2023 08:00:02 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t7bz9 (ro)
Containers:
  smf-chart:
    Container ID:  containerd://1287162aec9d762c982c87e5d29f09e559bd736de8e11e034251de79f1150adc
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf@sha256:dfe3ad0af411fc8a1ffe1cba3ca10d4f30add3d64e40831470b5636529986e01
    Port:          29519/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_smf.sh; echo smf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 03 Nov 2023 23:50:53 -0400
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 27 Oct 2023 09:59:20 -0400
      Finished:     Fri, 03 Nov 2023 23:50:53 -0400
    Ready:          True
    Restart Count:  4
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/log/ from smf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t7bz9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      smf-configmap
    Optional:  false
  smf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  smf-pv-claim
    ReadOnly:   false
  kube-api-access-t7bz9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-udm-55487f7687-hgl96
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:58:00 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-udm
              pod-template-hash=55487f7687
Annotations:  cni.projectcalico.org/containerID: 2ce6bc3034c49c0ee9e6c2dc971ebc7ed111b73da89d297d9916308763ea9c96
              cni.projectcalico.org/podIP: 192.168.163.133/32
              cni.projectcalico.org/podIPs: 192.168.163.133/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.133
IPs:
  IP:           192.168.163.133
Controlled By:  ReplicaSet/np2-qa-helm-udm-55487f7687
Containers:
  udm-chart:
    Container ID:  containerd://b14106932538f663dff47d00889ccaf0cb8c90a34559ef901098021b44815854
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udm:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udm@sha256:18f35916565fc89694b0d46234f6544f83134660f19df6dd715e53b00eb9d0be
    Port:          9000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_udm.sh; echo udm TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from udm-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vcp44 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      udm-configmap
    Optional:  false
  udm-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  udm-pv-claim
    ReadOnly:   false
  kube-api-access-vcp44:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         np2-qa-helm-udr-796b89f9d5-9z8bs
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:58:00 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-udr
              pod-template-hash=796b89f9d5
Annotations:  cni.projectcalico.org/containerID: ce1e5860d6ed2f4be56cad2613f3e50f5290f0cbaca787b773af943506e140a2
              cni.projectcalico.org/podIP: 192.168.163.188/32
              cni.projectcalico.org/podIPs: 192.168.163.188/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.188
IPs:
  IP:           192.168.163.188
Controlled By:  ReplicaSet/np2-qa-helm-udr-796b89f9d5
Init Containers:
  init-postgres:
    Container ID:  containerd://a2dcaf12ba9f26c74cb5fe63e9cb9a508d163e9fa1d17799045a18c753a93e0f
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr@sha256:bb22b6ac84fdafce895cca94e6025495713cac590af0ea65a023b537862774d0
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 06 Oct 2023 09:58:02 -0400
      Finished:     Fri, 06 Oct 2023 09:58:02 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qtshf (ro)
Containers:
  udr-chart:
    Container ID:  containerd://120dd602a2bf4052eaf5b817d8761b9da5b230868b9f1ee1303e2c74f644a7c4
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr@sha256:bb22b6ac84fdafce895cca94e6025495713cac590af0ea65a023b537862774d0
    Port:          8005/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_udr.sh; echo udr TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:03 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from udr-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qtshf (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      udr-configmap
    Optional:  false
  udr-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  udr-pv-claim
    ReadOnly:   false
  kube-api-access-qtshf:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         postgres-statefulset-0
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 27 Sep 2023 10:58:31 -0400
Labels:       app=postgres-app
              controller-revision-hash=postgres-statefulset-749fc55f4b
              statefulset.kubernetes.io/pod-name=postgres-statefulset-0
Annotations:  cni.projectcalico.org/containerID: 1664871878a9adba2187583bb5663578efd09be5093a3cf35bd91461fa40f963
              cni.projectcalico.org/podIP: 192.168.163.179/32
              cni.projectcalico.org/podIPs: 192.168.163.179/32
Status:       Running
IP:           192.168.163.179
IPs:
  IP:           192.168.163.179
Controlled By:  StatefulSet/postgres-statefulset
Containers:
  postgres:
    Container ID:  containerd://7b93a262c3641bca3de99cd37a111a53c559db4271b9e33a28bea76a4bbd4e10
    Image:         postgres:13.0
    Image ID:      docker.io/library/postgres@sha256:8f7c3c9b61d82a4a021da5d9618faf056633e089302a726d619fa467c73609e4
    Port:          5432/TCP
    Host Port:     0/TCP
    Args:
      -c
      max_connections=1000
      -c
      shared_buffers=1024MB
    State:          Running
      Started:      Wed, 27 Sep 2023 10:58:32 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      hugepages-1Gi:  2Gi
      memory:         1Gi
    Requests:
      hugepages-1Gi:  2Gi
      memory:         1Gi
    Environment:
      POSTGRES_PASSWORD:          
      POSTGRES_HOST_AUTH_METHOD:  trust
    Mounts:
      /var/lib/postgresql/data from postgres-pv-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z6ct2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  postgres-pv-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  postgres-pv-claim
    ReadOnly:   false
  kube-api-access-z6ct2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         prometheus-statefulset-0
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 27 Sep 2023 10:58:30 -0400
Labels:       app=prometheus-server
              controller-revision-hash=prometheus-statefulset-76b64d8c6c
              statefulset.kubernetes.io/pod-name=prometheus-statefulset-0
Annotations:  cni.projectcalico.org/containerID: 5c0ecc58cf06f1cfeb0217359df17e6a7537dc8ae62a715a09794c2ec068a76f
              cni.projectcalico.org/podIP: 192.168.163.136/32
              cni.projectcalico.org/podIPs: 192.168.163.136/32
Status:       Running
IP:           192.168.163.136
IPs:
  IP:           192.168.163.136
Controlled By:  StatefulSet/prometheus-statefulset
Containers:
  prometheus:
    Container ID:  containerd://00b682beafc7b3223fb5250da57b748f752baf310bbd34feea0e15e5533271cf
    Image:         prom/prometheus:latest
    Image ID:      docker.io/prom/prometheus@sha256:c5dd3503828713c4949ae1bccd1d8d69f382c33d441954674a6b78ebe69c3331
    Port:          9090/TCP
    Host Port:     0/TCP
    Args:
      --config.file=/etc/config/prometheus.yml
      --storage.tsdb.path=/prometheus/
      --storage.tsdb.retention=3d
    State:          Running
      Started:      Wed, 27 Sep 2023 10:58:32 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/config from config-vol (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5v7j9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-vol:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-server-conf
    Optional:  false
  kube-api-access-5v7j9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         provisioning-statefulset-0
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 27 Sep 2023 10:58:30 -0400
Labels:       app=provisioning-app
              controller-revision-hash=provisioning-statefulset-5bc95db44d
              statefulset.kubernetes.io/pod-name=provisioning-statefulset-0
Annotations:  cni.projectcalico.org/containerID: 5ed46ac2899b849096b399e4522780e8a9121c11988de4dba5ee2af8c3451b64
              cni.projectcalico.org/podIP: 192.168.163.161/32
              cni.projectcalico.org/podIPs: 192.168.163.161/32
Status:       Running
IP:           192.168.163.161
IPs:
  IP:           192.168.163.161
Controlled By:  StatefulSet/provisioning-statefulset
Containers:
  provisioning:
    Container ID:  containerd://1cf2723786f084a13ed3e68cea1460c70f48b7ed2a665f962961da50c767f4c0
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/provisioning:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/provisioning@sha256:02368a6aac62764a761f5f88ab8c0f4bd35601dedfe8e92975699de7d859d6d8
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      /defaults/start_provisioning.sh; tail -f /dev/null
    State:          Running
      Started:      Wed, 27 Sep 2023 10:58:31 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app from provisioning-pv-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gxc9f (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  provisioning-pv-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  provisioning-pv-claim
    ReadOnly:   false
  kube-api-access-gxc9f:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         pushgateway-statefulset-0
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 27 Sep 2023 10:58:30 -0400
Labels:       app=pushgateway-server
              controller-revision-hash=pushgateway-statefulset-75544d5988
              statefulset.kubernetes.io/pod-name=pushgateway-statefulset-0
Annotations:  cni.projectcalico.org/containerID: cf24638e5c1fcc1c21143033b805de298aaf3dddf49f4003b966325d935482af
              cni.projectcalico.org/podIP: 192.168.163.137/32
              cni.projectcalico.org/podIPs: 192.168.163.137/32
Status:       Running
IP:           192.168.163.137
IPs:
  IP:           192.168.163.137
Controlled By:  StatefulSet/pushgateway-statefulset
Containers:
  pushgateway:
    Container ID:   containerd://9ce9faef48e9a1795234789d7c2931b388bb598675bfd47b128fb8ae3764b48f
    Image:          prom/pushgateway:latest
    Image ID:       docker.io/prom/pushgateway@sha256:979a69ab4a4016c89f2b1c53dacaf6190cd676c9d55f7659aabdd208ba48b7c7
    Port:           9091/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 27 Sep 2023 10:58:32 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vc7jl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vc7jl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:         redis-statefulset-0
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 27 Sep 2023 10:58:53 -0400
Labels:       app=redis-app
              controller-revision-hash=redis-statefulset-7dbc765fbb
              statefulset.kubernetes.io/pod-name=redis-statefulset-0
Annotations:  cni.projectcalico.org/containerID: 39c0166fc65b3903149a79efa62faf0463cc15a3270f99fb08b563a9b3d4bbe7
              cni.projectcalico.org/podIP: 192.168.163.170/32
              cni.projectcalico.org/podIPs: 192.168.163.170/32
Status:       Running
IP:           192.168.163.170
IPs:
  IP:           192.168.163.170
Controlled By:  StatefulSet/redis-statefulset
Containers:
  redis:
    Container ID:  containerd://55b0f0508bffe93e312c1efed9a75b8b5b5a6a2f2de9b859d0c9a2d57f4bbf34
    Image:         redis:6.0.8
    Image ID:      docker.io/library/redis@sha256:21db12e5ab3cc343e9376d655e8eabbdbe5516801373e95a8a9e66010c5b8819
    Port:          6379/TCP
    Host Port:     0/TCP
    Args:
      redis-server
      --appendonly
      yes
    State:          Running
      Started:      Wed, 27 Sep 2023 10:58:54 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /data/ from redis-pv-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-r4rkk (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  redis-pv-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  redis-pv-claim
    ReadOnly:   false
  kube-api-access-r4rkk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>
root@as-dl380-r10-12:~/cluster# 