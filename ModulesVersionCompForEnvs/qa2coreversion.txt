root@as-dl360-r13-01:~/cluster# kubectl describe pods -n np2-qa2
Name:             np2-qa2-helm-af-64764cff78-gbpxm
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-af
                  pod-template-hash=64764cff78
Annotations:      cni.projectcalico.org/containerID: 40c8a541b278837f242433e1f2496dc8620848d9769eb7036a8c81c4060559c0
                  cni.projectcalico.org/podIP: 192.168.240.31/32
                  cni.projectcalico.org/podIPs: 192.168.240.31/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.31
IPs:
  IP:           192.168.240.31
Controlled By:  ReplicaSet/np2-qa2-helm-af-64764cff78
Containers:
  af-chart:
    Container ID:  docker://e81ed97e4af9d8edccee9d23c1eabb7e5bed542a06c3bcd2118c10d44035e8ea
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/af:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/af@sha256:8d165883b9f546265580b29204ceb188f5b0fb158b5fdbfb3c81bb059afbc63e
    Port:          8035/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_af.sh; echo af TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from af-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6vgfg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      af-configmap
    Optional:  false
  af-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  af-pv-claim
    ReadOnly:   false
  kube-api-access-6vgfg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-amf-7765fd49d9-m2xcg
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-amf
                  pod-template-hash=7765fd49d9
Annotations:      cni.projectcalico.org/containerID: 1d28de449378d62577a1c474016a1f745c5ad4b7e2a6be53b88f4482526b3b2d
                  cni.projectcalico.org/podIP: 192.168.240.10/32
                  cni.projectcalico.org/podIPs: 192.168.240.10/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.10
IPs:
  IP:           192.168.240.10
Controlled By:  ReplicaSet/np2-qa2-helm-amf-7765fd49d9
Containers:
  amf-chart:
    Container ID:  docker://00e63c318869017b3652df3fd77dfc8f2642b16e5364d3c9a94503a0a7da1deb
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/amf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/amf@sha256:95512263d4ac18f3eef0340f1ca9e8216a3c85950a6457e2767aa4b2160e8041
    Port:          29518/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_amf.sh; echo amf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/log/ from amf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-l6pff (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      amf-configmap
    Optional:  false
  amf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  amf-pv-claim
    ReadOnly:   false
  kube-api-access-l6pff:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-ausf-6d6dc5c4fc-r6qzz
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-ausf
                  pod-template-hash=6d6dc5c4fc
Annotations:      cni.projectcalico.org/containerID: 2cb0e9b12cd2393d483186f2c1b9674c26d7ee174df6e8332a2697e52a49818b
                  cni.projectcalico.org/podIP: 192.168.240.26/32
                  cni.projectcalico.org/podIPs: 192.168.240.26/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.26
IPs:
  IP:           192.168.240.26
Controlled By:  ReplicaSet/np2-qa2-helm-ausf-6d6dc5c4fc
Init Containers:
  init-postgres:
    Container ID:  docker://5ef25d2a14cde20560acb3304cbddf74412ddbe9411d861dc641f94ca35fc2ad
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf@sha256:5d2d61ab18a548a909ff37f8cec61da9dcdb9aeded079a0db9052e9bbb146b8d
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
      Finished:     Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qdpqt (ro)
Containers:
  ausf-chart:
    Container ID:  docker://1d66f53fdf6101172190b70060619cd10c3c4148a1b1c6159b4c6e88fa12238d
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf@sha256:5d2d61ab18a548a909ff37f8cec61da9dcdb9aeded079a0db9052e9bbb146b8d
    Port:          8000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_ausf.sh; echo ausf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:15 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from ausf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qdpqt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      ausf-configmap
    Optional:  false
  ausf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  ausf-pv-claim
    ReadOnly:   false
  kube-api-access-qdpqt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-bsf-799894c5-xjntt
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-bsf
                  pod-template-hash=799894c5
Annotations:      cni.projectcalico.org/containerID: 0bc5627132670df6085ce5b948ac315be71661475e66ddef3c481e17409fcb47
                  cni.projectcalico.org/podIP: 192.168.240.7/32
                  cni.projectcalico.org/podIPs: 192.168.240.7/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.7
IPs:
  IP:           192.168.240.7
Controlled By:  ReplicaSet/np2-qa2-helm-bsf-799894c5
Init Containers:
  init-postgres:
    Container ID:  docker://362edd337da39716d05093c6b307515b323dc649a7f1cc289bb050446279da12
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf@sha256:0187142e3ec679fb4b97b70f17c79bf3420f3720735fecb62101d7089bf03938
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:31 -0400
      Finished:     Wed, 01 Nov 2023 17:15:13 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9mjfm (ro)
Containers:
  bsf-chart:
    Container ID:  docker://138de3fd64ebc4b9271f348dff9acd6306f7421d42c134e0a7c57521395c3522
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf@sha256:0187142e3ec679fb4b97b70f17c79bf3420f3720735fecb62101d7089bf03938
    Port:          11000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_bsf.sh; echo bsf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from bsf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9mjfm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      bsf-configmap
    Optional:  false
  bsf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  bsf-pv-claim
    ReadOnly:   false
  kube-api-access-9mjfm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-nef-59978dd745-6p4tk
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-nef
                  pod-template-hash=59978dd745
Annotations:      cni.projectcalico.org/containerID: 26b2bdb35b3711feeecbe48cc4cd65381325b875b5d0593268f07693cc8d9cf4
                  cni.projectcalico.org/podIP: 192.168.240.8/32
                  cni.projectcalico.org/podIPs: 192.168.240.8/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.8
IPs:
  IP:           192.168.240.8
Controlled By:  ReplicaSet/np2-qa2-helm-nef-59978dd745
Containers:
  nef-chart:
    Container ID:  docker://85883bf8cc54491ddb6284c347fbfefec45b4bb51986b05d738e569a2ec6196c
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nef:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nef@sha256:296c084777712c5c13354ad44f69fd4e85f47c9a354c79d8d431da5a8f6f099f
    Port:          10000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nef.sh; echo nef TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nef-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qzwhx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nef-configmap
    Optional:  false
  nef-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nef-pv-claim
    ReadOnly:   false
  kube-api-access-qzwhx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-nrf-5588c65897-pq4gj
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-nrf
                  pod-template-hash=5588c65897
Annotations:      cni.projectcalico.org/containerID: a26640e5fd2ce18b8b809a1c114922e046d5eb7b34a558d3f65a07812af6bb8a
                  cni.projectcalico.org/podIP: 192.168.240.39/32
                  cni.projectcalico.org/podIPs: 192.168.240.39/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.39
IPs:
  IP:           192.168.240.39
Controlled By:  ReplicaSet/np2-qa2-helm-nrf-5588c65897
Init Containers:
  init-postgres:
    Container ID:  docker://861ba6b8fb182fc8cc3b578cff5245e6792408b13a6491451a01ec295ebfb05f
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf@sha256:3a842d17256f894c17a881b3df18533e669a9053b0da18682cc31cf9c0eaadfd
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
      Finished:     Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-j4m7p (ro)
Containers:
  nrf-chart:
    Container ID:  docker://183f4b4538ad38149aec6b976e82978952de227fd87e694eb4a9c134233ef660
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf@sha256:3a842d17256f894c17a881b3df18533e669a9053b0da18682cc31cf9c0eaadfd
    Port:          10500/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nrf.sh; echo nrf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nrf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-j4m7p (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nrf-configmap
    Optional:  false
  nrf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nrf-pv-claim
    ReadOnly:   false
  kube-api-access-j4m7p:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-nsacf-775c766d84-g4xp4
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-nsacf
                  pod-template-hash=775c766d84
Annotations:      cni.projectcalico.org/containerID: 61ba4a81d568686c932c35337f706326d21bcb81ca7458dc80aec1c4ed0686e6
                  cni.projectcalico.org/podIP: 192.168.240.20/32
                  cni.projectcalico.org/podIPs: 192.168.240.20/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.20
IPs:
  IP:           192.168.240.20
Controlled By:  ReplicaSet/np2-qa2-helm-nsacf-775c766d84
Init Containers:
  init-postgres:
    Container ID:  docker://e856c401d62d572db80a19f0099b9842c81b1f38e21f606933d6798528627898
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf@sha256:7fde39b7fb051f51aeaf6a98f6143e58d1d2e2cd7276d38d29189ed7a6d1b1f0
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
      Finished:     Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jmgvm (ro)
Containers:
  nsacf-chart:
    Container ID:  docker://4b4d56fdb3457bfeea4f9b554161a1a2d6cffa7bc9ea610ffb8269516aa9cfd5
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf@sha256:7fde39b7fb051f51aeaf6a98f6143e58d1d2e2cd7276d38d29189ed7a6d1b1f0
    Port:          8009/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nsacf.sh; echo nsacf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nsacf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jmgvm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nsacf-configmap
    Optional:  false
  nsacf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nsacf-pv-claim
    ReadOnly:   false
  kube-api-access-jmgvm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-nssaaf-f765675d7-n8jps
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-nssaaf
                  pod-template-hash=f765675d7
Annotations:      cni.projectcalico.org/containerID: 04166f26a4b91ad14d9aab9d7bd583da60962cff6680a17e0eb77a31a345e2c3
                  cni.projectcalico.org/podIP: 192.168.240.30/32
                  cni.projectcalico.org/podIPs: 192.168.240.30/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.30
IPs:
  IP:           192.168.240.30
Controlled By:  ReplicaSet/np2-qa2-helm-nssaaf-f765675d7
Containers:
  nssaaf-chart:
    Container ID:  docker://b03c51f0d0aef80559337ddfa143bd4428a5f9d2f5f4a1b2ee7b0fde1b50ada9
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssaaf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssaaf@sha256:0a27fc0914434ab7b4f70fbe138f6baa78878e0abac665768c6e6f34268f07b6
    Port:          80/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nssaaf.sh; echo nssaaf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Tue, 07 Nov 2023 02:15:40 -0500
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Mon, 06 Nov 2023 16:15:38 -0500
      Finished:     Tue, 07 Nov 2023 02:15:39 -0500
    Ready:          True
    Restart Count:  13
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nssaaf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cqnd5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nssaaf-configmap
    Optional:  false
  nssaaf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nssaaf-pv-claim
    ReadOnly:   false
  kube-api-access-cqnd5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-nssf-7554986db9-q4hww
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-nssf
                  pod-template-hash=7554986db9
Annotations:      cni.projectcalico.org/containerID: 988d39ac06e298416a6e490f1832209ce5369dfba4673086dd39bcff2eb1dfaf
                  cni.projectcalico.org/podIP: 192.168.240.41/32
                  cni.projectcalico.org/podIPs: 192.168.240.41/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.41
IPs:
  IP:           192.168.240.41
Controlled By:  ReplicaSet/np2-qa2-helm-nssf-7554986db9
Init Containers:
  init-postgres:
    Container ID:  docker://2a603e9426ebb30876e38f3c8ee252fa24517bdec1d1f58fa6be2964bb517185
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf@sha256:1e8f54691b1a09f98ce720a96fbe516dec5607baed45d0ae84ff671c79a10b31
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
      Finished:     Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p5gm6 (ro)
Containers:
  nssf-chart:
    Container ID:  docker://01a4466772faee8b17096a524bba40153de42317e71123130bf676b68611d37d
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf@sha256:1e8f54691b1a09f98ce720a96fbe516dec5607baed45d0ae84ff671c79a10b31
    Port:          29520/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nssf.sh; echo nssf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nssf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p5gm6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nssf-configmap
    Optional:  false
  nssf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nssf-pv-claim
    ReadOnly:   false
  kube-api-access-p5gm6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-pcf-7d8f6fbcf6-lb74g
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-pcf
                  pod-template-hash=7d8f6fbcf6
Annotations:      cni.projectcalico.org/containerID: d109f5d27387f5404eea67a4b53e1952933cc73bb746a3b179afa7c61f54b6f6
                  cni.projectcalico.org/podIP: 192.168.240.12/32
                  cni.projectcalico.org/podIPs: 192.168.240.12/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.12
IPs:
  IP:           192.168.240.12
Controlled By:  ReplicaSet/np2-qa2-helm-pcf-7d8f6fbcf6
Containers:
  pcf-chart:
    Container ID:  docker://1ace1010fba0337a9f2b8116df3b14783ee42b50d4695caea392753ae25c680e
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/pcf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/pcf@sha256:a0e1187d13485b20b1c27dfae4008fd42624b762db95274c15882ff2b0e8d3b4
    Port:          9001/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_pcf.sh; echo pcf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:31 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from pcf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v2nl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      pcf-configmap
    Optional:  false
  pcf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  pcf-pv-claim
    ReadOnly:   false
  kube-api-access-v2nl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-rx-7f694c7f87-dt9mk
Namespace:        np2-qa2
Priority:         0
Service Account:  np2-qa2-helm-rx
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-rx
                  pod-template-hash=7f694c7f87
Annotations:      cni.projectcalico.org/containerID: 09783040c1dbe77625fd49bb2702fb18560dc98c6e5f7265edd3deab6ee299c7
                  cni.projectcalico.org/podIP: 192.168.240.38/32
                  cni.projectcalico.org/podIPs: 192.168.240.38/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.38
IPs:
  IP:           192.168.240.38
Controlled By:  ReplicaSet/np2-qa2-helm-rx-7f694c7f87
Containers:
  rx-chart:
    Container ID:   docker://4dfa016e04bf92e41a5794ab9ee28a1cd40e7eb9168caddb188c249c826042d2
    Image:          registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/rx:23.2.6.1
    Image ID:       docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/rx@sha256:de2ed664a21fb5f0b761004c855a895363e18eb0ac7f89194a0c452513bc4fd4
    Port:           3868/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-k4d7t (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rx-configmap
    Optional:  false
  rx-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  rx-pv-claim1
    ReadOnly:   false
  kube-api-access-k4d7t:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-smf-9c54789b7-9zst7
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-smf
                  pod-template-hash=9c54789b7
Annotations:      cni.projectcalico.org/containerID: 802e8f12a85aca200c3ccdee91223e93f088b368ab8f312db10e5e1fdf26bdec
                  cni.projectcalico.org/podIP: 192.168.240.9/32
                  cni.projectcalico.org/podIPs: 192.168.240.9/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.9
IPs:
  IP:           192.168.240.9
Controlled By:  ReplicaSet/np2-qa2-helm-smf-9c54789b7
Init Containers:
  init-redis:
    Container ID:  docker://3ab53dbf4adedb702f887cba5266e4328d61efb175b09b278dec0bcc22117f07
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf@sha256:dfe3ad0af411fc8a1ffe1cba3ca10d4f30add3d64e40831470b5636529986e01
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until redis-cli -h redis-service ping;do echo waiting for redis database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
      Finished:     Wed, 01 Nov 2023 17:15:13 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdqfm (ro)
Containers:
  smf-chart:
    Container ID:  docker://0da62a2c77fcae045114db27c5851b5269d6289be396727c93f9599a02dd132e
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf@sha256:dfe3ad0af411fc8a1ffe1cba3ca10d4f30add3d64e40831470b5636529986e01
    Port:          29519/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_smf.sh; echo smf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/log/ from smf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdqfm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      smf-configmap
    Optional:  false
  smf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  smf-pv-claim
    ReadOnly:   false
  kube-api-access-hdqfm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-udm-6c69c4ddf8-vchzk
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-udm
                  pod-template-hash=6c69c4ddf8
Annotations:      cni.projectcalico.org/containerID: d51ca8d78f3e53eb82374f6e5c32af53ce4140ce4a68d27b13b80e58f5111d2f
                  cni.projectcalico.org/podIP: 192.168.240.54/32
                  cni.projectcalico.org/podIPs: 192.168.240.54/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.54
IPs:
  IP:           192.168.240.54
Controlled By:  ReplicaSet/np2-qa2-helm-udm-6c69c4ddf8
Containers:
  udm-chart:
    Container ID:  docker://bf9d6fd35e0267b329802332b45195c25212127b7073d534a00db0c2ee72c0b4
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udm:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udm@sha256:18f35916565fc89694b0d46234f6544f83134660f19df6dd715e53b00eb9d0be
    Port:          9000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_udm.sh; echo udm TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:31 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from udm-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-whjgj (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      udm-configmap
    Optional:  false
  udm-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  udm-pv-claim
    ReadOnly:   false
  kube-api-access-whjgj:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             np2-qa2-helm-udr-585bd678d8-58zz6
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-udr
                  pod-template-hash=585bd678d8
Annotations:      cni.projectcalico.org/containerID: ec83ca4f1dbfbc873fdf0daff9181fd8850fd21d161971be9bc3a2d77043d135
                  cni.projectcalico.org/podIP: 192.168.240.18/32
                  cni.projectcalico.org/podIPs: 192.168.240.18/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.18
IPs:
  IP:           192.168.240.18
Controlled By:  ReplicaSet/np2-qa2-helm-udr-585bd678d8
Init Containers:
  init-postgres:
    Container ID:  docker://0358e05e93883aa4a024ecb5014fc2ebfc89f1a6a900ae61353a4f128b070c5d
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr@sha256:bb22b6ac84fdafce895cca94e6025495713cac590af0ea65a023b537862774d0
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:31 -0400
      Finished:     Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g8mt4 (ro)
Containers:
  udr-chart:
    Container ID:  docker://dad814a2be81ef3f45ea4938ccbf8190803f5471e07f74f880e0c55d9271c4a9
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr@sha256:bb22b6ac84fdafce895cca94e6025495713cac590af0ea65a023b537862774d0
    Port:          8005/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_udr.sh; echo udr TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from udr-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g8mt4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      udr-configmap
    Optional:  false
  udr-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  udr-pv-claim
    ReadOnly:   false
  kube-api-access-g8mt4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             postgres-statefulset-0
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app=postgres-app
                  controller-revision-hash=postgres-statefulset-7d889c56c7
                  statefulset.kubernetes.io/pod-name=postgres-statefulset-0
Annotations:      cni.projectcalico.org/containerID: 9017e508c94dd4d0aa21b252b4832a494f81b6f51f78e71c46fa8f0e400501c8
                  cni.projectcalico.org/podIP: 192.168.240.33/32
                  cni.projectcalico.org/podIPs: 192.168.240.33/32
Status:           Running
IP:               192.168.240.33
IPs:
  IP:           192.168.240.33
Controlled By:  StatefulSet/postgres-statefulset
Containers:
  postgres:
    Container ID:  docker://8f5c17001385cfca9e86e1b371b63d20e1502943b8c917f1cd970ffbbae868cc
    Image:         postgres:13.0
    Image ID:      docker-pullable://postgres@sha256:8f7c3c9b61d82a4a021da5d9618faf056633e089302a726d619fa467c73609e4
    Port:          5432/TCP
    Host Port:     0/TCP
    Args:
      -c
      max_connections=1000
      -c
      shared_buffers=1024MB
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      hugepages-1Gi:  2Gi
      memory:         1Gi
    Requests:
      hugepages-1Gi:  2Gi
      memory:         1Gi
    Environment:
      POSTGRES_PASSWORD:          
      POSTGRES_HOST_AUTH_METHOD:  trust
    Mounts:
      /var/lib/postgresql/data from postgres-pv-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dtmb2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  postgres-pv-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  postgres-pv-claim
    ReadOnly:   false
  kube-api-access-dtmb2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             prometheus-statefulset-0
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:32 -0400
Labels:           app=prometheus-server
                  controller-revision-hash=prometheus-statefulset-7687f88ccb
                  statefulset.kubernetes.io/pod-name=prometheus-statefulset-0
Annotations:      cni.projectcalico.org/containerID: b298e87c5601d3a88d76a37953b0993bf0e26001bfe5ce1ddba3d3bae69a675a
                  cni.projectcalico.org/podIP: 192.168.240.40/32
                  cni.projectcalico.org/podIPs: 192.168.240.40/32
Status:           Running
IP:               192.168.240.40
IPs:
  IP:           192.168.240.40
Controlled By:  StatefulSet/prometheus-statefulset
Containers:
  prometheus:
    Container ID:  docker://76b3b308717816e8e13e7aeb8c9b8031c3fe1cf27acfd41e46dcd51616f59134
    Image:         prom/prometheus:latest
    Image ID:      docker-pullable://prom/prometheus@sha256:3002935850ea69a59816825d4cb718fafcdb9b124e4e6153ebc6894627525f7f
    Port:          9090/TCP
    Host Port:     0/TCP
    Args:
      --config.file=/etc/config/prometheus.yml
      --storage.tsdb.path=/prometheus/
      --storage.tsdb.retention=3d
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:35 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/config from config-vol (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-56lfw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-vol:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-server-conf
    Optional:  false
  kube-api-access-56lfw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             provisioning-statefulset-0
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:15:02 -0400
Labels:           app=provisioning-app
                  controller-revision-hash=provisioning-statefulset-648bcbf487
                  statefulset.kubernetes.io/pod-name=provisioning-statefulset-0
Annotations:      cni.projectcalico.org/containerID: e75ae540ce65eebc83bd73e3e404018905792eaffcc3fa16859e6889f4c15628
                  cni.projectcalico.org/podIP: 192.168.240.53/32
                  cni.projectcalico.org/podIPs: 192.168.240.53/32
Status:           Running
IP:               192.168.240.53
IPs:
  IP:           192.168.240.53
Controlled By:  StatefulSet/provisioning-statefulset
Containers:
  provisioning:
    Container ID:  docker://7b9b85d0907fcceec5ed887cd77035c50d6622cc54067ee76144f125d7113020
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/provisioning:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/provisioning@sha256:02368a6aac62764a761f5f88ab8c0f4bd35601dedfe8e92975699de7d859d6d8
    Port:          <none>
    Host Port:     <none>
    Command:
      bash
    Args:
      -c
      /defaults/start_provisioning.sh; tail -f /dev/null
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:03 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /app from provisioning-pv-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c44h6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  provisioning-pv-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  provisioning-pv-claim
    ReadOnly:   false
  kube-api-access-c44h6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             pushgateway-statefulset-0
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:32 -0400
Labels:           app=pushgateway-server
                  controller-revision-hash=pushgateway-statefulset-9d757c687
                  statefulset.kubernetes.io/pod-name=pushgateway-statefulset-0
Annotations:      cni.projectcalico.org/containerID: 66fa3143f89c4d94d09b029e5c937a0fe2e12d06dfeb9c73fefaf26afb9a6acb
                  cni.projectcalico.org/podIP: 192.168.240.50/32
                  cni.projectcalico.org/podIPs: 192.168.240.50/32
Status:           Running
IP:               192.168.240.50
IPs:
  IP:           192.168.240.50
Controlled By:  StatefulSet/pushgateway-statefulset
Containers:
  pushgateway:
    Container ID:   docker://116142dc8183fdaa1115d628b29d86091eb2ae401acdd2ad71a2844630b7f7a2
    Image:          prom/pushgateway:latest
    Image ID:       docker-pullable://prom/pushgateway@sha256:979a69ab4a4016c89f2b1c53dacaf6190cd676c9d55f7659aabdd208ba48b7c7
    Port:           9091/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:36 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wrz4z (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-wrz4z:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>


Name:             redis-statefulset-0
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:32 -0400
Labels:           app=redis-app
                  controller-revision-hash=redis-statefulset-6d6445965c
                  statefulset.kubernetes.io/pod-name=redis-statefulset-0
Annotations:      cni.projectcalico.org/containerID: fb0b1d89edad65c0d185f3ba558eb033eadbdf28ce3e82fe95d9d81011246a3c
                  cni.projectcalico.org/podIP: 192.168.240.37/32
                  cni.projectcalico.org/podIPs: 192.168.240.37/32
Status:           Running
IP:               192.168.240.37
IPs:
  IP:           192.168.240.37
Controlled By:  StatefulSet/redis-statefulset
Containers:
  redis:
    Container ID:  docker://75ea97513517053865f36646842a2e385fbee0d67c758a77e2c988669ec542bd
    Image:         redis:6.0.8
    Image ID:      docker-pullable://redis@sha256:21db12e5ab3cc343e9376d655e8eabbdbe5516801373e95a8a9e66010c5b8819
    Port:          6379/TCP
    Host Port:     0/TCP
    Args:
      redis-server
      --appendonly
      yes
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:33 -0400
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /data/ from redis-pv-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pt4lk (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  redis-pv-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  redis-pv-claim
    ReadOnly:   false
  kube-api-access-pt4lk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>
root@as-dl360-r13-01:~/cluster# 