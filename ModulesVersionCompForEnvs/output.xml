<?xml version="1.0" encoding="UTF-8"?>
<robot generator="Robot 6.1.1 (Python 3.10.12 on linux)" generated="20231107 17:33:45.093" rpa="false" schemaversion="4">
<suite id="s1" name="VersionComparison" source="/home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/VersionComparison.robot">
<test id="s1-t1" name="Extract Modules Version From CLI And Convert In JSON" line="17">
<kw name="Version_Comparision_Of_Modules_For_Envs">
<for flavor="IN">
<var>${sutdetails}</var>
<value>@{SUT_INPUT}</value>
<iter>
<var name="${sutdetails}">{'Sut_IP': '10.30.249.244', 'Core_IP': '10.30.249.221', 'User_Name': 'vzqa', 'Password': 'Netprizm@2023', 'Env': 'qa2'}</var>
<kw name="Evaluate" library="BuiltIn">
<var>${SUT_IP}</var>
<arg>${sutdetails}.get("Sut_IP","SUT_IP NOT FOUND")</arg>
<doc>Evaluates the given expression in Python and returns the result.</doc>
<msg timestamp="20231107 17:33:45.216" level="INFO">${SUT_IP} = 10.30.249.244</msg>
<status status="PASS" starttime="20231107 17:33:45.216" endtime="20231107 17:33:45.216"/>
</kw>
<kw name="Evaluate" library="BuiltIn">
<var>${Core_IP}</var>
<arg>${sutdetails}.get("Core_IP","Core_IP NOT FOUND")</arg>
<doc>Evaluates the given expression in Python and returns the result.</doc>
<msg timestamp="20231107 17:33:45.216" level="INFO">${Core_IP} = 10.30.249.221</msg>
<status status="PASS" starttime="20231107 17:33:45.216" endtime="20231107 17:33:45.216"/>
</kw>
<kw name="Evaluate" library="BuiltIn">
<var>${User_Name}</var>
<arg>${sutdetails}.get("User_Name","User_Name NOT FOUND")</arg>
<doc>Evaluates the given expression in Python and returns the result.</doc>
<msg timestamp="20231107 17:33:45.217" level="INFO">${User_Name} = vzqa</msg>
<status status="PASS" starttime="20231107 17:33:45.216" endtime="20231107 17:33:45.217"/>
</kw>
<kw name="Evaluate" library="BuiltIn">
<var>${Password}</var>
<arg>${sutdetails}.get("Password","SUT_Password NOT FOUND")</arg>
<doc>Evaluates the given expression in Python and returns the result.</doc>
<msg timestamp="20231107 17:33:45.217" level="INFO">${Password} = Netprizm@2023</msg>
<status status="PASS" starttime="20231107 17:33:45.217" endtime="20231107 17:33:45.217"/>
</kw>
<kw name="Evaluate" library="BuiltIn">
<var>${Env}</var>
<arg>${sutdetails}.get("Env","Env Name NOT FOUND")</arg>
<doc>Evaluates the given expression in Python and returns the result.</doc>
<msg timestamp="20231107 17:33:45.218" level="INFO">${Env} = qa2</msg>
<status status="PASS" starttime="20231107 17:33:45.217" endtime="20231107 17:33:45.218"/>
</kw>
<kw name="Catenate" library="BuiltIn">
<var>${clilogfile}</var>
<arg>${EXECDIR}/${Env}extractVersionsInput.txt</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:33:45.218" level="INFO">${clilogfile} = /home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qa2extractVersionsInput.txt</msg>
<status status="PASS" starttime="20231107 17:33:45.218" endtime="20231107 17:33:45.218"/>
</kw>
<kw name="Create .txt file of modules version from CLI with SSH" library="resource">
<arg>${SUT_IP}</arg>
<arg>${User_Name}</arg>
<arg>${Password}</arg>
<arg>${clilogfile}</arg>
<doc>This keyword is using for login to remote machine with SSH, login as a defined user,
Validate user, Validate Directory and binary file of CLI, Run the CLI on remote machine for version check
and create log file with "extractVersionsInput.txt" name.
It takes Three arguments 1.Server Ip, 2.UserName, 3.Password.</doc>
<kw name="ssh login" library="resource">
<arg>${server_ip}</arg>
<arg>${UserName}</arg>
<arg>${Password}</arg>
<doc>This keyword takes three arguments Server Ip, UserName, Password. This is Used for login remote machine with SSH.</doc>
<kw name="Open Connection" library="SSHLibrary">
<arg>${server_ip1}</arg>
<doc>Opens a new SSH connection to the given ``host`` and ``port``.</doc>
<status status="PASS" starttime="20231107 17:33:45.218" endtime="20231107 17:33:45.219"/>
</kw>
<kw name="Login" library="SSHLibrary">
<arg>${UserName1}</arg>
<arg>${Password1}</arg>
<arg>allow_agent=False</arg>
<arg>look_for_keys=False</arg>
<doc>Logs into the SSH server with the given ``username`` and ``password``.</doc>
<msg timestamp="20231107 17:33:45.219" level="INFO">Logging into '10.30.249.244:22' as 'vzqa'.</msg>
<msg timestamp="20231107 17:33:51.039" level="INFO">Read output: Welcome to Ubuntu 18.04.6 LTS (GNU/Linux 5.4.0-81-lowlatency x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Tue Nov  7 07:03:48 EST 2023

  System load:                    108.83
  Usage of /:                     87.4% of 1.72TB
  Memory usage:                   33%
  Swap usage:                     31%
  Processes:                      1583
  Users logged in:                1
  IP address for eno5:            10.30.249.244
  IP address for docker0:         172.22.0.1
  IP address for br-e145f035970a: 172.17.0.1
  IP address for br-5771f647ff0d: 172.25.1.1
  IP address for br-70ebc9e4f734: 172.24.2.1
  IP address for br-df35bd1171d8: 172.24.1.1
  IP address for br-b8d38e5f84fe: 172.26.0.1

  =&gt; / is using 87.4% of 1.72TB

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

Expanded Security Maintenance for Infrastructure is not enabled.

49 updates can be applied immediately.
33 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable

98 additional security updates can be applied with ESM Infra.
Learn more about enabling ESM Infra service for Ubuntu 18.04 at
https://ubuntu.com/18-04

New release '20.04.6 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Tue Nov  7 05:06:25 2023 from 10.30.248.13
vzqa@as-dl360-r13-24:~$</msg>
<status status="PASS" starttime="20231107 17:33:45.219" endtime="20231107 17:33:51.040"/>
</kw>
<kw name="Set Client Configuration" library="SSHLibrary">
<arg>timeout=2 min</arg>
<doc>Update the `configuration` of the current connection.</doc>
<status status="PASS" starttime="20231107 17:33:51.040" endtime="20231107 17:33:51.040"/>
</kw>
<status status="PASS" starttime="20231107 17:33:45.218" endtime="20231107 17:33:51.040"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.3</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:33:51.341" level="INFO">Slept 300 milliseconds</msg>
<status status="PASS" starttime="20231107 17:33:51.041" endtime="20231107 17:33:51.341"/>
</kw>
<kw name="Catenate" library="BuiltIn">
<var>${commandforchangeuser}</var>
<arg>${SSH_Variables.Command_For_ChangeUser}</arg>
<arg>${SSH_Variables.UserName}</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:33:51.344" level="INFO">${commandforchangeuser} = sudo -i vzqa</msg>
<status status="PASS" starttime="20231107 17:33:51.342" endtime="20231107 17:33:51.344"/>
</kw>
<kw name="Change Directory For Run CLI" library="resource">
<arg>${SSH_Variables.Path_Of_CLI_Directory}</arg>
<doc>This Keyword is using change the directory on remote machine. Its Validate also Directory is changed or not.
If directory changed it returns True
It takes One argument 1.Path of the Directory.</doc>
<kw name="Write Command With SSH" library="resource">
<arg>${Path Of CLI Directory}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:33:51.962" level="INFO">cd /opt/netprizm2/cli</msg>
<status status="PASS" starttime="20231107 17:33:51.346" endtime="20231107 17:33:51.962"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:33:52.063" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:33:51.962" endtime="20231107 17:33:52.063"/>
</kw>
<status status="PASS" starttime="20231107 17:33:51.345" endtime="20231107 17:33:52.063"/>
</kw>
<status status="PASS" starttime="20231107 17:33:51.344" endtime="20231107 17:33:52.063"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>${SSH_Variables.CLI_Run_Command}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:33:52.402" level="INFO">vzqa@as-dl360-r13-24:/opt/netprizm2/cli$ ./np2-cli-tool.sh</msg>
<status status="PASS" starttime="20231107 17:33:52.065" endtime="20231107 17:33:52.402"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:33:52.503" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:33:52.402" endtime="20231107 17:33:52.503"/>
</kw>
<status status="PASS" starttime="20231107 17:33:52.064" endtime="20231107 17:33:52.503"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:33:53.504" level="INFO">Slept 1 second</msg>
<status status="PASS" starttime="20231107 17:33:52.503" endtime="20231107 17:33:53.504"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>nand.kishor@amantyatech.com</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<status status="PASS" starttime="20231107 17:33:53.505" endtime="20231107 17:33:53.506"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:33:53.607" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:33:53.506" endtime="20231107 17:33:53.607"/>
</kw>
<status status="PASS" starttime="20231107 17:33:53.505" endtime="20231107 17:33:53.607"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:33:54.608" level="INFO">Slept 1 second</msg>
<status status="PASS" starttime="20231107 17:33:53.608" endtime="20231107 17:33:54.609"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>Nandk@8468</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:33:54.611" level="INFO">[1m***********************************************[m</msg>
<status status="PASS" starttime="20231107 17:33:54.609" endtime="20231107 17:33:54.611"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:33:54.712" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:33:54.611" endtime="20231107 17:33:54.712"/>
</kw>
<status status="PASS" starttime="20231107 17:33:54.609" endtime="20231107 17:33:54.712"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:33:55.713" level="INFO">Slept 1 second</msg>
<status status="PASS" starttime="20231107 17:33:54.712" endtime="20231107 17:33:55.713"/>
</kw>
<kw name="Read Output Of Command with SSH" library="resource">
<var>${data}</var>
<doc>This Keyword is using for read the any command output or console log of remote machine with SSH.</doc>
<kw name="Read" library="SSHLibrary">
<var>${output}</var>
<arg>delay=.2s</arg>
<doc>Consumes and returns everything available on the server output.</doc>
<msg timestamp="20231107 17:33:55.915" level="INFO">[1mNetprizm CLI Login[m  (Press enter to skip login)
Auto-login successful![m
[m
Welcome: [1mVZ QA[m
Session expires: [1m11-07-2023 15:01:08 EST[m
[1m***********************************************[m




[1mCommand Line Interface (CLI) for Netprizm (CLI Version: 23.09.29.1)[m

   [1m[4mHOME[m[m  |  SYSTEM  |  NETWORK  | USE CASE  |  UE  |  TRAFFIC

   Home Command     | Description
   ---------------- | ---------------------------------------
    h|?,   help     | Help
    su,    signup   | Create a new user
    lgn,   login    | Log into application
    sys,   system   | System Management
    net,   network  | Network Management
    tfc,   traffic  | Traffic Management
    uc,    usecase  | Use Case Management
    ue,    ue       | UE Management
    v,     version  | Get version information
    q,     quit     | Quit and exit CLI



$(home)&gt; nand.kishor@amantyatech.com

[1mInvalid command: 'nand.kishor@amantyatech.com'[m  [1mFor help type: 'h' or 'help' [m

$(home)&gt; Nandk@8468

[1mInvalid command: 'Nandk@8468'[m  [1mFor help type: 'h' or 'help' [m

$(home)&gt;</msg>
<msg timestamp="20231107 17:33:55.915" level="INFO">${output} = [1mNetprizm CLI Login[m  (Press enter to skip login)
Auto-login successful![m
[m
Welcome: [1mVZ QA[m
Session expires: [1m11-07-2023 15:01:08 EST[m
[1m*******************************...</msg>
<status status="PASS" starttime="20231107 17:33:55.714" endtime="20231107 17:33:55.915"/>
</kw>
<msg timestamp="20231107 17:33:55.916" level="INFO">${data} = [1mNetprizm CLI Login[m  (Press enter to skip login)
Auto-login successful![m
[m
Welcome: [1mVZ QA[m
Session expires: [1m11-07-2023 15:01:08 EST[m
[1m*******************************...</msg>
<status status="PASS" starttime="20231107 17:33:55.713" endtime="20231107 17:33:55.916"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>5s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:00.917" level="INFO">Slept 5 seconds</msg>
<status status="PASS" starttime="20231107 17:33:55.916" endtime="20231107 17:34:00.917"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>${SSH_Variables.Modules_Version_Check_Command}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:01.155" level="INFO">v</msg>
<status status="PASS" starttime="20231107 17:34:00.919" endtime="20231107 17:34:01.155"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:01.255" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:01.155" endtime="20231107 17:34:01.255"/>
</kw>
<status status="PASS" starttime="20231107 17:34:00.917" endtime="20231107 17:34:01.255"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:02.256" level="INFO">Slept 1 second</msg>
<status status="PASS" starttime="20231107 17:34:01.256" endtime="20231107 17:34:02.256"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>sys</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<status status="PASS" starttime="20231107 17:34:02.257" endtime="20231107 17:34:02.257"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:02.358" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:02.257" endtime="20231107 17:34:02.358"/>
</kw>
<status status="PASS" starttime="20231107 17:34:02.257" endtime="20231107 17:34:02.358"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:03.359" level="INFO">Slept 1 second</msg>
<status status="PASS" starttime="20231107 17:34:02.358" endtime="20231107 17:34:03.359"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>${SSH_Variables.Modules_Version_Check_Command}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:03.363" level="INFO">[1mRetrieving 'Version' information. Please Wait...[m</msg>
<status status="PASS" starttime="20231107 17:34:03.361" endtime="20231107 17:34:03.363"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:03.464" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:03.363" endtime="20231107 17:34:03.464"/>
</kw>
<status status="PASS" starttime="20231107 17:34:03.360" endtime="20231107 17:34:03.464"/>
</kw>
<kw name="Log To Console" library="BuiltIn">
<arg>${data}</arg>
<doc>Logs the given message to the console.</doc>
<status status="PASS" starttime="20231107 17:34:03.464" endtime="20231107 17:34:03.465"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>datacomplete</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:03.467" level="INFO">Request took .042 seconds on Tue Nov  7 07:04:01 EST 2023.[m</msg>
<status status="PASS" starttime="20231107 17:34:03.466" endtime="20231107 17:34:03.467"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:03.568" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:03.467" endtime="20231107 17:34:03.568"/>
</kw>
<status status="PASS" starttime="20231107 17:34:03.465" endtime="20231107 17:34:03.568"/>
</kw>
<kw name="Read Until Regexp" library="SSHLibrary">
<var>${data1}</var>
<arg>Invalid command:</arg>
<doc>Consumes and returns the server output until a match to ``regexp`` is found.</doc>
<msg timestamp="20231107 17:34:04.672" level="INFO">[1m   NAME               HOST                   VERSION          MESSAGE                       [m
[m   ----------------   --------------------   --------------   ------------------------------[m
   [mCLI_TOOL           10.30.249.244          23.09.29.1       Command Line Interface Tool   [m
   UI_SERVICE         10.30.249.244:9025     23.09.27.2       UI Service for Netprizm       [m
   [mCCL_SERVICE        localhost:9020         23.09.20.2       CCL Service                   [m
   LMM_SERVICE        localhost:8081         23.2.6.3         LMM Service                   [m
   [mUEM_SERVICE        localhost:9026         23.07.11         UE Mapper service for Netprizm[m
   NAF_SERVICE        localhost:9019         23.09.29         NAF Service for Netprizm      [m
   [mEM_SERVICE         localhost:9028         23.09.29.2       Execution Manager Service for Netprizm[m
      NAF_SERVICE     localhost:9019         23.09.29         NAF Service for Netprizm      [m
   [m   SD_SERVICE      10.30.249.244:9027     23.09.21         Simulation Data service for Netprizm[m
   AM_SERVICE         ab-ams.vzsme.com:9092   23.07.05         Account Management Service for Netprizm[m


$(home)&gt; sys





[1mCommand Line Interface (CLI) for Netprizm (CLI Version: 23.09.29.1)[m

   HOME  |  [1m[4mSYSTEM[m[m  |  NETWORK  | USE CASE  |  UE  |  TRAFFIC

   System Command   | Description
   ---------------- | ---------------------------------------
    h,   help       | Help
    hh,  health     | Get health information
    mem  memory     | Show current shared memory cache usage
    on,  start      | Start all system components
    off, stop       | Stop all system components
    l,   logs       | Available Logs from previous runs
    p,   pcap       | Get Pcap Status
    s,   status     | Get current status information
    v,   version    | Get version information
    q,   quit       | Quit and exit CLI



$(system)&gt; v

[1mRetrieving 'Systems Version' information. Please Wait...[m
Request took .297 seconds on Tue Nov  7 07:04:03 EST 2023.[m

datacomplete
[1m   NUM  NAME               HOST                    CNTLR VERSION  APP VERSION    [m
[m   ---  ----------------   --------------------    -------------- -------------- [m
   [m  1  CU_01              10.30.249.222:9022      23.09.25       02.05.13.1     [m[m
     2  PUBSUB_BROKER      10.30.249.244:9033      23.09.25       v23.9.1.1      [m[m
   [m  3  DU11_RANP_PUBSUB   10.30.249.223:9023      23.09.25       v23.9.1.1      [m[m
     4  DU12_RANP_PUBSUB   10.30.249.224:9023      23.09.25       v23.9.1.1      [m[m
   [m  5  CHV_PUBSUB         10.30.249.244:9031      23.09.25       v23.9.1.1      [m[m
     6  UE_01_UEMP_PUBS…   10.30.249.226:9023      23.09.25       v23.9.1.1      [m[m
   [m  7  UE_02_UEMP_PUBS…   10.30.249.227:9023      23.09.25       v23.9.1.1      [m[m
     8  UE_03_UEMP_PUBS…   10.30.249.228:9023      23.09.25       v23.9.1.1      [m[m
   [m  9  UE_04_UEMP_PUBS…   10.30.249.229:9023      23.09.25       v23.9.1.1      [m[m
    10  UE_05_UEMP_PUBS…   10.30.249.230:9023      23.09.25       v23.9.1.1      [m[m
   [m 11  UE_06_UEMP_PUBS…   10.30.249.231:9023      23.09.25       v23.9.1.1      [m[m
    12  UE_07_UEMP_PUBS…   10.30.249.232:9023      23.09.25       v23.9.1.1      [m[m
   [m 13  UE_08_UEMP_PUBS…   10.30.249.233:9023      23.09.25       v23.9.1.1      [m[m
    14  UE_09_UEMP_PUBS…   10.30.249.234:9023      23.09.25       v23.9.1.1      [m[m
   [m 15  UE_10_UEMP_PUBS…   10.30.249.235:9023      23.09.25       v23.9.1.1      [m[m
    16  UE_11_UEMP_PUBS…   10.30.249.236:9023      23.09.25       v23.9.1.1      [m[m
   [m 17  UE_12_UEMP_PUBS…   10.30.249.237:9023      23.09.25       v23.9.1.1      [m[m
    18  UE_13_UEMP_PUBS…   10.30.249.238:9023      23.09.25       v23.9.1.1      [m[m
   [m 19  UE_14_UEMP_PUBS…   10.30.249.239:9023      23.09.25       v23.9.1.1      [m[m
    20  UE_15_UEMP_PUBS…   10.30.249.240:9023      23.09.25       v23.9.1.1      [m[m
   [m 21  UE_16_UEMP_PUBS…   10.30.249.241:9023      23.09.25       v23.9.1.1      [m[m
    22  UE_01_UEMP_CSM     10.30.249.226:9023      23.09.25       v23.9.1.1      [m[m
   [m 23  UE_02_UEMP_CSM     10.30.249.227:9023      23.09.25       v23.9.1.1      [m[m
    24  UE_03_UEMP_CSM     10.30.249.228:9023      23.09.25       v23.9.1.1      [m[m
   [m 25  UE_04_UEMP_CSM     10.30.249.229:9023      23.09.25       v23.9.1.1      [m[m
    26  UE_05_UEMP_CSM     10.30.249.230:9023      23.09.25       v23.9.1.1      [m[m
   [m 27  UE_06_UEMP_CSM     10.30.249.231:9023      23.09.25       v23.9.1.1      [m[m
    28  UE_07_UEMP_CSM     10.30.249.232:9023      23.09.25       v23.9.1.1      [m[m
   [m 29  UE_08_UEMP_CSM     10.30.249.233:9023      23.09.25       v23.9.1.1      [m[m
    30  UE_09_UEMP_CSM     10.30.249.234:9023      23.09.25       v23.9.1.1      [m[m
   [m 31  UE_10_UEMP_CSM     10.30.249.235:9023      23.09.25       v23.9.1.1      [m[m
    32  UE_11_UEMP_CSM     10.30.249.236:9023      23.09.25       v23.9.1.1      [m[m
   [m 33  UE_12_UEMP_CSM     10.30.249.237:9023      23.09.25       v23.9.1.1      [m[m
    34  UE_13_UEMP_CSM     10.30.249.238:9023      23.09.25       v23.9.1.1      [m[m
   [m 35  UE_14_UEMP_CSM     10.30.249.239:9023      23.09.25       v23.9.1.1      [m[m
    36  UE_15_UEMP_CSM     10.30.249.240:9023      23.09.25       v23.9.1.1      [m[m
   [m 37  UE_16_UEMP_CSM     10.30.249.241:9023      23.09.25       v23.9.1.1      [m[m
    38  CHV_CSM            10.30.249.244:9031      23.09.25       v23.9.1.1      [m[m
   [m 39  DU_11_RANP_CSM     10.30.249.223:9023      23.09.25       v23.9.1.1      [m[m
    40  DU_12_RANP_CSM     10.30.249.224:9023      23.09.25       v23.9.1.1      [m[m
   [m 41  CHV                10.30.249.244:9031      23.09.25       RF_23.9.1.0    [m[m
    42  DU_11_RANP         10.30.249.223:9023      23.09.25       v23.9.1.1      [m[m
   [m 43  DU_12_RANP         10.30.249.224:9023      23.09.25       v23.9.1.1      [m[m
    44  DU_11              10.30.249.223:9021      23.09.25       02.05.13       [m[m
   [m 45  DU_12              10.30.249.224:9021      23.09.25       02.05.13       [m[m
    46  UE_01_UEMP         10.30.249.226:9023      23.09.25       v23.9.1.1      [m[m
   [m 47  UE_02_UEMP         10.30.249.227:9023      23.09.25       v23.9.1.1      [m[m
    48  UE_03_UEMP         10.30.249.228:9023      23.09.25       v23.9.1.1      [m[m
   [m 49  UE_04_UEMP         10.30.249.229:9023      23.09.25       v23.9.1.1      [m[m
    50  UE_05_UEMP         10.30.249.230:9023      23.09.25       v23.9.1.1      [m[m
   [m 51  UE_06_UEMP         10.30.249.231:9023      23.09.25       v23.9.1.1      [m[m
    52  UE_07_UEMP         10.30.249.232:9023      23.09.25       v23.9.1.1      [m[m
   [m 53  UE_08_UEMP         10.30.249.233:9023      23.09.25       v23.9.1.1      [m[m
    54  UE_09_UEMP         10.30.249.234:9023      23.09.25       v23.9.1.1      [m[m
   [m 55  UE_10_UEMP         10.30.249.235:9023      23.09.25       v23.9.1.1      [m[m
    56  UE_11_UEMP         10.30.249.236:9023      23.09.25       v23.9.1.1      [m[m
   [m 57  UE_12_UEMP         10.30.249.237:9023      23.09.25       v23.9.1.1      [m[m
    58  UE_13_UEMP         10.30.249.238:9023      23.09.25       v23.9.1.1      [m[m
   [m 59  UE_14_UEMP         10.30.249.239:9023      23.09.25       v23.9.1.1      [m[m
    60  UE_15_UEMP         10.30.249.240:9023      23.09.25       v23.9.1.1      [m[m
   [m 61  UE_16_UEMP         10.30.249.241:9023      23.09.25       v23.9.1.1      [m[m
    62  UE_01              10.30.249.226:9021      23.09.23.1     04.01.02       [m[m
   [m 63  UE_02              10.30.249.227:9021      23.09.23.1     04.01.02       [m[m
    64  UE_03              10.30.249.228:9021      23.09.23.1     04.01.02       [m[m
   [m 65  UE_04              10.30.249.229:9021      23.09.23.1     04.01.02       [m[m
    66  UE_05              10.30.249.230:9021      23.09.23.1     04.01.02       [m[m
   [m 67  UE_06              10.30.249.231:9021      23.09.23.1     04.01.02       [m[m
    68  UE_07              10.30.249.232:9021      23.09.23.1     04.01.02       [m[m
   [m 69  UE_08              10.30.249.233:9021      23.09.23.1     04.01.02       [m[m
    70  UE_09              10.30.249.234:9021      23.09.23.1     04.01.02       [m[m
   [m 71  UE_10              10.30.249.235:9021      23.09.23.1     04.01.03       [m[m
    72  UE_11              10.30.249.236:9021      23.09.23.1     04.01.02       [m[m
   [m 73  UE_12              10.30.249.237:9021      23.09.23.1     04.01.02       [m[m
    74  UE_13              10.30.249.238:9021      23.09.23.1     04.01.02       [m[m
   [m 75  UE_14              10.30.249.239:9021      23.09.23.1     04.01.02       [m[m
    76  UE_15              10.30.249.240:9021      23.09.23.1     04.01.02       [m[m
   [m 77  UE_16              10.30.249.241:9021      23.09.23.1     04.01.02       [m[m
    78  VATEST             10.30.249.244:9031      23.09.25       RF_23.9.1.0    [m[m



$(system)&gt; datacomplete

[1mInvalid command:</msg>
<msg timestamp="20231107 17:34:04.673" level="INFO">${data1} = 
[1m   NAME               HOST                   VERSION          MESSAGE                       [m
[m   ----------------   --------------------   --------------   ------------------------------...</msg>
<status status="PASS" starttime="20231107 17:34:03.569" endtime="20231107 17:34:04.673"/>
</kw>
<kw name="Create File" library="OperatingSystem">
<arg>${clilogfile}</arg>
<arg>${data1}</arg>
<arg>encoding=UTF-8</arg>
<doc>Creates a file with the given content and encoding.</doc>
<msg timestamp="20231107 17:34:04.673" level="INFO" html="true">Created file '&lt;a href="file:///home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qa2extractVersionsInput.txt"&gt;/home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qa2extractVersionsInput.txt&lt;/a&gt;'.</msg>
<status status="PASS" starttime="20231107 17:34:04.673" endtime="20231107 17:34:04.673"/>
</kw>
<kw name="Close All Connections" library="SSHLibrary">
<doc>Closes all open connections.</doc>
<status status="PASS" starttime="20231107 17:34:04.673" endtime="20231107 17:34:04.673"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:05.674" level="INFO">Slept 1 second</msg>
<status status="PASS" starttime="20231107 17:34:04.674" endtime="20231107 17:34:05.674"/>
</kw>
<status status="PASS" starttime="20231107 17:33:45.218" endtime="20231107 17:34:05.674"/>
</kw>
<kw name="Core Version" library="resource">
<var>${Core_data}</var>
<arg>${Core_IP}</arg>
<arg>${Password}</arg>
<arg>${User_Name}</arg>
<arg>${Env}</arg>
<kw name="Sleep" library="BuiltIn">
<arg>2s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:07.676" level="INFO">Slept 2 seconds</msg>
<status status="PASS" starttime="20231107 17:34:05.676" endtime="20231107 17:34:07.676"/>
</kw>
<kw name="ssh login" library="resource">
<arg>${Core_IP}</arg>
<arg>${Core_Password}</arg>
<arg>${User_Name}</arg>
<doc>This keyword takes three arguments Server Ip, UserName, Password. This is Used for login remote machine with SSH.</doc>
<kw name="Open Connection" library="SSHLibrary">
<arg>${server_ip1}</arg>
<doc>Opens a new SSH connection to the given ``host`` and ``port``.</doc>
<status status="PASS" starttime="20231107 17:34:07.678" endtime="20231107 17:34:07.678"/>
</kw>
<kw name="Login" library="SSHLibrary">
<arg>${UserName1}</arg>
<arg>${Password1}</arg>
<arg>allow_agent=False</arg>
<arg>look_for_keys=False</arg>
<doc>Logs into the SSH server with the given ``username`` and ``password``.</doc>
<msg timestamp="20231107 17:34:07.679" level="INFO">Logging into '10.30.249.221:22' as 'vzqa'.</msg>
<msg timestamp="20231107 17:34:15.202" level="INFO">Read output: Welcome to Ubuntu 18.04.6 LTS (GNU/Linux 5.4.0-81-lowlatency x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Tue Nov  7 07:04:12 EST 2023

  System load:  1.17             Users logged in:        0
  Usage of /:   5.3% of 1.72TB   IP address for eno5:    10.30.249.221
  Memory usage: 8%               IP address for docker0: 172.22.0.1
  Swap usage:   0%               IP address for tunl0:   192.168.240.0
  Processes:    1487

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

Expanded Security Maintenance for Infrastructure is not enabled.

56 updates can be applied immediately.
33 of these updates are standard security updates.
To see these additional updates run: apt list --upgradable

97 additional security updates can be applied with ESM Infra.
Learn more about enabling ESM Infra service for Ubuntu 18.04 at
https://ubuntu.com/18-04


Last login: Thu Nov  2 05:56:27 2023 from 10.20.251.171
vzqa@as-dl360-r13-01:~$</msg>
<status status="PASS" starttime="20231107 17:34:07.678" endtime="20231107 17:34:15.202"/>
</kw>
<kw name="Set Client Configuration" library="SSHLibrary">
<arg>timeout=2 min</arg>
<doc>Update the `configuration` of the current connection.</doc>
<status status="PASS" starttime="20231107 17:34:15.203" endtime="20231107 17:34:15.203"/>
</kw>
<status status="PASS" starttime="20231107 17:34:07.677" endtime="20231107 17:34:15.203"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.3</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:15.504" level="INFO">Slept 300 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:15.203" endtime="20231107 17:34:15.504"/>
</kw>
<kw name="Login As User In ssh" library="resource">
<arg>${Core_SSH_Variables.Command_For_ChangeUser}</arg>
<arg>${User_Name}</arg>
<arg>${Core_Password}</arg>
<doc>With the help of this keyword  we login remote machine as user like root , vzqa etc. after this its validate the user also after login.
It takes two arguments 1.UserName, 2. Command for validate user</doc>
<kw name="Write Command With SSH" library="resource">
<arg>${command for enter as user}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:15.947" level="INFO">sudo -i</msg>
<status status="PASS" starttime="20231107 17:34:15.507" endtime="20231107 17:34:15.947"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:16.047" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:15.947" endtime="20231107 17:34:16.048"/>
</kw>
<status status="PASS" starttime="20231107 17:34:15.506" endtime="20231107 17:34:16.048"/>
</kw>
<kw name="Log To Console" library="BuiltIn">
<arg>${command for enter as user}</arg>
<doc>Logs the given message to the console.</doc>
<status status="PASS" starttime="20231107 17:34:16.048" endtime="20231107 17:34:16.049"/>
</kw>
<kw name="Read Output Of Command with SSH" library="resource">
<var>${userdata}</var>
<doc>This Keyword is using for read the any command output or console log of remote machine with SSH.</doc>
<kw name="Read" library="SSHLibrary">
<var>${output}</var>
<arg>delay=.2s</arg>
<doc>Consumes and returns everything available on the server output.</doc>
<msg timestamp="20231107 17:34:16.250" level="INFO">root@as-dl360-r13-01:~#</msg>
<msg timestamp="20231107 17:34:16.251" level="INFO">${output} = root@as-dl360-r13-01:~# </msg>
<status status="PASS" starttime="20231107 17:34:16.049" endtime="20231107 17:34:16.251"/>
</kw>
<msg timestamp="20231107 17:34:16.251" level="INFO">${userdata} = root@as-dl360-r13-01:~# </msg>
<status status="PASS" starttime="20231107 17:34:16.049" endtime="20231107 17:34:16.251"/>
</kw>
<kw name="Log To Console" library="BuiltIn">
<arg>${userdata}</arg>
<doc>Logs the given message to the console.</doc>
<status status="PASS" starttime="20231107 17:34:16.251" endtime="20231107 17:34:16.252"/>
</kw>
<kw name="Set Variable" library="BuiltIn">
<var>${userdata1}</var>
<arg>${userdata}</arg>
<doc>Returns the given values which can then be assigned to a variables.</doc>
<msg timestamp="20231107 17:34:16.252" level="INFO">${userdata1} = root@as-dl360-r13-01:~# </msg>
<status status="PASS" starttime="20231107 17:34:16.252" endtime="20231107 17:34:16.252"/>
</kw>
<kw name="Run Keyword And Return Status" library="BuiltIn">
<var>${ad}</var>
<arg>Should Contain</arg>
<arg>${userdata1}</arg>
<arg>Password:</arg>
<doc>Runs the given keyword with given arguments and returns the status as a Boolean value.</doc>
<kw name="Should Contain" library="BuiltIn">
<arg>${userdata1}</arg>
<arg>Password:</arg>
<doc>Fails if ``container`` does not contain ``item`` one or more times.</doc>
<msg timestamp="20231107 17:34:16.253" level="FAIL">'root@as-dl360-r13-01:~# ' does not contain 'Password:'</msg>
<status status="FAIL" starttime="20231107 17:34:16.253" endtime="20231107 17:34:16.254"/>
</kw>
<msg timestamp="20231107 17:34:16.254" level="INFO">${ad} = False</msg>
<status status="PASS" starttime="20231107 17:34:16.253" endtime="20231107 17:34:16.254"/>
</kw>
<if>
<branch type="IF" condition="$ad">
<kw name="Write Command With SSH" library="resource">
<arg>${Password}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<status status="NOT RUN" starttime="20231107 17:34:16.257" endtime="20231107 17:34:16.257"/>
</kw>
<status status="NOT RUN" starttime="20231107 17:34:16.254" endtime="20231107 17:34:16.257"/>
</branch>
<status status="PASS" starttime="20231107 17:34:16.254" endtime="20231107 17:34:16.257"/>
</if>
<status status="PASS" starttime="20231107 17:34:15.504" endtime="20231107 17:34:16.257"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>cd cluster</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:16.498" level="INFO">cd cluster</msg>
<status status="PASS" starttime="20231107 17:34:16.258" endtime="20231107 17:34:16.498"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:16.599" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:16.498" endtime="20231107 17:34:16.599"/>
</kw>
<status status="PASS" starttime="20231107 17:34:16.258" endtime="20231107 17:34:16.599"/>
</kw>
<kw name="Catenate" library="BuiltIn">
<var>${podsourcecommand}</var>
<arg>SEPARATOR=</arg>
<arg>source np2-</arg>
<arg>${Env}</arg>
<arg>.sh</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:34:16.600" level="INFO">${podsourcecommand} = source np2-qa2.sh</msg>
<status status="PASS" starttime="20231107 17:34:16.599" endtime="20231107 17:34:16.600"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>${podsourcecommand}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:17.157" level="INFO">root@as-dl360-r13-01:~/cluster# source np2-qa2.sh</msg>
<status status="PASS" starttime="20231107 17:34:16.601" endtime="20231107 17:34:17.157"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:17.257" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:17.157" endtime="20231107 17:34:17.257"/>
</kw>
<status status="PASS" starttime="20231107 17:34:16.600" endtime="20231107 17:34:17.257"/>
</kw>
<kw name="Catenate" library="BuiltIn">
<var>${podcommand}</var>
<arg>SEPARATOR=</arg>
<arg>kubectl describe pods -n np2-</arg>
<arg>${Env}</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:34:17.258" level="INFO">${podcommand} = kubectl describe pods -n np2-qa2</msg>
<status status="PASS" starttime="20231107 17:34:17.258" endtime="20231107 17:34:17.258"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>${podcommand}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:17.260" level="INFO">Switched to context "np2-qa2".</msg>
<status status="PASS" starttime="20231107 17:34:17.259" endtime="20231107 17:34:17.260"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:17.361" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:17.260" endtime="20231107 17:34:17.361"/>
</kw>
<status status="PASS" starttime="20231107 17:34:17.258" endtime="20231107 17:34:17.361"/>
</kw>
<kw name="Read" library="SSHLibrary">
<var>${podslog}</var>
<arg>delay=4s</arg>
<doc>Consumes and returns everything available on the server output.</doc>
<msg timestamp="20231107 17:34:29.372" level="INFO">root@as-dl360-r13-01:~/cluster# kubectl describe pods -n np2-qa2
Name:             np2-qa2-helm-af-64764cff78-gbpxm
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-af
                  pod-template-hash=64764cff78
Annotations:      cni.projectcalico.org/containerID: 40c8a541b278837f242433e1f2496dc8620848d9769eb7036a8c81c4060559c0
                  cni.projectcalico.org/podIP: 192.168.240.31/32
                  cni.projectcalico.org/podIPs: 192.168.240.31/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.31
IPs:
  IP:           192.168.240.31
Controlled By:  ReplicaSet/np2-qa2-helm-af-64764cff78
Containers:
  af-chart:
    Container ID:  docker://e81ed97e4af9d8edccee9d23c1eabb7e5bed542a06c3bcd2118c10d44035e8ea
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/af:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/af@sha256:8d165883b9f546265580b29204ceb188f5b0fb158b5fdbfb3c81bb059afbc63e
    Port:          8035/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_af.sh; echo af TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from af-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6vgfg (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      af-configmap
    Optional:  false
  af-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  af-pv-claim
    ReadOnly:   false
  kube-api-access-6vgfg:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-amf-7765fd49d9-m2xcg
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-amf
                  pod-template-hash=7765fd49d9
Annotations:      cni.projectcalico.org/containerID: 1d28de449378d62577a1c474016a1f745c5ad4b7e2a6be53b88f4482526b3b2d
                  cni.projectcalico.org/podIP: 192.168.240.10/32
                  cni.projectcalico.org/podIPs: 192.168.240.10/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.10
IPs:
  IP:           192.168.240.10
Controlled By:  ReplicaSet/np2-qa2-helm-amf-7765fd49d9
Containers:
  amf-chart:
    Container ID:  docker://00e63c318869017b3652df3fd77dfc8f2642b16e5364d3c9a94503a0a7da1deb
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/amf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/amf@sha256:95512263d4ac18f3eef0340f1ca9e8216a3c85950a6457e2767aa4b2160e8041
    Port:          29518/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_amf.sh; echo amf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/log/ from amf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-l6pff (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      amf-configmap
    Optional:  false
  amf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  amf-pv-claim
    ReadOnly:   false
  kube-api-access-l6pff:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-ausf-6d6dc5c4fc-r6qzz
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-ausf
                  pod-template-hash=6d6dc5c4fc
Annotations:      cni.projectcalico.org/containerID: 2cb0e9b12cd2393d483186f2c1b9674c26d7ee174df6e8332a2697e52a49818b
                  cni.projectcalico.org/podIP: 192.168.240.26/32
                  cni.projectcalico.org/podIPs: 192.168.240.26/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.26
IPs:
  IP:           192.168.240.26
Controlled By:  ReplicaSet/np2-qa2-helm-ausf-6d6dc5c4fc
Init Containers:
  init-postgres:
    Container ID:  docker://5ef25d2a14cde20560acb3304cbddf74412ddbe9411d861dc641f94ca35fc2ad
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf@sha256:5d2d61ab18a548a909ff37f8cec61da9dcdb9aeded079a0db9052e9bbb146b8d
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
      Finished:     Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qdpqt (ro)
Containers:
  ausf-chart:
    Container ID:  docker://1d66f53fdf6101172190b70060619cd10c3c4148a1b1c6159b4c6e88fa12238d
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf@sha256:5d2d61ab18a548a909ff37f8cec61da9dcdb9aeded079a0db9052e9bbb146b8d
    Port:          8000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_ausf.sh; echo ausf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:15 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from ausf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qdpqt (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      ausf-configmap
    Optional:  false
  ausf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  ausf-pv-claim
    ReadOnly:   false
  kube-api-access-qdpqt:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-bsf-799894c5-xjntt
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-bsf
                  pod-template-hash=799894c5
Annotations:      cni.projectcalico.org/containerID: 0bc5627132670df6085ce5b948ac315be71661475e66ddef3c481e17409fcb47
                  cni.projectcalico.org/podIP: 192.168.240.7/32
                  cni.projectcalico.org/podIPs: 192.168.240.7/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.7
IPs:
  IP:           192.168.240.7
Controlled By:  ReplicaSet/np2-qa2-helm-bsf-799894c5
Init Containers:
  init-postgres:
    Container ID:  docker://362edd337da39716d05093c6b307515b323dc649a7f1cc289bb050446279da12
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf@sha256:0187142e3ec679fb4b97b70f17c79bf3420f3720735fecb62101d7089bf03938
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:31 -0400
      Finished:     Wed, 01 Nov 2023 17:15:13 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9mjfm (ro)
Containers:
  bsf-chart:
    Container ID:  docker://138de3fd64ebc4b9271f348dff9acd6306f7421d42c134e0a7c57521395c3522
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf@sha256:0187142e3ec679fb4b97b70f17c79bf3420f3720735fecb62101d7089bf03938
    Port:          11000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_bsf.sh; echo bsf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from bsf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-9mjfm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      bsf-configmap
    Optional:  false
  bsf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  bsf-pv-claim
    ReadOnly:   false
  kube-api-access-9mjfm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-nef-59978dd745-6p4tk
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-nef
                  pod-template-hash=59978dd745
Annotations:      cni.projectcalico.org/containerID: 26b2bdb35b3711feeecbe48cc4cd65381325b875b5d0593268f07693cc8d9cf4
                  cni.projectcalico.org/podIP: 192.168.240.8/32
                  cni.projectcalico.org/podIPs: 192.168.240.8/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.8
IPs:
  IP:           192.168.240.8
Controlled By:  ReplicaSet/np2-qa2-helm-nef-59978dd745
Containers:
  nef-chart:
    Container ID:  docker://85883bf8cc54491ddb6284c347fbfefec45b4bb51986b05d738e569a2ec6196c
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nef:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nef@sha256:296c084777712c5c13354ad44f69fd4e85f47c9a354c79d8d431da5a8f6f099f
    Port:          10000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nef.sh; echo nef TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nef-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qzwhx (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nef-configmap
    Optional:  false
  nef-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nef-pv-claim
    ReadOnly:   false
  kube-api-access-qzwhx:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-nrf-5588c65897-pq4gj
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-nrf
                  pod-template-hash=5588c65897
Annotations:      cni.projectcalico.org/containerID: a26640e5fd2ce18b8b809a1c114922e046d5eb7b34a558d3f65a07812af6bb8a
                  cni.projectcalico.org/podIP: 192.168.240.39/32
                  cni.projectcalico.org/podIPs: 192.168.240.39/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.39
IPs:
  IP:           192.168.240.39
Controlled By:  ReplicaSet/np2-qa2-helm-nrf-5588c65897
Init Containers:
  init-postgres:
    Container ID:  docker://861ba6b8fb182fc8cc3b578cff5245e6792408b13a6491451a01ec295ebfb05f
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf@sha256:3a842d17256f894c17a881b3df18533e669a9053b0da18682cc31cf9c0eaadfd
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
      Finished:     Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-j4m7p (ro)
Containers:
  nrf-chart:
    Container ID:  docker://183f4b4538ad38149aec6b976e82978952de227fd87e694eb4a9c134233ef660
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf@sha256:3a842d17256f894c17a881b3df18533e669a9053b0da18682cc31cf9c0eaadfd
    Port:          10500/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nrf.sh; echo nrf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nrf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-j4m7p (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nrf-configmap
    Optional:  false
  nrf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nrf-pv-claim
    ReadOnly:   false
  kube-api-access-j4m7p:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-nsacf-775c766d84-g4xp4
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-nsacf
                  pod-template-hash=775c766d84
Annotations:      cni.projectcalico.org/containerID: 61ba4a81d568686c932c35337f706326d21bcb81ca7458dc80aec1c4ed0686e6
                  cni.projectcalico.org/podIP: 192.168.240.20/32
                  cni.projectcalico.org/podIPs: 192.168.240.20/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.20
IPs:
  IP:           192.168.240.20
Controlled By:  ReplicaSet/np2-qa2-helm-nsacf-775c766d84
Init Containers:
  init-postgres:
    Container ID:  docker://e856c401d62d572db80a19f0099b9842c81b1f38e21f606933d6798528627898
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf@sha256:7fde39b7fb051f51aeaf6a98f6143e58d1d2e2cd7276d38d29189ed7a6d1b1f0
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
      Finished:     Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jmgvm (ro)
Containers:
  nsacf-chart:
    Container ID:  docker://4b4d56fdb3457bfeea4f9b554161a1a2d6cffa7bc9ea610ffb8269516aa9cfd5
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf@sha256:7fde39b7fb051f51aeaf6a98f6143e58d1d2e2cd7276d38d29189ed7a6d1b1f0
    Port:          8009/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nsacf.sh; echo nsacf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nsacf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jmgvm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nsacf-configmap
    Optional:  false
  nsacf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nsacf-pv-claim
    ReadOnly:   false
  kube-api-access-jmgvm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-nssaaf-f765675d7-n8jps
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-nssaaf
                  pod-template-hash=f765675d7
Annotations:      cni.projectcalico.org/containerID: 04166f26a4b91ad14d9aab9d7bd583da60962cff6680a17e0eb77a31a345e2c3
                  cni.projectcalico.org/podIP: 192.168.240.30/32
                  cni.projectcalico.org/podIPs: 192.168.240.30/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.30
IPs:
  IP:           192.168.240.30
Controlled By:  ReplicaSet/np2-qa2-helm-nssaaf-f765675d7
Containers:
  nssaaf-chart:
    Container ID:  docker://b03c51f0d0aef80559337ddfa143bd4428a5f9d2f5f4a1b2ee7b0fde1b50ada9
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssaaf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssaaf@sha256:0a27fc0914434ab7b4f70fbe138f6baa78878e0abac665768c6e6f34268f07b6
    Port:          80/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nssaaf.sh; echo nssaaf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Tue, 07 Nov 2023 02:15:40 -0500
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Mon, 06 Nov 2023 16:15:38 -0500
      Finished:     Tue, 07 Nov 2023 02:15:39 -0500
    Ready:          True
    Restart Count:  13
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nssaaf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cqnd5 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nssaaf-configmap
    Optional:  false
  nssaaf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nssaaf-pv-claim
    ReadOnly:   false
  kube-api-access-cqnd5:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-nssf-7554986db9-q4hww
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-nssf
                  pod-template-hash=7554986db9
Annotations:      cni.projectcalico.org/containerID: 988d39ac06e298416a6e490f1832209ce5369dfba4673086dd39bcff2eb1dfaf
                  cni.projectcalico.org/podIP: 192.168.240.41/32
                  cni.projectcalico.org/podIPs: 192.168.240.41/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.41
IPs:
  IP:           192.168.240.41
Controlled By:  ReplicaSet/np2-qa2-helm-nssf-7554986db9
Init Containers:
  init-postgres:
    Container ID:  docker://2a603e9426ebb30876e38f3c8ee252fa24517bdec1d1f58fa6be2964bb517185
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf@sha256:1e8f54691b1a09f98ce720a96fbe516dec5607baed45d0ae84ff671c79a10b31
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
      Finished:     Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p5gm6 (ro)
Containers:
  nssf-chart:
    Container ID:  docker://01a4466772faee8b17096a524bba40153de42317e71123130bf676b68611d37d
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf@sha256:1e8f54691b1a09f98ce720a96fbe516dec5607baed45d0ae84ff671c79a10b31
    Port:          29520/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nssf.sh; echo nssf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nssf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-p5gm6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nssf-configmap
    Optional:  false
  nssf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nssf-pv-claim
    ReadOnly:   false
  kube-api-access-p5gm6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-pcf-7d8f6fbcf6-lb74g
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-pcf
                  pod-template-hash=7d8f6fbcf6
Annotations:      cni.projectcalico.org/containerID: d109f5d27387f5404eea67a4b53e1952933cc73bb746a3b179afa7c61f54b6f6
                  cni.projectcalico.org/podIP: 192.168.240.12/32
                  cni.projectcalico.org/podIPs: 192.168.240.12/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.12
IPs:
  IP:           192.168.240.12
Controlled By:  ReplicaSet/np2-qa2-helm-pcf-7d8f6fbcf6
Containers:
  pcf-chart:
    Container ID:  docker://1ace1010fba0337a9f2b8116df3b14783ee42b50d4695caea392753ae25c680e
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/pcf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/pcf@sha256:a0e1187d13485b20b1c27dfae4008fd42624b762db95274c15882ff2b0e8d3b4
    Port:          9001/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_pcf.sh; echo pcf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:31 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from pcf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v2nl4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      pcf-configmap
    Optional:  false
  pcf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  pcf-pv-claim
    ReadOnly:   false
  kube-api-access-v2nl4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-rx-7f694c7f87-dt9mk
Namespace:        np2-qa2
Priority:         0
Service Account:  np2-qa2-helm-rx
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-rx
                  pod-template-hash=7f694c7f87
Annotations:      cni.projectcalico.org/containerID: 09783040c1dbe77625fd49bb2702fb18560dc98c6e5f7265edd3deab6ee299c7
                  cni.projectcalico.org/podIP: 192.168.240.38/32
                  cni.projectcalico.org/podIPs: 192.168.240.38/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.38
IPs:
  IP:           192.168.240.38
Controlled By:  ReplicaSet/np2-qa2-helm-rx-7f694c7f87
Containers:
  rx-chart:
    Container ID:   docker://4dfa016e04bf92e41a5794ab9ee28a1cd40e7eb9168caddb188c249c826042d2
    Image:          registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/rx:23.2.6.1
    Image ID:       docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/rx@sha256:de2ed664a21fb5f0b761004c855a895363e18eb0ac7f89194a0c452513bc4fd4
    Port:           3868/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-k4d7t (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rx-configmap
    Optional:  false
  rx-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  rx-pv-claim1
    ReadOnly:   false
  kube-api-access-k4d7t:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-smf-9c54789b7-9zst7
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-smf
                  pod-template-hash=9c54789b7
Annotations:      cni.projectcalico.org/containerID: 802e8f12a85aca200c3ccdee91223e93f088b368ab8f312db10e5e1fdf26bdec
                  cni.projectcalico.org/podIP: 192.168.240.9/32
                  cni.projectcalico.org/podIPs: 192.168.240.9/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.9
IPs:
  IP:           192.168.240.9
Controlled By:  ReplicaSet/np2-qa2-helm-smf-9c54789b7
Init Containers:
  init-redis:
    Container ID:  docker://3ab53dbf4adedb702f887cba5266e4328d61efb175b09b278dec0bcc22117f07
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf@sha256:dfe3ad0af411fc8a1ffe1cba3ca10d4f30add3d64e40831470b5636529986e01
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until redis-cli -h redis-service ping;do echo waiting for redis database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
      Finished:     Wed, 01 Nov 2023 17:15:13 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdqfm (ro)
Containers:
  smf-chart:
    Container ID:  docker://0da62a2c77fcae045114db27c5851b5269d6289be396727c93f9599a02dd132e
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf@sha256:dfe3ad0af411fc8a1ffe1cba3ca10d4f30add3d64e40831470b5636529986e01
    Port:          29519/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_smf.sh; echo smf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/log/ from smf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-hdqfm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      smf-configmap
    Optional:  false
  smf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  smf-pv-claim
    ReadOnly:   false
  kube-api-access-hdqfm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-udm-6c69c4ddf8-vchzk
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-udm
                  pod-template-hash=6c69c4ddf8
Annotations:      cni.projectcalico.org/containerID: d51ca8d78f3e53eb82374f6e5c32af53ce4140ce4a68d27b13b80e58f5111d2f
                  cni.projectcalico.org/podIP: 192.168.240.54/32
                  cni.projectcalico.org/podIPs: 192.168.240.54/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.54
IPs:
  IP:           192.168.240.54
Controlled By:  ReplicaSet/np2-qa2-helm-udm-6c69c4ddf8
Containers:
  udm-chart:
    Container ID:  docker://bf9d6fd35e0267b329802332b45195c25212127b7073d534a00db0c2ee72c0b4
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udm:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udm@sha256:18f35916565fc89694b0d46234f6544f83134660f19df6dd715e53b00eb9d0be
    Port:          9000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_udm.sh; echo udm TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:31 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from udm-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-whjgj (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      udm-configmap
    Optional:  false
  udm-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  udm-pv-claim
    ReadOnly:   false
  kube-api-access-whjgj:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             np2-qa2-helm-udr-585bd678d8-58zz6
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app.kubernetes.io/instance=np2-qa2
                  app.kubernetes.io/name=helm-udr
                  pod-template-hash=585bd678d8
Annotations:      cni.projectcalico.org/containerID: ec83ca4f1dbfbc873fdf0daff9181fd8850fd21d161971be9bc3a2d77043d135
                  cni.projectcalico.org/podIP: 192.168.240.18/32
                  cni.projectcalico.org/podIPs: 192.168.240.18/32
                  kubectl.kubernetes.io/restartedAt: 2023-10-20T14:39:55-04:00
Status:           Running
IP:               192.168.240.18
IPs:
  IP:           192.168.240.18
Controlled By:  ReplicaSet/np2-qa2-helm-udr-585bd678d8
Init Containers:
  init-postgres:
    Container ID:  docker://0358e05e93883aa4a024ecb5014fc2ebfc89f1a6a900ae61353a4f128b070c5d
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr@sha256:bb22b6ac84fdafce895cca94e6025495713cac590af0ea65a023b537862774d0
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 01 Nov 2023 17:14:31 -0400
      Finished:     Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g8mt4 (ro)
Containers:
  udr-chart:
    Container ID:  docker://dad814a2be81ef3f45ea4938ccbf8190803f5471e07f74f880e0c55d9271c4a9
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr@sha256:bb22b6ac84fdafce895cca94e6025495713cac590af0ea65a023b537862774d0
    Port:          8005/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_udr.sh; echo udr TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:14 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from udr-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g8mt4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      udr-configmap
    Optional:  false
  udr-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  udr-pv-claim
    ReadOnly:   false
  kube-api-access-g8mt4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             postgres-statefulset-0
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:31 -0400
Labels:           app=postgres-app
                  controller-revision-hash=postgres-statefulset-7d889c56c7
                  statefulset.kubernetes.io/pod-name=postgres-statefulset-0
Annotations:      cni.projectcalico.org/containerID: 9017e508c94dd4d0aa21b252b4832a494f81b6f51f78e71c46fa8f0e400501c8
                  cni.projectcalico.org/podIP: 192.168.240.33/32
                  cni.projectcalico.org/podIPs: 192.168.240.33/32
Status:           Running
IP:               192.168.240.33
IPs:
  IP:           192.168.240.33
Controlled By:  StatefulSet/postgres-statefulset
Containers:
  postgres:
    Container ID:  docker://8f5c17001385cfca9e86e1b371b63d20e1502943b8c917f1cd970ffbbae868cc
    Image:         postgres:13.0
    Image ID:      docker-pullable://postgres@sha256:8f7c3c9b61d82a4a021da5d9618faf056633e089302a726d619fa467c73609e4
    Port:          5432/TCP
    Host Port:     0/TCP
    Args:
      -c
      max_connections=1000
      -c
      shared_buffers=1024MB
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:32 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      hugepages-1Gi:  2Gi
      memory:         1Gi
    Requests:
      hugepages-1Gi:  2Gi
      memory:         1Gi
    Environment:
      POSTGRES_PASSWORD:          
      POSTGRES_HOST_AUTH_METHOD:  trust
    Mounts:
      /var/lib/postgresql/data from postgres-pv-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-dtmb2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  postgres-pv-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  postgres-pv-claim
    ReadOnly:   false
  kube-api-access-dtmb2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             prometheus-statefulset-0
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:32 -0400
Labels:           app=prometheus-server
                  controller-revision-hash=prometheus-statefulset-7687f88ccb
                  statefulset.kubernetes.io/pod-name=prometheus-statefulset-0
Annotations:      cni.projectcalico.org/containerID: b298e87c5601d3a88d76a37953b0993bf0e26001bfe5ce1ddba3d3bae69a675a
                  cni.projectcalico.org/podIP: 192.168.240.40/32
                  cni.projectcalico.org/podIPs: 192.168.240.40/32
Status:           Running
IP:               192.168.240.40
IPs:
  IP:           192.168.240.40
Controlled By:  StatefulSet/prometheus-statefulset
Containers:
  prometheus:
    Container ID:  docker://76b3b308717816e8e13e7aeb8c9b8031c3fe1cf27acfd41e46dcd51616f59134
    Image:         prom/prometheus:latest
    Image ID:      docker-pullable://prom/prometheus@sha256:3002935850ea69a59816825d4cb718fafcdb9b124e4e6153ebc6894627525f7f
    Port:          9090/TCP
    Host Port:     0/TCP
    Args:
      --config.file=/etc/config/prometheus.yml
      --storage.tsdb.path=/prometheus/
      --storage.tsdb.retention=3d
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:35 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /etc/config from config-vol (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-56lfw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-vol:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-server-conf
    Optional:  false
  kube-api-access-56lfw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             provisioning-statefulset-0
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:15:02 -0400
Labels:           app=provisioning-app
                  controller-revision-hash=provisioning-statefulset-648bcbf487
                  statefulset.kubernetes.io/pod-name=provisioning-statefulset-0
Annotations:      cni.projectcalico.org/containerID: e75ae540ce65eebc83bd73e3e404018905792eaffcc3fa16859e6889f4c15628
                  cni.projectcalico.org/podIP: 192.168.240.53/32
                  cni.projectcalico.org/podIPs: 192.168.240.53/32
Status:           Running
IP:               192.168.240.53
IPs:
  IP:           192.168.240.53
Controlled By:  StatefulSet/provisioning-statefulset
Containers:
  provisioning:
    Container ID:  docker://7b9b85d0907fcceec5ed887cd77035c50d6622cc54067ee76144f125d7113020
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/provisioning:23.2.6.1
    Image ID:      docker-pullable://registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/provisioning@sha256:02368a6aac62764a761f5f88ab8c0f4bd35601dedfe8e92975699de7d859d6d8
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      /defaults/start_provisioning.sh; tail -f /dev/null
    State:          Running
      Started:      Wed, 01 Nov 2023 17:15:03 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app from provisioning-pv-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-c44h6 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  provisioning-pv-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  provisioning-pv-claim
    ReadOnly:   false
  kube-api-access-c44h6:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             pushgateway-statefulset-0
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:32 -0400
Labels:           app=pushgateway-server
                  controller-revision-hash=pushgateway-statefulset-9d757c687
                  statefulset.kubernetes.io/pod-name=pushgateway-statefulset-0
Annotations:      cni.projectcalico.org/containerID: 66fa3143f89c4d94d09b029e5c937a0fe2e12d06dfeb9c73fefaf26afb9a6acb
                  cni.projectcalico.org/podIP: 192.168.240.50/32
                  cni.projectcalico.org/podIPs: 192.168.240.50/32
Status:           Running
IP:               192.168.240.50
IPs:
  IP:           192.168.240.50
Controlled By:  StatefulSet/pushgateway-statefulset
Containers:
  pushgateway:
    Container ID:   docker://116142dc8183fdaa1115d628b29d86091eb2ae401acdd2ad71a2844630b7f7a2
    Image:          prom/pushgateway:latest
    Image ID:       docker-pullable://prom/pushgateway@sha256:979a69ab4a4016c89f2b1c53dacaf6190cd676c9d55f7659aabdd208ba48b7c7
    Port:           9091/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:36 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wrz4z (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-wrz4z:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:             redis-statefulset-0
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:             as-dl360-r13-01.netprizm.local/10.30.249.221
Start Time:       Wed, 01 Nov 2023 17:14:32 -0400
Labels:           app=redis-app
                  controller-revision-hash=redis-statefulset-6d6445965c
                  statefulset.kubernetes.io/pod-name=redis-statefulset-0
Annotations:      cni.projectcalico.org/containerID: fb0b1d89edad65c0d185f3ba558eb033eadbdf28ce3e82fe95d9d81011246a3c
                  cni.projectcalico.org/podIP: 192.168.240.37/32
                  cni.projectcalico.org/podIPs: 192.168.240.37/32
Status:           Running
IP:               192.168.240.37
IPs:
  IP:           192.168.240.37
Controlled By:  StatefulSet/redis-statefulset
Containers:
  redis:
    Container ID:  docker://75ea97513517053865f36646842a2e385fbee0d67c758a77e2c988669ec542bd
    Image:         redis:6.0.8
    Image ID:      docker-pullable://redis@sha256:21db12e5ab3cc343e9376d655e8eabbdbe5516801373e95a8a9e66010c5b8819
    Port:          6379/TCP
    Host Port:     0/TCP
    Args:
      redis-server
      --appendonly
      yes
    State:          Running
      Started:      Wed, 01 Nov 2023 17:14:33 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /data/ from redis-pv-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pt4lk (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  redis-pv-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  redis-pv-claim
    ReadOnly:   false
  kube-api-access-pt4lk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r13-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;
root@as-dl360-r13-01:~/cluster#</msg>
<msg timestamp="20231107 17:34:29.373" level="INFO">${podslog} = root@as-dl360-r13-01:~/cluster# kubectl describe pods -n np2-qa2
Name:             np2-qa2-helm-af-64764cff78-gbpxm
Namespace:        np2-qa2
Priority:         0
Service Account:  default
Node:  ...</msg>
<status status="PASS" starttime="20231107 17:34:17.361" endtime="20231107 17:34:29.373"/>
</kw>
<kw name="Catenate" library="BuiltIn">
<var>${corelogfile}</var>
<arg>${EXECDIR}/${Env}coreversion.txt</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:34:29.374" level="INFO">${corelogfile} = /home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qa2coreversion.txt</msg>
<status status="PASS" starttime="20231107 17:34:29.373" endtime="20231107 17:34:29.374"/>
</kw>
<kw name="Create File" library="OperatingSystem">
<arg>${corelogfile}</arg>
<arg>${podslog}</arg>
<arg>encoding=UTF-8</arg>
<doc>Creates a file with the given content and encoding.</doc>
<msg timestamp="20231107 17:34:29.375" level="INFO" html="true">Created file '&lt;a href="file:///home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qa2coreversion.txt"&gt;/home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qa2coreversion.txt&lt;/a&gt;'.</msg>
<status status="PASS" starttime="20231107 17:34:29.374" endtime="20231107 17:34:29.375"/>
</kw>
<kw name="Getversioncore" library="ParseInJsonV1">
<var>${coremoduledict}</var>
<arg>${corelogfile}</arg>
<msg timestamp="20231107 17:34:29.377" level="INFO">Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/amf:23.2.6.1

/amf: 23.2.6.1

{'AMF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf:23.2.6.1

/ausf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf:23.2.6.1

/ausf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf:23.2.6.1

/nrf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf:23.2.6.1

/nrf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/pcf:23.2.6.1

/pcf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf:23.2.6.1

/smf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf:23.2.6.1

/smf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udm:23.2.6.1

/udm: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1', 'UDM_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr:23.2.6.1

/udr: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1', 'UDM_Version': '23.2.6.1', 'UDR_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr:23.2.6.1

/udr: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1', 'UDM_Version': '23.2.6.1', 'UDR_Version': '23.2.6.1'}
core modele dict :  {'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1', 'UDM_Version': '23.2.6.1', 'UDR_Version': '23.2.6.1'}</msg>
<msg timestamp="20231107 17:34:29.377" level="INFO">${coremoduledict} = {'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1', 'UDM_Version': '23.2.6.1', 'UDR_Version': '23.2.6.1'}</msg>
<status status="PASS" starttime="20231107 17:34:29.375" endtime="20231107 17:34:29.377"/>
</kw>
<kw name="Log To Console" library="BuiltIn">
<arg>${coremoduledict}</arg>
<doc>Logs the given message to the console.</doc>
<status status="PASS" starttime="20231107 17:34:29.377" endtime="20231107 17:34:29.377"/>
</kw>
<msg timestamp="20231107 17:34:29.377" level="INFO">${Core_data} = {'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1', 'UDM_Version': '23.2.6.1', 'UDR_Version': '23.2.6.1'}</msg>
<status status="PASS" starttime="20231107 17:34:05.675" endtime="20231107 17:34:29.377"/>
</kw>
<kw name="Catenate" library="BuiltIn">
<var>${jsonfilename}</var>
<arg>${EXECDIR}/${Env}VersionsOutput.json</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:34:29.378" level="INFO">${jsonfilename} = /home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qa2VersionsOutput.json</msg>
<status status="PASS" starttime="20231107 17:34:29.378" endtime="20231107 17:34:29.378"/>
</kw>
<kw name="Convert From Txt To JSON" library="resource">
<arg>${Env}</arg>
<arg>${SUT_IP}</arg>
<arg>${User_Name}</arg>
<arg>${clilogfile}</arg>
<arg>${jsonfilename}</arg>
<arg>${Core_data}</arg>
<doc>This Keyword is using for Parse data and convert from txt to JSON. Create JSON file with name "VersionsOutput.JSON" in current directory.
It is a Userdefined Keyword.
It takes Five argument 1.Env Name., 2.HostName, 3.UserName, 4.Input Log File Path, 5.Output Json file Path</doc>
<kw name="Catenate" library="BuiltIn">
<var>${LogFilePathAfterRemovingExtraData}</var>
<arg>${EXECDIR}/${EnvName}FilteredCliLog.txt</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:34:29.378" level="INFO">${LogFilePathAfterRemovingExtraData} = /home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qa2FilteredCliLog.txt</msg>
<status status="PASS" starttime="20231107 17:34:29.378" endtime="20231107 17:34:29.378"/>
</kw>
<if>
<branch type="IF" condition="${coremoduledict} == {}">
<kw name="Log To Console" library="BuiltIn">
<arg>Actual data : ${coremoduledict} No data found from "Kubectl describe pods" command</arg>
<doc>Logs the given message to the console.</doc>
<status status="NOT RUN" starttime="20231107 17:34:29.378" endtime="20231107 17:34:29.378"/>
</kw>
<kw name="Fail" library="BuiltIn">
<arg>*HTML*</arg>
<arg>Actual data : ${coremoduledict} No data found from "Kubectl describe pods" command</arg>
<doc>Fails the test with the given message and optionally alters its tags.</doc>
<status status="NOT RUN" starttime="20231107 17:34:29.378" endtime="20231107 17:34:29.378"/>
</kw>
<status status="NOT RUN" starttime="20231107 17:34:29.378" endtime="20231107 17:34:29.378"/>
</branch>
<branch type="ELSE">
<kw name="Parse Injson" library="ParseInJsonV1">
<var>${PassFailStatus}</var>
<arg>${EnvName}</arg>
<arg>${HostName}</arg>
<arg>${UserName}</arg>
<arg>${config_file_path}</arg>
<arg>${input_text_file_path}</arg>
<arg>${coremoduledict}</arg>
<arg>${LogFilePathAfterRemovingExtraData}</arg>
<arg>${output_Json_file_path}</arg>
<msg timestamp="20231107 17:34:29.382" level="INFO">Config file path:  /home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/config.yaml
['CLI_TOOL', '10.30.249.244', '23.09.29.1', 'Command', 'Line', 'Interface', 'Tool']
['UI_SERVICE', '10.30.249.244:9025', '23.09.27.2', 'UI', 'Service', 'for', 'Netprizm']
['CCL_SERVICE', 'localhost:9020', '23.09.20.2', 'CCL', 'Service']
['LMM_SERVICE', 'localhost:8081', '23.2.6.3', 'LMM', 'Service']
['UEM_SERVICE', 'localhost:9026', '23.07.11', 'UE', 'Mapper', 'service', 'for', 'Netprizm']
['NAF_SERVICE', 'localhost:9019', '23.09.29', 'NAF', 'Service', 'for', 'Netprizm']
['EM_SERVICE', 'localhost:9028', '23.09.29.2', 'Execution', 'Manager', 'Service', 'for', 'Netprizm']
['NAF_SERVICE', 'localhost:9019', '23.09.29', 'NAF', 'Service', 'for', 'Netprizm']
['SD_SERVICE', '10.30.249.244:9027', '23.09.21', 'Simulation', 'Data', 'service', 'for', 'Netprizm']
['AM_SERVICE', 'ab-ams.vzsme.com:9092', '23.07.05', 'Account', 'Management', 'Service', 'for', 'Netprizm']
['1', 'CU_01', '10.30.249.222:9022', '23.09.25', '02.05.13.1']
['2', 'PUBSUB_BROKER', '10.30.249.244:9033', '23.09.25', 'v23.9.1.1']
['3', 'DU11_RANP_PUBSUB', '10.30.249.223:9023', '23.09.25', 'v23.9.1.1']
['4', 'DU12_RANP_PUBSUB', '10.30.249.224:9023', '23.09.25', 'v23.9.1.1']
['5', 'CHV_PUBSUB', '10.30.249.244:9031', '23.09.25', 'v23.9.1.1']
['6', 'UE_01_UEMP_PUBS…', '10.30.249.226:9023', '23.09.25', 'v23.9.1.1']
['7', 'UE_02_UEMP_PUBS…', '10.30.249.227:9023', '23.09.25', 'v23.9.1.1']
['8', 'UE_03_UEMP_PUBS…', '10.30.249.228:9023', '23.09.25', 'v23.9.1.1']
['9', 'UE_04_UEMP_PUBS…', '10.30.249.229:9023', '23.09.25', 'v23.9.1.1']
['10', 'UE_05_UEMP_PUBS…', '10.30.249.230:9023', '23.09.25', 'v23.9.1.1']
['11', 'UE_06_UEMP_PUBS…', '10.30.249.231:9023', '23.09.25', 'v23.9.1.1']
['12', 'UE_07_UEMP_PUBS…', '10.30.249.232:9023', '23.09.25', 'v23.9.1.1']
['13', 'UE_08_UEMP_PUBS…', '10.30.249.233:9023', '23.09.25', 'v23.9.1.1']
['14', 'UE_09_UEMP_PUBS…', '10.30.249.234:9023', '23.09.25', 'v23.9.1.1']
['15', 'UE_10_UEMP_PUBS…', '10.30.249.235:9023', '23.09.25', 'v23.9.1.1']
['16', 'UE_11_UEMP_PUBS…', '10.30.249.236:9023', '23.09.25', 'v23.9.1.1']
['17', 'UE_12_UEMP_PUBS…', '10.30.249.237:9023', '23.09.25', 'v23.9.1.1']
['18', 'UE_13_UEMP_PUBS…', '10.30.249.238:9023', '23.09.25', 'v23.9.1.1']
['19', 'UE_14_UEMP_PUBS…', '10.30.249.239:9023', '23.09.25', 'v23.9.1.1']
['20', 'UE_15_UEMP_PUBS…', '10.30.249.240:9023', '23.09.25', 'v23.9.1.1']
['21', 'UE_16_UEMP_PUBS…', '10.30.249.241:9023', '23.09.25', 'v23.9.1.1']
['22', 'UE_01_UEMP_CSM', '10.30.249.226:9023', '23.09.25', 'v23.9.1.1']
['23', 'UE_02_UEMP_CSM', '10.30.249.227:9023', '23.09.25', 'v23.9.1.1']
['24', 'UE_03_UEMP_CSM', '10.30.249.228:9023', '23.09.25', 'v23.9.1.1']
['25', 'UE_04_UEMP_CSM', '10.30.249.229:9023', '23.09.25', 'v23.9.1.1']
['26', 'UE_05_UEMP_CSM', '10.30.249.230:9023', '23.09.25', 'v23.9.1.1']
['27', 'UE_06_UEMP_CSM', '10.30.249.231:9023', '23.09.25', 'v23.9.1.1']
['28', 'UE_07_UEMP_CSM', '10.30.249.232:9023', '23.09.25', 'v23.9.1.1']
['29', 'UE_08_UEMP_CSM', '10.30.249.233:9023', '23.09.25', 'v23.9.1.1']
['30', 'UE_09_UEMP_CSM', '10.30.249.234:9023', '23.09.25', 'v23.9.1.1']
['31', 'UE_10_UEMP_CSM', '10.30.249.235:9023', '23.09.25', 'v23.9.1.1']
['32', 'UE_11_UEMP_CSM', '10.30.249.236:9023', '23.09.25', 'v23.9.1.1']
['33', 'UE_12_UEMP_CSM', '10.30.249.237:9023', '23.09.25', 'v23.9.1.1']
['34', 'UE_13_UEMP_CSM', '10.30.249.238:9023', '23.09.25', 'v23.9.1.1']
['35', 'UE_14_UEMP_CSM', '10.30.249.239:9023', '23.09.25', 'v23.9.1.1']
['36', 'UE_15_UEMP_CSM', '10.30.249.240:9023', '23.09.25', 'v23.9.1.1']
['37', 'UE_16_UEMP_CSM', '10.30.249.241:9023', '23.09.25', 'v23.9.1.1']
['38', 'CHV_CSM', '10.30.249.244:9031', '23.09.25', 'v23.9.1.1']
['39', 'DU_11_RANP_CSM', '10.30.249.223:9023', '23.09.25', 'v23.9.1.1']
['40', 'DU_12_RANP_CSM', '10.30.249.224:9023', '23.09.25', 'v23.9.1.1']
['41', 'CHV', '10.30.249.244:9031', '23.09.25', 'RF_23.9.1.0']
['42', 'DU_11_RANP', '10.30.249.223:9023', '23.09.25', 'v23.9.1.1']
['43', 'DU_12_RANP', '10.30.249.224:9023', '23.09.25', 'v23.9.1.1']
['44', 'DU_11', '10.30.249.223:9021', '23.09.25', '02.05.13']
['45', 'DU_12', '10.30.249.224:9021', '23.09.25', '02.05.13']
['46', 'UE_01_UEMP', '10.30.249.226:9023', '23.09.25', 'v23.9.1.1']
['47', 'UE_02_UEMP', '10.30.249.227:9023', '23.09.25', 'v23.9.1.1']
['48', 'UE_03_UEMP', '10.30.249.228:9023', '23.09.25', 'v23.9.1.1']
['49', 'UE_04_UEMP', '10.30.249.229:9023', '23.09.25', 'v23.9.1.1']
['50', 'UE_05_UEMP', '10.30.249.230:9023', '23.09.25', 'v23.9.1.1']
['51', 'UE_06_UEMP', '10.30.249.231:9023', '23.09.25', 'v23.9.1.1']
['52', 'UE_07_UEMP', '10.30.249.232:9023', '23.09.25', 'v23.9.1.1']
['53', 'UE_08_UEMP', '10.30.249.233:9023', '23.09.25', 'v23.9.1.1']
['54', 'UE_09_UEMP', '10.30.249.234:9023', '23.09.25', 'v23.9.1.1']
['55', 'UE_10_UEMP', '10.30.249.235:9023', '23.09.25', 'v23.9.1.1']
['56', 'UE_11_UEMP', '10.30.249.236:9023', '23.09.25', 'v23.9.1.1']
['57', 'UE_12_UEMP', '10.30.249.237:9023', '23.09.25', 'v23.9.1.1']
['58', 'UE_13_UEMP', '10.30.249.238:9023', '23.09.25', 'v23.9.1.1']
['59', 'UE_14_UEMP', '10.30.249.239:9023', '23.09.25', 'v23.9.1.1']
['60', 'UE_15_UEMP', '10.30.249.240:9023', '23.09.25', 'v23.9.1.1']
['61', 'UE_16_UEMP', '10.30.249.241:9023', '23.09.25', 'v23.9.1.1']
['62', 'UE_01', '10.30.249.226:9021', '23.09.23.1', '04.01.02']
['63', 'UE_02', '10.30.249.227:9021', '23.09.23.1', '04.01.02']
['64', 'UE_03', '10.30.249.228:9021', '23.09.23.1', '04.01.02']
['65', 'UE_04', '10.30.249.229:9021', '23.09.23.1', '04.01.02']
['66', 'UE_05', '10.30.249.230:9021', '23.09.23.1', '04.01.02']
['67', 'UE_06', '10.30.249.231:9021', '23.09.23.1', '04.01.02']
['68', 'UE_07', '10.30.249.232:9021', '23.09.23.1', '04.01.02']
['69', 'UE_08', '10.30.249.233:9021', '23.09.23.1', '04.01.02']
['70', 'UE_09', '10.30.249.234:9021', '23.09.23.1', '04.01.02']
['71', 'UE_10', '10.30.249.235:9021', '23.09.23.1', '04.01.03']
['72', 'UE_11', '10.30.249.236:9021', '23.09.23.1', '04.01.02']
['73', 'UE_12', '10.30.249.237:9021', '23.09.23.1', '04.01.02']
['74', 'UE_13', '10.30.249.238:9021', '23.09.23.1', '04.01.02']
['75', 'UE_14', '10.30.249.239:9021', '23.09.23.1', '04.01.02']
['76', 'UE_15', '10.30.249.240:9021', '23.09.23.1', '04.01.02']
['77', 'UE_16', '10.30.249.241:9021', '23.09.23.1', '04.01.02']
['78', 'VATEST', '10.30.249.244:9031', '23.09.25', 'RF_23.9.1.0']
{
  "modules": {
    "Cores": [
      {
        "CORE_1_Amantya": {
          "OBR_HOST": "10.30.249.221",
          "OBR_USERNAME": "vzqa",
          "AMF_Version": "23.2.6.1",
          "AUSF_Version": "23.2.6.1",
          "NRF_Version": "23.2.6.1",
          "PCF_Version": "23.2.6.1",
          "SMF_Version": "23.2.6.1",
          "UDM_Version": "23.2.6.1",
          "UDR_Version": "23.2.6.1"
        }
      }
    ],
    "CLIs": [
      {
        "qa2": {
          "OBR_HOST": "10.30.249.244",
          "OBR_USERNAME": "vzqa",
          "CLI_TOOL": {
            "app_version": "23.09.29.1"
          },
          "UI_SERVICE": {
            "app_version": "23.09.27.2"
          },
          "CCL_SERVICE": {
            "app_version": "23.09.20.2"
          },
          "LMM_SERVICE": {
            "app_version": "23.2.6.3"
          },
          "UEM_SERVICE": {
            "app_version": "23.07.11"
          },
          "NAF_SERVICE": {
            "app_version": "23.09.29"
          },
          "EM_SERVICE": {
            "app_version": "23.09.29.2"
          },
          "SD_SERVICE": {
            "app_version": "23.09.21"
          },
          "AM_SERVICE": {
            "app_version": "23.07.05"
          },
          "CU_01": {
            "OBR_HOST": "10.30.249.222",
            "app_version": "02.05.13.1",
            "ctlr_version": "23.09.25"
          },
          "PUBSUB_BROKER": {
            "OBR_HOST": "10.30.249.244",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "DU11_RANP_PUBSUB": {
            "OBR_HOST": "10.30.249.223",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "DU12_RANP_PUBSUB": {
            "OBR_HOST": "10.30.249.224",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "CHV_PUBSUB": {
            "OBR_HOST": "10.30.249.244",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_01_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.226",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_02_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.227",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_03_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.228",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_04_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.229",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_05_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.230",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_06_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.231",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_07_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.232",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_08_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.233",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_09_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.234",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_10_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.235",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_11_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.236",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_12_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.237",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_13_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.238",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_14_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.239",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_15_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.240",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_16_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.241",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_01_UEMP_CSM": {
            "OBR_HOST": "10.30.249.226",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_02_UEMP_CSM": {
            "OBR_HOST": "10.30.249.227",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_03_UEMP_CSM": {
            "OBR_HOST": "10.30.249.228",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_04_UEMP_CSM": {
            "OBR_HOST": "10.30.249.229",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_05_UEMP_CSM": {
            "OBR_HOST": "10.30.249.230",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_06_UEMP_CSM": {
            "OBR_HOST": "10.30.249.231",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_07_UEMP_CSM": {
            "OBR_HOST": "10.30.249.232",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_08_UEMP_CSM": {
            "OBR_HOST": "10.30.249.233",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_09_UEMP_CSM": {
            "OBR_HOST": "10.30.249.234",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_10_UEMP_CSM": {
            "OBR_HOST": "10.30.249.235",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_11_UEMP_CSM": {
            "OBR_HOST": "10.30.249.236",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_12_UEMP_CSM": {
            "OBR_HOST": "10.30.249.237",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_13_UEMP_CSM": {
            "OBR_HOST": "10.30.249.238",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_14_UEMP_CSM": {
            "OBR_HOST": "10.30.249.239",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_15_UEMP_CSM": {
            "OBR_HOST": "10.30.249.240",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_16_UEMP_CSM": {
            "OBR_HOST": "10.30.249.241",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "CHV_CSM": {
            "OBR_HOST": "10.30.249.244",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "DU_11_RANP_CSM": {
            "OBR_HOST": "10.30.249.223",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "DU_12_RANP_CSM": {
            "OBR_HOST": "10.30.249.224",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "CHV": {
            "OBR_HOST": "10.30.249.244",
            "app_version": "RF_23.9.1.0",
            "ctlr_version": "23.09.25"
          },
          "DU_11_RANP": {
            "OBR_HOST": "10.30.249.223",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "DU_12_RANP": {
            "OBR_HOST": "10.30.249.224",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "DU_11": {
            "OBR_HOST": "10.30.249.223",
            "app_version": "02.05.13",
            "ctlr_version": "23.09.25"
          },
          "DU_12": {
            "OBR_HOST": "10.30.249.224",
            "app_version": "02.05.13",
            "ctlr_version": "23.09.25"
          },
          "UE_01_UEMP": {
            "OBR_HOST": "10.30.249.226",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_02_UEMP": {
            "OBR_HOST": "10.30.249.227",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_03_UEMP": {
            "OBR_HOST": "10.30.249.228",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_04_UEMP": {
            "OBR_HOST": "10.30.249.229",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_05_UEMP": {
            "OBR_HOST": "10.30.249.230",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_06_UEMP": {
            "OBR_HOST": "10.30.249.231",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_07_UEMP": {
            "OBR_HOST": "10.30.249.232",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_08_UEMP": {
            "OBR_HOST": "10.30.249.233",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_09_UEMP": {
            "OBR_HOST": "10.30.249.234",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_10_UEMP": {
            "OBR_HOST": "10.30.249.235",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_11_UEMP": {
            "OBR_HOST": "10.30.249.236",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_12_UEMP": {
            "OBR_HOST": "10.30.249.237",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_13_UEMP": {
            "OBR_HOST": "10.30.249.238",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_14_UEMP": {
            "OBR_HOST": "10.30.249.239",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_15_UEMP": {
            "OBR_HOST": "10.30.249.240",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_16_UEMP": {
            "OBR_HOST": "10.30.249.241",
            "app_version": "v23.9.1.1",
            "ctlr_version": "23.09.25"
          },
          "UE_01": {
            "OBR_HOST": "10.30.249.226",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_02": {
            "OBR_HOST": "10.30.249.227",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_03": {
            "OBR_HOST": "10.30.249.228",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_04": {
            "OBR_HOST": "10.30.249.229",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_05": {
            "OBR_HOST": "10.30.249.230",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_06": {
            "OBR_HOST": "10.30.249.231",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_07": {
            "OBR_HOST": "10.30.249.232",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_08": {
            "OBR_HOST": "10.30.249.233",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_09": {
            "OBR_HOST": "10.30.249.234",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_10": {
            "OBR_HOST": "10.30.249.235",
            "app_version": "04.01.03",
            "ctlr_version": "23.09.23.1"
          },
          "UE_11": {
            "OBR_HOST": "10.30.249.236",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_12": {
            "OBR_HOST": "10.30.249.237",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_13": {
            "OBR_HOST": "10.30.249.238",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_14": {
            "OBR_HOST": "10.30.249.239",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_15": {
            "OBR_HOST": "10.30.249.240",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "UE_16": {
            "OBR_HOST": "10.30.249.241",
            "app_version": "04.01.02",
            "ctlr_version": "23.09.23.1"
          },
          "VATEST": {
            "OBR_HOST": "10.30.249.244",
            "app_version": "RF_23.9.1.0",
            "ctlr_version": "23.09.25"
          }
        }
      }
    ]
  }
}</msg>
<msg timestamp="20231107 17:34:29.382" level="INFO">${PassFailStatus} = 1</msg>
<status status="PASS" starttime="20231107 17:34:29.379" endtime="20231107 17:34:29.382"/>
</kw>
<kw name="Run Keyword And Return Status" library="BuiltIn">
<var>${ValidatePassFailStatus}</var>
<arg>Should Contain</arg>
<arg>${PassFailStatus}</arg>
<arg>1</arg>
<doc>Runs the given keyword with given arguments and returns the status as a Boolean value.</doc>
<kw name="Should Contain" library="BuiltIn">
<arg>${PassFailStatus}</arg>
<arg>1</arg>
<doc>Fails if ``container`` does not contain ``item`` one or more times.</doc>
<status status="PASS" starttime="20231107 17:34:29.383" endtime="20231107 17:34:29.383"/>
</kw>
<msg timestamp="20231107 17:34:29.383" level="INFO">${ValidatePassFailStatus} = True</msg>
<status status="PASS" starttime="20231107 17:34:29.383" endtime="20231107 17:34:29.383"/>
</kw>
<if>
<branch type="IF" condition="${ValidatePassFailStatus}==False">
<kw name="Fail" library="BuiltIn">
<arg>*HTML*${PassFailStatus}</arg>
<doc>Fails the test with the given message and optionally alters its tags.</doc>
<status status="NOT RUN" starttime="20231107 17:34:29.383" endtime="20231107 17:34:29.383"/>
</kw>
<status status="NOT RUN" starttime="20231107 17:34:29.383" endtime="20231107 17:34:29.383"/>
</branch>
<status status="PASS" starttime="20231107 17:34:29.383" endtime="20231107 17:34:29.383"/>
</if>
<status status="PASS" starttime="20231107 17:34:29.378" endtime="20231107 17:34:29.383"/>
</branch>
<status status="PASS" starttime="20231107 17:34:29.378" endtime="20231107 17:34:29.383"/>
</if>
<status status="PASS" starttime="20231107 17:34:29.378" endtime="20231107 17:34:29.383"/>
</kw>
<kw name="Append To List" library="Collections">
<arg>${jsonfilenamelist}</arg>
<arg>${jsonfilename}</arg>
<doc>Adds ``values`` to the end of ``list``.</doc>
<status status="PASS" starttime="20231107 17:34:29.383" endtime="20231107 17:34:29.383"/>
</kw>
<status status="PASS" starttime="20231107 17:33:45.216" endtime="20231107 17:34:29.383"/>
</iter>
<iter>
<var name="${sutdetails}">{'Sut_IP': '10.30.249.96', 'Core_IP': '10.30.249.96', 'User_Name': 'vzqa', 'Password': 'B^m~9$', 'Env': 'qa'}</var>
<kw name="Evaluate" library="BuiltIn">
<var>${SUT_IP}</var>
<arg>${sutdetails}.get("Sut_IP","SUT_IP NOT FOUND")</arg>
<doc>Evaluates the given expression in Python and returns the result.</doc>
<msg timestamp="20231107 17:34:29.384" level="INFO">${SUT_IP} = 10.30.249.96</msg>
<status status="PASS" starttime="20231107 17:34:29.383" endtime="20231107 17:34:29.384"/>
</kw>
<kw name="Evaluate" library="BuiltIn">
<var>${Core_IP}</var>
<arg>${sutdetails}.get("Core_IP","Core_IP NOT FOUND")</arg>
<doc>Evaluates the given expression in Python and returns the result.</doc>
<msg timestamp="20231107 17:34:29.384" level="INFO">${Core_IP} = 10.30.249.96</msg>
<status status="PASS" starttime="20231107 17:34:29.384" endtime="20231107 17:34:29.384"/>
</kw>
<kw name="Evaluate" library="BuiltIn">
<var>${User_Name}</var>
<arg>${sutdetails}.get("User_Name","User_Name NOT FOUND")</arg>
<doc>Evaluates the given expression in Python and returns the result.</doc>
<msg timestamp="20231107 17:34:29.384" level="INFO">${User_Name} = vzqa</msg>
<status status="PASS" starttime="20231107 17:34:29.384" endtime="20231107 17:34:29.384"/>
</kw>
<kw name="Evaluate" library="BuiltIn">
<var>${Password}</var>
<arg>${sutdetails}.get("Password","SUT_Password NOT FOUND")</arg>
<doc>Evaluates the given expression in Python and returns the result.</doc>
<msg timestamp="20231107 17:34:29.384" level="INFO">${Password} = B^m~9$</msg>
<status status="PASS" starttime="20231107 17:34:29.384" endtime="20231107 17:34:29.384"/>
</kw>
<kw name="Evaluate" library="BuiltIn">
<var>${Env}</var>
<arg>${sutdetails}.get("Env","Env Name NOT FOUND")</arg>
<doc>Evaluates the given expression in Python and returns the result.</doc>
<msg timestamp="20231107 17:34:29.385" level="INFO">${Env} = qa</msg>
<status status="PASS" starttime="20231107 17:34:29.385" endtime="20231107 17:34:29.385"/>
</kw>
<kw name="Catenate" library="BuiltIn">
<var>${clilogfile}</var>
<arg>${EXECDIR}/${Env}extractVersionsInput.txt</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:34:29.385" level="INFO">${clilogfile} = /home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qaextractVersionsInput.txt</msg>
<status status="PASS" starttime="20231107 17:34:29.385" endtime="20231107 17:34:29.385"/>
</kw>
<kw name="Create .txt file of modules version from CLI with SSH" library="resource">
<arg>${SUT_IP}</arg>
<arg>${User_Name}</arg>
<arg>${Password}</arg>
<arg>${clilogfile}</arg>
<doc>This keyword is using for login to remote machine with SSH, login as a defined user,
Validate user, Validate Directory and binary file of CLI, Run the CLI on remote machine for version check
and create log file with "extractVersionsInput.txt" name.
It takes Three arguments 1.Server Ip, 2.UserName, 3.Password.</doc>
<kw name="ssh login" library="resource">
<arg>${server_ip}</arg>
<arg>${UserName}</arg>
<arg>${Password}</arg>
<doc>This keyword takes three arguments Server Ip, UserName, Password. This is Used for login remote machine with SSH.</doc>
<kw name="Open Connection" library="SSHLibrary">
<arg>${server_ip1}</arg>
<doc>Opens a new SSH connection to the given ``host`` and ``port``.</doc>
<status status="PASS" starttime="20231107 17:34:29.385" endtime="20231107 17:34:29.385"/>
</kw>
<kw name="Login" library="SSHLibrary">
<arg>${UserName1}</arg>
<arg>${Password1}</arg>
<arg>allow_agent=False</arg>
<arg>look_for_keys=False</arg>
<doc>Logs into the SSH server with the given ``username`` and ``password``.</doc>
<msg timestamp="20231107 17:34:29.386" level="INFO">Logging into '10.30.249.96:22' as 'vzqa'.</msg>
<msg timestamp="20231107 17:34:34.659" level="INFO">Read output: Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 5.4.0-81-lowlatency x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Tue Nov  7 07:04:31 EST 2023

  System load:                    16.48
  Usage of /:                     57.2% of 1.72TB
  Memory usage:                   35%
  Swap usage:                     7%
  Processes:                      1185
  Users logged in:                1
  IP address for eno5:            10.30.249.96
  IP address for docker0:         172.17.0.1
  IP address for br-5f4e93bc1bad: 172.18.0.1
  IP address for br-75a8787c16ae: 172.24.1.1
  IP address for br-2b1a5009ad8b: 172.24.2.1
  IP address for br-9dd2cb5cb370: 172.25.1.1

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

 * Canonical Livepatch is available for installation.
   - Reduce system reboots and improve kernel security. Activate at:
     https://ubuntu.com/livepatch

313 packages can be updated.
236 updates are security updates.

New release '20.04.6 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Tue Nov  7 01:59:01 2023 from 10.20.251.180</msg>
<status status="PASS" starttime="20231107 17:34:29.386" endtime="20231107 17:34:34.660"/>
</kw>
<kw name="Set Client Configuration" library="SSHLibrary">
<arg>timeout=2 min</arg>
<doc>Update the `configuration` of the current connection.</doc>
<status status="PASS" starttime="20231107 17:34:34.660" endtime="20231107 17:34:34.661"/>
</kw>
<status status="PASS" starttime="20231107 17:34:29.385" endtime="20231107 17:34:34.661"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.3</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:34.961" level="INFO">Slept 300 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:34.661" endtime="20231107 17:34:34.962"/>
</kw>
<kw name="Catenate" library="BuiltIn">
<var>${commandforchangeuser}</var>
<arg>${SSH_Variables.Command_For_ChangeUser}</arg>
<arg>${SSH_Variables.UserName}</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:34:34.964" level="INFO">${commandforchangeuser} = sudo -i vzqa</msg>
<status status="PASS" starttime="20231107 17:34:34.962" endtime="20231107 17:34:34.964"/>
</kw>
<kw name="Change Directory For Run CLI" library="resource">
<arg>${SSH_Variables.Path_Of_CLI_Directory}</arg>
<doc>This Keyword is using change the directory on remote machine. Its Validate also Directory is changed or not.
If directory changed it returns True
It takes One argument 1.Path of the Directory.</doc>
<kw name="Write Command With SSH" library="resource">
<arg>${Path Of CLI Directory}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:35.250" level="INFO">cd /opt/netprizm2/cli</msg>
<status status="PASS" starttime="20231107 17:34:34.966" endtime="20231107 17:34:35.250"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:35.351" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:35.251" endtime="20231107 17:34:35.351"/>
</kw>
<status status="PASS" starttime="20231107 17:34:34.966" endtime="20231107 17:34:35.351"/>
</kw>
<status status="PASS" starttime="20231107 17:34:34.964" endtime="20231107 17:34:35.351"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>${SSH_Variables.CLI_Run_Command}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:35.694" level="INFO">./np2-cli-tool.sh</msg>
<status status="PASS" starttime="20231107 17:34:35.353" endtime="20231107 17:34:35.694"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:35.795" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:35.694" endtime="20231107 17:34:35.795"/>
</kw>
<status status="PASS" starttime="20231107 17:34:35.352" endtime="20231107 17:34:35.795"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:36.796" level="INFO">Slept 1 second</msg>
<status status="PASS" starttime="20231107 17:34:35.795" endtime="20231107 17:34:36.796"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>nand.kishor@amantyatech.com</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:36.798" level="INFO">vzqa@as-dl380-r10-12:~$ cd /opt/netprizm2/cli</msg>
<status status="PASS" starttime="20231107 17:34:36.797" endtime="20231107 17:34:36.798"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:36.899" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:36.799" endtime="20231107 17:34:36.899"/>
</kw>
<status status="PASS" starttime="20231107 17:34:36.796" endtime="20231107 17:34:36.900"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:37.901" level="INFO">Slept 1 second</msg>
<status status="PASS" starttime="20231107 17:34:36.900" endtime="20231107 17:34:37.901"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>Nandk@8468</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:37.903" level="INFO">vzqa@as-dl380-r10-12:/opt/netprizm2/cli$ ./np2-cli-tool.sh</msg>
<status status="PASS" starttime="20231107 17:34:37.902" endtime="20231107 17:34:37.903"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:38.004" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:37.904" endtime="20231107 17:34:38.005"/>
</kw>
<status status="PASS" starttime="20231107 17:34:37.901" endtime="20231107 17:34:38.005"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:39.006" level="INFO">Slept 1 second</msg>
<status status="PASS" starttime="20231107 17:34:38.005" endtime="20231107 17:34:39.006"/>
</kw>
<kw name="Read Output Of Command with SSH" library="resource">
<var>${data}</var>
<doc>This Keyword is using for read the any command output or console log of remote machine with SSH.</doc>
<kw name="Read" library="SSHLibrary">
<var>${output}</var>
<arg>delay=.2s</arg>
<doc>Consumes and returns everything available on the server output.</doc>
<msg timestamp="20231107 17:34:39.208" level="INFO">[1m***********************************************[m
[1mNetprizm CLI Login[m  (Press enter to skip login)
Auto-login successful![m
[m
Welcome: [1mVZ QA[m
Session expires: [1m11-07-2023 13:01:06 EST[m
[1m***********************************************[m




[1mCommand Line Interface (CLI) for Netprizm (CLI Version: 23.08.16.1)[m

   [1m[4mHOME[m[m  |  SYSTEM  |  NETWORK  | USE CASE  |  UE  |  TRAFFIC

   Home Command     | Description
   ---------------- | ---------------------------------------
    h|?,   help     | Help
    su,    signup   | Create a new user
    lgn,   login    | Log into application
    sys,   system   | System Management
    net,   network  | Network Management
    tfc,   traffic  | Traffic Management
    uc,    usecase  | Use Case Management
    ue,    ue       | UE Management
    v,     version  | Get version information
    q,     quit     | Quit and exit CLI



$(home)&gt; nand.kishor@amantyatech.com

[1mInvalid command: 'nand.kishor@amantyatech.com'[m  [1mFor help type: 'h' or 'help' [m

$(home)&gt; Nandk@8468

[1mInvalid command: 'Nandk@8468'[m  [1mFor help type: 'h' or 'help' [m

$(home)&gt;</msg>
<msg timestamp="20231107 17:34:39.208" level="INFO">${output} = 
[1m***********************************************[m
[1mNetprizm CLI Login[m  (Press enter to skip login)
Auto-login successful![m
[m
Welcome: [1mVZ QA[m
Session expires: [1m11-07...</msg>
<status status="PASS" starttime="20231107 17:34:39.007" endtime="20231107 17:34:39.208"/>
</kw>
<msg timestamp="20231107 17:34:39.208" level="INFO">${data} = 
[1m***********************************************[m
[1mNetprizm CLI Login[m  (Press enter to skip login)
Auto-login successful![m
[m
Welcome: [1mVZ QA[m
Session expires: [1m11-07...</msg>
<status status="PASS" starttime="20231107 17:34:39.006" endtime="20231107 17:34:39.208"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>5s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:44.209" level="INFO">Slept 5 seconds</msg>
<status status="PASS" starttime="20231107 17:34:39.209" endtime="20231107 17:34:44.209"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>${SSH_Variables.Modules_Version_Check_Command}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:44.490" level="INFO">v</msg>
<status status="PASS" starttime="20231107 17:34:44.211" endtime="20231107 17:34:44.490"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:44.591" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:44.490" endtime="20231107 17:34:44.591"/>
</kw>
<status status="PASS" starttime="20231107 17:34:44.210" endtime="20231107 17:34:44.591"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:45.592" level="INFO">Slept 1 second</msg>
<status status="PASS" starttime="20231107 17:34:44.592" endtime="20231107 17:34:45.592"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>sys</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<status status="PASS" starttime="20231107 17:34:45.593" endtime="20231107 17:34:45.594"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:45.695" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:45.594" endtime="20231107 17:34:45.695"/>
</kw>
<status status="PASS" starttime="20231107 17:34:45.593" endtime="20231107 17:34:45.695"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:46.696" level="INFO">Slept 1 second</msg>
<status status="PASS" starttime="20231107 17:34:45.695" endtime="20231107 17:34:46.696"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>${SSH_Variables.Modules_Version_Check_Command}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:46.699" level="INFO">[1mRetrieving 'Version' information. Please Wait...[m</msg>
<status status="PASS" starttime="20231107 17:34:46.698" endtime="20231107 17:34:46.699"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:46.800" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:46.700" endtime="20231107 17:34:46.800"/>
</kw>
<status status="PASS" starttime="20231107 17:34:46.696" endtime="20231107 17:34:46.801"/>
</kw>
<kw name="Log To Console" library="BuiltIn">
<arg>${data}</arg>
<doc>Logs the given message to the console.</doc>
<status status="PASS" starttime="20231107 17:34:46.801" endtime="20231107 17:34:46.801"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>datacomplete</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:46.803" level="INFO">Request took .052 seconds on Tue Nov  7 07:04:44 EST 2023.[m</msg>
<status status="PASS" starttime="20231107 17:34:46.802" endtime="20231107 17:34:46.804"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:46.904" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:46.804" endtime="20231107 17:34:46.905"/>
</kw>
<status status="PASS" starttime="20231107 17:34:46.802" endtime="20231107 17:34:46.905"/>
</kw>
<kw name="Read Until Regexp" library="SSHLibrary">
<var>${data1}</var>
<arg>Invalid command:</arg>
<doc>Consumes and returns the server output until a match to ``regexp`` is found.</doc>
<msg timestamp="20231107 17:34:48.065" level="INFO">[1m   NAME               HOST                   VERSION          MESSAGE                       [m
[m   ----------------   --------------------   --------------   ------------------------------[m
   [mCLI_TOOL           10.30.249.96           23.08.16.1       Command Line Interface Tool   [m
   UI_SERVICE         10.30.249.96:9025      23.08.28.1       UI Service for Netprizm       [m
   [mCCL_SERVICE        localhost:9020         23.08.21         CCL Service                   [m
   LMM_SERVICE        localhost:8081         2.6.0            LMM Service                   [m
   [mUEM_SERVICE        localhost:9026         23.07.11         UE Mapper service for Netprizm[m
   NAF_SERVICE        localhost:9019         23.09.11         NAF Service for Netprizm      [m
   [mEM_SERVICE         localhost:9028         23.08.28.1       Execution Manager Service for Netprizm[m
      NAF_SERVICE     localhost:9019         23.09.11         NAF Service for Netprizm      [m
   [m   SD_SERVICE      10.30.249.96:9027      23.08.23         Simulation Data service for Netprizm[m
   AM_SERVICE         ab-ams.vzsme.com:9092   23.07.05         Account Management Service for Netprizm[m


$(home)&gt; sys





[1mCommand Line Interface (CLI) for Netprizm (CLI Version: 23.08.16.1)[m

   HOME  |  [1m[4mSYSTEM[m[m  |  NETWORK  | USE CASE  |  UE  |  TRAFFIC

   System Command   | Description
   ---------------- | ---------------------------------------
    h,   help       | Help
    hh,  health     | Get health information
    mem  memory     | Show current shared memory cache usage
    on,  start      | Start all system components
    off, stop       | Stop all system components
    l,   logs       | Available Logs from previous runs
    s,   status     | Get current status information
    v,   version    | Get version information
    q,   quit       | Quit and exit CLI



$(system)&gt; v

[1mRetrieving 'Systems Version' information. Please Wait...[m
Request took .281 seconds on Tue Nov  7 07:04:47 EST 2023.[m

[1m   NUM  NAME               HOST                    CNTLR VERSION  APP VERSION    [m
[m   ---  ----------------   --------------------    -------------- -------------- [m
   [m  1  CU_01              10.30.249.86:9022       23.08.15       02.05.13       [m[m
     2  PUBSUB_BROKER      10.30.249.96:9033       23.08.13       v23.7.2.1      [m[m
   [m  3  DU_11_RANP_PUBS…   10.30.249.87:9023       23.08.13       v23.7.2.1      [m[m
     4  DU_12_RANP_PUBS…   10.30.249.88:9023       23.08.13       v23.7.2.1      [m[m
   [m  5  CHV_PUBSUB         10.30.249.96:9031       23.08.13       v23.7.2.1      [m[m
     6  UE_01_UEMP_PUBS…   10.30.249.91:9023       23.08.13       v23.7.2.1      [m[m
datacomplete
   [m  7  UE_02_UEMP_PUBS…   10.30.249.92:9023       23.08.13       v23.7.2.1      [m[m
     8  UE_03_UEMP_PUBS…   10.30.249.93:9023       23.08.13       v23.7.2.1      [m[m
   [m  9  UE_04_UEMP_PUBS…   10.30.249.94:9023       23.08.13       v23.7.2.1      [m[m
    10  UE_05_UEMP_PUBS…   10.30.249.89:9023       23.08.13       v23.7.2.1      [m[m
   [m 11  UE_06_UEMP_PUBS…   10.30.249.90:9023       23.08.13       v23.7.2.1      [m[m
    12  UE_01_UEMP_CSM     10.30.249.91:9023       23.08.13       v23.7.2.1      [m[m
   [m 13  UE_02_UEMP_CSM     10.30.249.92:9023       23.08.13       v23.7.2.1      [m[m
    14  UE_03_UEMP_CSM     10.30.249.93:9023       23.08.13       v23.7.2.1      [m[m
   [m 15  UE_04_UEMP_CSM     10.30.249.94:9023       23.08.13       v23.7.2.1      [m[m
    16  UE_05_UEMP_CSM     10.30.249.89:9023       23.08.13       v23.7.2.1      [m[m
   [m 17  UE_06_UEMP_CSM     10.30.249.90:9023       23.08.13       v23.7.2.1      [m[m
    18  CHV_CSM            10.30.249.96:9031       23.08.13       v23.7.2.1      [m[m
   [m 19  DU_11_RANP_CSM     10.30.249.87:9023       23.08.13       v23.7.2.1      [m[m
    20  DU_12_RANP_CSM     10.30.249.88:9023       23.08.13       v23.7.2.1      [m[m
   [m 21  CHV                10.30.249.96:9031       23.08.13       RF_23.8.1.1    [m[m
    22  DU_11_RANP         10.30.249.87:9023       23.08.13       v23.7.2.1      [m[m
   [m 23  DU_12_RANP         10.30.249.88:9023       23.08.13       v23.7.2.1      [m[m
    24  UE_01_UEMP         10.30.249.91:9023       23.08.13       v23.7.2.1      [m[m
   [m 25  UE_02_UEMP         10.30.249.92:9023       23.08.13       v23.7.2.1      [m[m
    26  UE_03_UEMP         10.30.249.93:9023       23.08.13       v23.7.2.1      [m[m
   [m 27  UE_04_UEMP         10.30.249.94:9023       23.08.13       v23.7.2.1      [m[m
    28  UE_05_UEMP         10.30.249.89:9023       23.08.13       v23.7.2.1      [m[m
   [m 29  UE_06_UEMP         10.30.249.90:9023       23.08.13       v23.7.2.1      [m[m
    30  DU_11              10.30.249.87:9021       23.08.15.1     02.05.13       [m[m
   [m 31  DU_12              10.30.249.88:9021       23.08.15.1     02.05.13       [m[m
    32  UE_01              10.30.249.91:9021       23.08.28       03.01.05.01    [m[m
   [m 33  UE_02              10.30.249.92:9021       23.08.28       03.01.05.01    [m[m
    34  UE_03              10.30.249.93:9021       23.08.28       03.01.05.01    [m[m
   [m 35  UE_04              10.30.249.94:9021       23.08.28       03.01.05.01    [m[m
    36  UE_05              10.30.249.89:9021       23.08.28       03.01.05.01    [m[m
   [m 37  UE_06              10.30.249.90:9021       23.08.28       03.01.05.01    [m[m
    38  VATEST             10.30.249.96:9031       23.08.13       RF_23.8.1.1    [m[m



$(system)&gt; datacomplete

[1mInvalid command:</msg>
<msg timestamp="20231107 17:34:48.065" level="INFO">${data1} = 
[1m   NAME               HOST                   VERSION          MESSAGE                       [m
[m   ----------------   --------------------   --------------   ------------------------------...</msg>
<status status="PASS" starttime="20231107 17:34:46.905" endtime="20231107 17:34:48.065"/>
</kw>
<kw name="Create File" library="OperatingSystem">
<arg>${clilogfile}</arg>
<arg>${data1}</arg>
<arg>encoding=UTF-8</arg>
<doc>Creates a file with the given content and encoding.</doc>
<msg timestamp="20231107 17:34:48.066" level="INFO" html="true">Created file '&lt;a href="file:///home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qaextractVersionsInput.txt"&gt;/home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qaextractVersionsInput.txt&lt;/a&gt;'.</msg>
<status status="PASS" starttime="20231107 17:34:48.065" endtime="20231107 17:34:48.066"/>
</kw>
<kw name="Close All Connections" library="SSHLibrary">
<doc>Closes all open connections.</doc>
<status status="PASS" starttime="20231107 17:34:48.066" endtime="20231107 17:34:48.066"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:49.066" level="INFO">Slept 1 second</msg>
<status status="PASS" starttime="20231107 17:34:48.066" endtime="20231107 17:34:49.067"/>
</kw>
<status status="PASS" starttime="20231107 17:34:29.385" endtime="20231107 17:34:49.067"/>
</kw>
<kw name="Core Version" library="resource">
<var>${Core_data}</var>
<arg>${Core_IP}</arg>
<arg>${Password}</arg>
<arg>${User_Name}</arg>
<arg>${Env}</arg>
<kw name="Sleep" library="BuiltIn">
<arg>2s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:51.068" level="INFO">Slept 2 seconds</msg>
<status status="PASS" starttime="20231107 17:34:49.068" endtime="20231107 17:34:51.069"/>
</kw>
<kw name="ssh login" library="resource">
<arg>${Core_IP}</arg>
<arg>${Core_Password}</arg>
<arg>${User_Name}</arg>
<doc>This keyword takes three arguments Server Ip, UserName, Password. This is Used for login remote machine with SSH.</doc>
<kw name="Open Connection" library="SSHLibrary">
<arg>${server_ip1}</arg>
<doc>Opens a new SSH connection to the given ``host`` and ``port``.</doc>
<status status="PASS" starttime="20231107 17:34:51.070" endtime="20231107 17:34:51.071"/>
</kw>
<kw name="Login" library="SSHLibrary">
<arg>${UserName1}</arg>
<arg>${Password1}</arg>
<arg>allow_agent=False</arg>
<arg>look_for_keys=False</arg>
<doc>Logs into the SSH server with the given ``username`` and ``password``.</doc>
<msg timestamp="20231107 17:34:51.072" level="INFO">Logging into '10.30.249.96:22' as 'vzqa'.</msg>
<msg timestamp="20231107 17:34:56.899" level="INFO">Read output: Welcome to Ubuntu 18.04.4 LTS (GNU/Linux 5.4.0-81-lowlatency x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Tue Nov  7 07:04:53 EST 2023

  System load:                    30.92
  Usage of /:                     57.2% of 1.72TB
  Memory usage:                   35%
  Swap usage:                     7%
  Processes:                      1191
  Users logged in:                1
  IP address for eno5:            10.30.249.96
  IP address for docker0:         172.17.0.1
  IP address for br-5f4e93bc1bad: 172.18.0.1
  IP address for br-75a8787c16ae: 172.24.1.1
  IP address for br-2b1a5009ad8b: 172.24.2.1
  IP address for br-9dd2cb5cb370: 172.25.1.1

 * Strictly confined Kubernetes makes edge and IoT secure. Learn how MicroK8s
   just raised the bar for easy, resilient and secure K8s cluster deployment.

   https://ubuntu.com/engage/secure-kubernetes-at-the-edge

 * Canonical Livepatch is available for installation.
   - Reduce system reboots and improve kernel security. Activate at:
     https://ubuntu.com/livepatch

313 packages can be updated.
236 updates are security updates.

New release '20.04.6 LTS' available.
Run 'do-release-upgrade' to upgrade to it.


Last login: Tue Nov  7 07:04:33 2023 from 10.20.251.182
vzqa@as-dl380-r10-12:~$</msg>
<status status="PASS" starttime="20231107 17:34:51.071" endtime="20231107 17:34:56.899"/>
</kw>
<kw name="Set Client Configuration" library="SSHLibrary">
<arg>timeout=2 min</arg>
<doc>Update the `configuration` of the current connection.</doc>
<status status="PASS" starttime="20231107 17:34:56.900" endtime="20231107 17:34:56.900"/>
</kw>
<status status="PASS" starttime="20231107 17:34:51.069" endtime="20231107 17:34:56.900"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.3</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:57.201" level="INFO">Slept 300 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:56.900" endtime="20231107 17:34:57.201"/>
</kw>
<kw name="Login As User In ssh" library="resource">
<arg>${Core_SSH_Variables.Command_For_ChangeUser}</arg>
<arg>${User_Name}</arg>
<arg>${Core_Password}</arg>
<doc>With the help of this keyword  we login remote machine as user like root , vzqa etc. after this its validate the user also after login.
It takes two arguments 1.UserName, 2. Command for validate user</doc>
<kw name="Write Command With SSH" library="resource">
<arg>${command for enter as user}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:57.438" level="INFO">sudo -i</msg>
<status status="PASS" starttime="20231107 17:34:57.204" endtime="20231107 17:34:57.438"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:57.538" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:57.438" endtime="20231107 17:34:57.539"/>
</kw>
<status status="PASS" starttime="20231107 17:34:57.203" endtime="20231107 17:34:57.539"/>
</kw>
<kw name="Log To Console" library="BuiltIn">
<arg>${command for enter as user}</arg>
<doc>Logs the given message to the console.</doc>
<status status="PASS" starttime="20231107 17:34:57.539" endtime="20231107 17:34:57.539"/>
</kw>
<kw name="Read Output Of Command with SSH" library="resource">
<var>${userdata}</var>
<doc>This Keyword is using for read the any command output or console log of remote machine with SSH.</doc>
<kw name="Read" library="SSHLibrary">
<var>${output}</var>
<arg>delay=.2s</arg>
<doc>Consumes and returns everything available on the server output.</doc>
<msg timestamp="20231107 17:34:57.942" level="INFO">root@as-dl380-r10-12:~#</msg>
<msg timestamp="20231107 17:34:57.942" level="INFO">${output} = root@as-dl380-r10-12:~# </msg>
<status status="PASS" starttime="20231107 17:34:57.540" endtime="20231107 17:34:57.942"/>
</kw>
<msg timestamp="20231107 17:34:57.942" level="INFO">${userdata} = root@as-dl380-r10-12:~# </msg>
<status status="PASS" starttime="20231107 17:34:57.540" endtime="20231107 17:34:57.942"/>
</kw>
<kw name="Log To Console" library="BuiltIn">
<arg>${userdata}</arg>
<doc>Logs the given message to the console.</doc>
<status status="PASS" starttime="20231107 17:34:57.943" endtime="20231107 17:34:57.943"/>
</kw>
<kw name="Set Variable" library="BuiltIn">
<var>${userdata1}</var>
<arg>${userdata}</arg>
<doc>Returns the given values which can then be assigned to a variables.</doc>
<msg timestamp="20231107 17:34:57.944" level="INFO">${userdata1} = root@as-dl380-r10-12:~# </msg>
<status status="PASS" starttime="20231107 17:34:57.943" endtime="20231107 17:34:57.944"/>
</kw>
<kw name="Run Keyword And Return Status" library="BuiltIn">
<var>${ad}</var>
<arg>Should Contain</arg>
<arg>${userdata1}</arg>
<arg>Password:</arg>
<doc>Runs the given keyword with given arguments and returns the status as a Boolean value.</doc>
<kw name="Should Contain" library="BuiltIn">
<arg>${userdata1}</arg>
<arg>Password:</arg>
<doc>Fails if ``container`` does not contain ``item`` one or more times.</doc>
<msg timestamp="20231107 17:34:57.944" level="FAIL">'root@as-dl380-r10-12:~# ' does not contain 'Password:'</msg>
<status status="FAIL" starttime="20231107 17:34:57.944" endtime="20231107 17:34:57.945"/>
</kw>
<msg timestamp="20231107 17:34:57.945" level="INFO">${ad} = False</msg>
<status status="PASS" starttime="20231107 17:34:57.944" endtime="20231107 17:34:57.945"/>
</kw>
<if>
<branch type="IF" condition="$ad">
<kw name="Write Command With SSH" library="resource">
<arg>${Password}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<status status="NOT RUN" starttime="20231107 17:34:57.945" endtime="20231107 17:34:57.945"/>
</kw>
<status status="NOT RUN" starttime="20231107 17:34:57.945" endtime="20231107 17:34:57.945"/>
</branch>
<status status="PASS" starttime="20231107 17:34:57.945" endtime="20231107 17:34:57.946"/>
</if>
<status status="PASS" starttime="20231107 17:34:57.201" endtime="20231107 17:34:57.946"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>cd cluster</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:58.311" level="INFO">cd cluster</msg>
<status status="PASS" starttime="20231107 17:34:57.946" endtime="20231107 17:34:58.311"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:58.411" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:58.311" endtime="20231107 17:34:58.411"/>
</kw>
<status status="PASS" starttime="20231107 17:34:57.946" endtime="20231107 17:34:58.412"/>
</kw>
<kw name="Catenate" library="BuiltIn">
<var>${podsourcecommand}</var>
<arg>SEPARATOR=</arg>
<arg>source np2-</arg>
<arg>${Env}</arg>
<arg>.sh</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:34:58.412" level="INFO">${podsourcecommand} = source np2-qa.sh</msg>
<status status="PASS" starttime="20231107 17:34:58.412" endtime="20231107 17:34:58.412"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>${podsourcecommand}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:58.649" level="INFO">root@as-dl380-r10-12:~/cluster# source np2-qa.sh</msg>
<status status="PASS" starttime="20231107 17:34:58.413" endtime="20231107 17:34:58.649"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:58.749" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:58.649" endtime="20231107 17:34:58.750"/>
</kw>
<status status="PASS" starttime="20231107 17:34:58.413" endtime="20231107 17:34:58.750"/>
</kw>
<kw name="Catenate" library="BuiltIn">
<var>${podcommand}</var>
<arg>SEPARATOR=</arg>
<arg>kubectl describe pods -n np2-</arg>
<arg>${Env}</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:34:58.751" level="INFO">${podcommand} = kubectl describe pods -n np2-qa</msg>
<status status="PASS" starttime="20231107 17:34:58.750" endtime="20231107 17:34:58.751"/>
</kw>
<kw name="Write Command With SSH" library="resource">
<arg>${podcommand}</arg>
<doc>This Keyword is using for write any command on remote machine console with SSh.
It takes One argument 1.Command</doc>
<kw name="Write" library="SSHLibrary">
<arg>${command}</arg>
<doc>Writes the given ``text`` on the remote machine and appends a newline.</doc>
<msg timestamp="20231107 17:34:58.872" level="INFO">Switched to context "np2-qa".</msg>
<status status="PASS" starttime="20231107 17:34:58.751" endtime="20231107 17:34:58.872"/>
</kw>
<kw name="Sleep" library="BuiltIn">
<arg>.1s</arg>
<doc>Pauses the test executed for the given time.</doc>
<msg timestamp="20231107 17:34:58.972" level="INFO">Slept 100 milliseconds</msg>
<status status="PASS" starttime="20231107 17:34:58.872" endtime="20231107 17:34:58.973"/>
</kw>
<status status="PASS" starttime="20231107 17:34:58.751" endtime="20231107 17:34:58.973"/>
</kw>
<kw name="Read" library="SSHLibrary">
<var>${podslog}</var>
<arg>delay=4s</arg>
<doc>Consumes and returns everything available on the server output.</doc>
<msg timestamp="20231107 17:35:06.979" level="INFO">root@as-dl380-r10-12:~/cluster# kubectl describe pods -n np2-qa
Name:         np2-qa-helm-af-78b8598d95-2jnzm
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-af
              pod-template-hash=78b8598d95
Annotations:  cni.projectcalico.org/containerID: 656b065e9134d3cc88ed55c29bcbaacd3f9c0ae84ab173d98f737205fdf831e9
              cni.projectcalico.org/podIP: 192.168.163.146/32
              cni.projectcalico.org/podIPs: 192.168.163.146/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.146
IPs:
  IP:           192.168.163.146
Controlled By:  ReplicaSet/np2-qa-helm-af-78b8598d95
Containers:
  af-chart:
    Container ID:  containerd://62ec899d72a7152a1c38e64aed53d03b5cd411a292fc92cfa3ebf56620f89b7f
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/af:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/af@sha256:8d165883b9f546265580b29204ceb188f5b0fb158b5fdbfb3c81bb059afbc63e
    Port:          8035/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_af.sh; echo af TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:00 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from af-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tc7qh (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      af-configmap
    Optional:  false
  af-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  af-pv-claim
    ReadOnly:   false
  kube-api-access-tc7qh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-amf-998bcfcb4-cn8gj
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 11 Oct 2023 08:00:01 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-amf
              pod-template-hash=998bcfcb4
Annotations:  cni.projectcalico.org/containerID: 2f20e17046bb2baeb88b9578c0eb2ec4c6d461f27d4177ebafe1fa090365a2bf
              cni.projectcalico.org/podIP: 192.168.163.185/32
              cni.projectcalico.org/podIPs: 192.168.163.185/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.185
IPs:
  IP:           192.168.163.185
Controlled By:  ReplicaSet/np2-qa-helm-amf-998bcfcb4
Containers:
  amf-chart:
    Container ID:  containerd://3038fe42fa9348f8a69f06814f45daf4340c8badf068883ebb3573ec550207b8
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/amf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/amf@sha256:95512263d4ac18f3eef0340f1ca9e8216a3c85950a6457e2767aa4b2160e8041
    Port:          29518/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_amf.sh; echo amf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Wed, 11 Oct 2023 08:00:02 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/log/ from amf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-529bl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      amf-configmap
    Optional:  false
  amf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  amf-pv-claim
    ReadOnly:   false
  kube-api-access-529bl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-ausf-65c4c5588-886hn
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-ausf
              pod-template-hash=65c4c5588
Annotations:  cni.projectcalico.org/containerID: 75a3c3d4a89c5ab510bc25dcb2dcdef074e3c2eb90c30294595bb3845d41354f
              cni.projectcalico.org/podIP: 192.168.163.159/32
              cni.projectcalico.org/podIPs: 192.168.163.159/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.159
IPs:
  IP:           192.168.163.159
Controlled By:  ReplicaSet/np2-qa-helm-ausf-65c4c5588
Init Containers:
  init-postgres:
    Container ID:  containerd://5ecdaf15b08d4c8331049121f8890cd68eb239aaa2a4cbd703c56fba6abc44b5
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf@sha256:5d2d61ab18a548a909ff37f8cec61da9dcdb9aeded079a0db9052e9bbb146b8d
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 06 Oct 2023 09:58:00 -0400
      Finished:     Fri, 06 Oct 2023 09:58:00 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jr4rv (ro)
Containers:
  ausf-chart:
    Container ID:  containerd://850018e11db968f7406ffea2ab3dc3eb44206d282df35cef18fd8ff5c3d936a0
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf@sha256:5d2d61ab18a548a909ff37f8cec61da9dcdb9aeded079a0db9052e9bbb146b8d
    Port:          8000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_ausf.sh; echo ausf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from ausf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jr4rv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      ausf-configmap
    Optional:  false
  ausf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  ausf-pv-claim
    ReadOnly:   false
  kube-api-access-jr4rv:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-bsf-7c49c964db-srgm7
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-bsf
              pod-template-hash=7c49c964db
Annotations:  cni.projectcalico.org/containerID: 3845a32f31242d19d8c1805b605a87a5a61a371766044ec270975b77485a4d69
              cni.projectcalico.org/podIP: 192.168.163.190/32
              cni.projectcalico.org/podIPs: 192.168.163.190/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.190
IPs:
  IP:           192.168.163.190
Controlled By:  ReplicaSet/np2-qa-helm-bsf-7c49c964db
Init Containers:
  init-postgres:
    Container ID:  containerd://c67fbfbbcf7f2ddbe7af46fb0d7d1d1ff07a0fe45d968595cc4d97f11fe1f4f1
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf@sha256:0187142e3ec679fb4b97b70f17c79bf3420f3720735fecb62101d7089bf03938
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 06 Oct 2023 09:58:00 -0400
      Finished:     Fri, 06 Oct 2023 09:58:00 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-n4vd4 (ro)
Containers:
  bsf-chart:
    Container ID:  containerd://59a3bbcdad9282c1003339eb5ca139aac4fc740900b7a362ada2c0503fd6b837
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/bsf@sha256:0187142e3ec679fb4b97b70f17c79bf3420f3720735fecb62101d7089bf03938
    Port:          11000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_bsf.sh; echo bsf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from bsf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-n4vd4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      bsf-configmap
    Optional:  false
  bsf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  bsf-pv-claim
    ReadOnly:   false
  kube-api-access-n4vd4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-nef-6d5df896d8-58lmq
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-nef
              pod-template-hash=6d5df896d8
Annotations:  cni.projectcalico.org/containerID: d5d2f42f52225c09a72c3aca56c7a0f72c168b9d394197cc5a41248b08b6f288
              cni.projectcalico.org/podIP: 192.168.163.147/32
              cni.projectcalico.org/podIPs: 192.168.163.147/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.147
IPs:
  IP:           192.168.163.147
Controlled By:  ReplicaSet/np2-qa-helm-nef-6d5df896d8
Containers:
  nef-chart:
    Container ID:  containerd://699ac1ea50f32aa055f7833b1490288af3d9048b7653a7ee0f0ff2a1cbc69c67
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nef:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nef@sha256:296c084777712c5c13354ad44f69fd4e85f47c9a354c79d8d431da5a8f6f099f
    Port:          10000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nef.sh; echo nef TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:00 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nef-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-kgjpr (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nef-configmap
    Optional:  false
  nef-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nef-pv-claim
    ReadOnly:   false
  kube-api-access-kgjpr:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-nrf-7f48b47b96-zgnqx
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-nrf
              pod-template-hash=7f48b47b96
Annotations:  cni.projectcalico.org/containerID: cd792bc8c0d85b5476637043036f9e9a853ee7213f9be10bd46b4f78310046f2
              cni.projectcalico.org/podIP: 192.168.163.141/32
              cni.projectcalico.org/podIPs: 192.168.163.141/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.141
IPs:
  IP:           192.168.163.141
Controlled By:  ReplicaSet/np2-qa-helm-nrf-7f48b47b96
Init Containers:
  init-postgres:
    Container ID:  containerd://1539e7eee41d4fa5e2dfffbb6fca825999be0987ee58773cce0746237fd7101a
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf@sha256:3a842d17256f894c17a881b3df18533e669a9053b0da18682cc31cf9c0eaadfd
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 06 Oct 2023 09:58:00 -0400
      Finished:     Fri, 06 Oct 2023 09:58:00 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rcp7g (ro)
Containers:
  nrf-chart:
    Container ID:  containerd://08a720b156e311c8721c3d2136669055ae272db90a035b689c13e29b2cad5e2b
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf@sha256:3a842d17256f894c17a881b3df18533e669a9053b0da18682cc31cf9c0eaadfd
    Port:          10500/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nrf.sh; echo nrf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nrf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rcp7g (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nrf-configmap
    Optional:  false
  nrf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nrf-pv-claim
    ReadOnly:   false
  kube-api-access-rcp7g:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-nsacf-55797458bf-kpv8v
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-nsacf
              pod-template-hash=55797458bf
Annotations:  cni.projectcalico.org/containerID: 147fc71c6f7c3d4912f61808b87782d73cba98376b6ce9c07adc5b96bde5bd92
              cni.projectcalico.org/podIP: 192.168.163.139/32
              cni.projectcalico.org/podIPs: 192.168.163.139/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.139
IPs:
  IP:           192.168.163.139
Controlled By:  ReplicaSet/np2-qa-helm-nsacf-55797458bf
Init Containers:
  init-postgres:
    Container ID:  containerd://2c313d252f8fb0663cbce4a03819ac2b8027b4da1143532ec517c6eee66dd75b
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf@sha256:7fde39b7fb051f51aeaf6a98f6143e58d1d2e2cd7276d38d29189ed7a6d1b1f0
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 06 Oct 2023 09:58:00 -0400
      Finished:     Fri, 06 Oct 2023 09:58:00 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-grbjc (ro)
Containers:
  nsacf-chart:
    Container ID:  containerd://301c6e42778b4b492387491821dbbed7a5c246c5958197d2226277adc38b6717
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nsacf@sha256:7fde39b7fb051f51aeaf6a98f6143e58d1d2e2cd7276d38d29189ed7a6d1b1f0
    Port:          8009/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nsacf.sh; echo nsacf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:02 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nsacf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-grbjc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nsacf-configmap
    Optional:  false
  nsacf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nsacf-pv-claim
    ReadOnly:   false
  kube-api-access-grbjc:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-nssaaf-869d47c478-2lcqb
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-nssaaf
              pod-template-hash=869d47c478
Annotations:  cni.projectcalico.org/containerID: 322f9276a78866cf63039f016ec986fe12c3d54716757575b3691df887d159d2
              cni.projectcalico.org/podIP: 192.168.163.150/32
              cni.projectcalico.org/podIPs: 192.168.163.150/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.150
IPs:
  IP:           192.168.163.150
Controlled By:  ReplicaSet/np2-qa-helm-nssaaf-869d47c478
Containers:
  nssaaf-chart:
    Container ID:  containerd://9cd0bf3cc8908cfc36d525805484d453806a73f565ea67c9439d4a21dcba3a98
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssaaf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssaaf@sha256:0a27fc0914434ab7b4f70fbe138f6baa78878e0abac665768c6e6f34268f07b6
    Port:          80/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nssaaf.sh; echo nssaaf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Tue, 07 Nov 2023 01:01:45 -0500
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Mon, 06 Nov 2023 15:01:41 -0500
      Finished:     Tue, 07 Nov 2023 01:01:43 -0500
    Ready:          True
    Restart Count:  76
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nssaaf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-v8s6x (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nssaaf-configmap
    Optional:  false
  nssaaf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nssaaf-pv-claim
    ReadOnly:   false
  kube-api-access-v8s6x:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-nssf-768ff4cdd4-2f56h
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-nssf
              pod-template-hash=768ff4cdd4
Annotations:  cni.projectcalico.org/containerID: bda5f024d1299502af22464f02ed82ef6a649f1569159fc0a6c89e76d1b63340
              cni.projectcalico.org/podIP: 192.168.163.144/32
              cni.projectcalico.org/podIPs: 192.168.163.144/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.144
IPs:
  IP:           192.168.163.144
Controlled By:  ReplicaSet/np2-qa-helm-nssf-768ff4cdd4
Init Containers:
  init-postgres:
    Container ID:  containerd://bf7f586cda38ceb09b56964e151a0417b7ab721f9bd1aa2adc30844727a9a515
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf@sha256:1e8f54691b1a09f98ce720a96fbe516dec5607baed45d0ae84ff671c79a10b31
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
      Finished:     Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tqc2t (ro)
Containers:
  nssf-chart:
    Container ID:  containerd://fc9e8db6e156c8ecda992f9cc0c0d6006ed4e952a729e04538e8e7dbb399d672
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nssf@sha256:1e8f54691b1a09f98ce720a96fbe516dec5607baed45d0ae84ff671c79a10b31
    Port:          29520/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_nssf.sh; echo nssf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:02 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from nssf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tqc2t (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      nssf-configmap
    Optional:  false
  nssf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  nssf-pv-claim
    ReadOnly:   false
  kube-api-access-tqc2t:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-pcf-687b4d5977-njfhh
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:57:59 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-pcf
              pod-template-hash=687b4d5977
Annotations:  cni.projectcalico.org/containerID: 5bb4f5bdc784acade7b5d1ecaf2e200a00ddf8fb46d9ac10871b922ac2938582
              cni.projectcalico.org/podIP: 192.168.163.187/32
              cni.projectcalico.org/podIPs: 192.168.163.187/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.187
IPs:
  IP:           192.168.163.187
Controlled By:  ReplicaSet/np2-qa-helm-pcf-687b4d5977
Containers:
  pcf-chart:
    Container ID:  containerd://f8c11126c5b47a1d06202c3b7d9f0a4224aea076d9624bf7c9d5bbbb25a0f7d1
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/pcf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/pcf@sha256:a0e1187d13485b20b1c27dfae4008fd42624b762db95274c15882ff2b0e8d3b4
    Port:          9001/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_pcf.sh; echo pcf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from pcf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h78g4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      pcf-configmap
    Optional:  false
  pcf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  pcf-pv-claim
    ReadOnly:   false
  kube-api-access-h78g4:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-rx-c8f646774-kkg5r
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:58:00 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-rx
              pod-template-hash=c8f646774
Annotations:  cni.projectcalico.org/containerID: ba2bb1a16ae02f92ddc1329a6070c230df1dd070d5e17c96aebc50b3acef0c85
              cni.projectcalico.org/podIP: 192.168.163.154/32
              cni.projectcalico.org/podIPs: 192.168.163.154/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.154
IPs:
  IP:           192.168.163.154
Controlled By:  ReplicaSet/np2-qa-helm-rx-c8f646774
Containers:
  rx-chart:
    Container ID:   containerd://01f995d8fa3b542821322ed153e4d714322880f7528c71aad598a5cd685aa4aa
    Image:          registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/rx:23.2.6.1
    Image ID:       registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/rx@sha256:de2ed664a21fb5f0b761004c855a895363e18eb0ac7f89194a0c452513bc4fd4
    Port:           3868/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qz6pw (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      rx-configmap
    Optional:  false
  rx-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  rx-pv-claim1
    ReadOnly:   false
  kube-api-access-qz6pw:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-smf-86bbbf97db-wmjdm
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 11 Oct 2023 08:00:01 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-smf
              pod-template-hash=86bbbf97db
Annotations:  cni.projectcalico.org/containerID: 00d98653a53e5679691d7b9d30ae9db6ae16ec835015ba47add4bb348ec9b91c
              cni.projectcalico.org/podIP: 192.168.163.184/32
              cni.projectcalico.org/podIPs: 192.168.163.184/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.184
IPs:
  IP:           192.168.163.184
Controlled By:  ReplicaSet/np2-qa-helm-smf-86bbbf97db
Init Containers:
  init-redis:
    Container ID:  containerd://ac124639cdcbbb46f6109f66d19caa54ff6b7a479ac7394506dc5d051af4f41c
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf@sha256:dfe3ad0af411fc8a1ffe1cba3ca10d4f30add3d64e40831470b5636529986e01
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until redis-cli -h redis-service ping;do echo waiting for redis database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Wed, 11 Oct 2023 08:00:02 -0400
      Finished:     Wed, 11 Oct 2023 08:00:02 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t7bz9 (ro)
Containers:
  smf-chart:
    Container ID:  containerd://1287162aec9d762c982c87e5d29f09e559bd736de8e11e034251de79f1150adc
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf@sha256:dfe3ad0af411fc8a1ffe1cba3ca10d4f30add3d64e40831470b5636529986e01
    Port:          29519/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_smf.sh; echo smf TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 03 Nov 2023 23:50:53 -0400
    Last State:     Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 27 Oct 2023 09:59:20 -0400
      Finished:     Fri, 03 Nov 2023 23:50:53 -0400
    Ready:          True
    Restart Count:  4
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/log/ from smf-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-t7bz9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      smf-configmap
    Optional:  false
  smf-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  smf-pv-claim
    ReadOnly:   false
  kube-api-access-t7bz9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-udm-55487f7687-hgl96
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:58:00 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-udm
              pod-template-hash=55487f7687
Annotations:  cni.projectcalico.org/containerID: 2ce6bc3034c49c0ee9e6c2dc971ebc7ed111b73da89d297d9916308763ea9c96
              cni.projectcalico.org/podIP: 192.168.163.133/32
              cni.projectcalico.org/podIPs: 192.168.163.133/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.133
IPs:
  IP:           192.168.163.133
Controlled By:  ReplicaSet/np2-qa-helm-udm-55487f7687
Containers:
  udm-chart:
    Container ID:  containerd://b14106932538f663dff47d00889ccaf0cb8c90a34559ef901098021b44815854
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udm:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udm@sha256:18f35916565fc89694b0d46234f6544f83134660f19df6dd715e53b00eb9d0be
    Port:          9000/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_udm.sh; echo udm TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:01 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from udm-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vcp44 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      udm-configmap
    Optional:  false
  udm-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  udm-pv-claim
    ReadOnly:   false
  kube-api-access-vcp44:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         np2-qa-helm-udr-796b89f9d5-9z8bs
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Fri, 06 Oct 2023 09:58:00 -0400
Labels:       app.kubernetes.io/instance=np2-qa
              app.kubernetes.io/name=helm-udr
              pod-template-hash=796b89f9d5
Annotations:  cni.projectcalico.org/containerID: ce1e5860d6ed2f4be56cad2613f3e50f5290f0cbaca787b773af943506e140a2
              cni.projectcalico.org/podIP: 192.168.163.188/32
              cni.projectcalico.org/podIPs: 192.168.163.188/32
              kubectl.kubernetes.io/restartedAt: 2023-10-06T09:57:59-04:00
Status:       Running
IP:           192.168.163.188
IPs:
  IP:           192.168.163.188
Controlled By:  ReplicaSet/np2-qa-helm-udr-796b89f9d5
Init Containers:
  init-postgres:
    Container ID:  containerd://a2dcaf12ba9f26c74cb5fe63e9cb9a508d163e9fa1d17799045a18c753a93e0f
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr@sha256:bb22b6ac84fdafce895cca94e6025495713cac590af0ea65a023b537862774d0
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      until pg_isready -h postgres-service -p 5432;do echo waiting for database; sleep 1; done;
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Fri, 06 Oct 2023 09:58:02 -0400
      Finished:     Fri, 06 Oct 2023 09:58:02 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qtshf (ro)
Containers:
  udr-chart:
    Container ID:  containerd://120dd602a2bf4052eaf5b817d8761b9da5b230868b9f1ee1303e2c74f644a7c4
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr@sha256:bb22b6ac84fdafce895cca94e6025495713cac590af0ea65a023b537862774d0
    Port:          8005/TCP
    Host Port:     0/TCP
    Command:
      bash
    Args:
      -c
      ./start_udr.sh; echo udr TERMINATED. WILL RESTART IN 10 hours. Start debugging; sleep 10h;
    State:          Running
      Started:      Fri, 06 Oct 2023 09:58:03 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app/config from config-volume (rw)
      /app/logs/ from udr-pv-logs (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qtshf (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      udr-configmap
    Optional:  false
  udr-pv-logs:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  udr-pv-claim
    ReadOnly:   false
  kube-api-access-qtshf:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         postgres-statefulset-0
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 27 Sep 2023 10:58:31 -0400
Labels:       app=postgres-app
              controller-revision-hash=postgres-statefulset-749fc55f4b
              statefulset.kubernetes.io/pod-name=postgres-statefulset-0
Annotations:  cni.projectcalico.org/containerID: 1664871878a9adba2187583bb5663578efd09be5093a3cf35bd91461fa40f963
              cni.projectcalico.org/podIP: 192.168.163.179/32
              cni.projectcalico.org/podIPs: 192.168.163.179/32
Status:       Running
IP:           192.168.163.179
IPs:
  IP:           192.168.163.179
Controlled By:  StatefulSet/postgres-statefulset
Containers:
  postgres:
    Container ID:  containerd://7b93a262c3641bca3de99cd37a111a53c559db4271b9e33a28bea76a4bbd4e10
    Image:         postgres:13.0
    Image ID:      docker.io/library/postgres@sha256:8f7c3c9b61d82a4a021da5d9618faf056633e089302a726d619fa467c73609e4
    Port:          5432/TCP
    Host Port:     0/TCP
    Args:
      -c
      max_connections=1000
      -c
      shared_buffers=1024MB
    State:          Running
      Started:      Wed, 27 Sep 2023 10:58:32 -0400
    Ready:          True
    Restart Count:  0
    Limits:
      hugepages-1Gi:  2Gi
      memory:         1Gi
    Requests:
      hugepages-1Gi:  2Gi
      memory:         1Gi
    Environment:
      POSTGRES_PASSWORD:          
      POSTGRES_HOST_AUTH_METHOD:  trust
    Mounts:
      /var/lib/postgresql/data from postgres-pv-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-z6ct2 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  postgres-pv-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  postgres-pv-claim
    ReadOnly:   false
  kube-api-access-z6ct2:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         prometheus-statefulset-0
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 27 Sep 2023 10:58:30 -0400
Labels:       app=prometheus-server
              controller-revision-hash=prometheus-statefulset-76b64d8c6c
              statefulset.kubernetes.io/pod-name=prometheus-statefulset-0
Annotations:  cni.projectcalico.org/containerID: 5c0ecc58cf06f1cfeb0217359df17e6a7537dc8ae62a715a09794c2ec068a76f
              cni.projectcalico.org/podIP: 192.168.163.136/32
              cni.projectcalico.org/podIPs: 192.168.163.136/32
Status:       Running
IP:           192.168.163.136
IPs:
  IP:           192.168.163.136
Controlled By:  StatefulSet/prometheus-statefulset
Containers:
  prometheus:
    Container ID:  containerd://00b682beafc7b3223fb5250da57b748f752baf310bbd34feea0e15e5533271cf
    Image:         prom/prometheus:latest
    Image ID:      docker.io/prom/prometheus@sha256:c5dd3503828713c4949ae1bccd1d8d69f382c33d441954674a6b78ebe69c3331
    Port:          9090/TCP
    Host Port:     0/TCP
    Args:
      --config.file=/etc/config/prometheus.yml
      --storage.tsdb.path=/prometheus/
      --storage.tsdb.retention=3d
    State:          Running
      Started:      Wed, 27 Sep 2023 10:58:32 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /etc/config from config-vol (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5v7j9 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-vol:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      prometheus-server-conf
    Optional:  false
  kube-api-access-5v7j9:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         provisioning-statefulset-0
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 27 Sep 2023 10:58:30 -0400
Labels:       app=provisioning-app
              controller-revision-hash=provisioning-statefulset-5bc95db44d
              statefulset.kubernetes.io/pod-name=provisioning-statefulset-0
Annotations:  cni.projectcalico.org/containerID: 5ed46ac2899b849096b399e4522780e8a9121c11988de4dba5ee2af8c3451b64
              cni.projectcalico.org/podIP: 192.168.163.161/32
              cni.projectcalico.org/podIPs: 192.168.163.161/32
Status:       Running
IP:           192.168.163.161
IPs:
  IP:           192.168.163.161
Controlled By:  StatefulSet/provisioning-statefulset
Containers:
  provisioning:
    Container ID:  containerd://1cf2723786f084a13ed3e68cea1460c70f48b7ed2a665f962961da50c767f4c0
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/provisioning:23.2.6.1
    Image ID:      registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/provisioning@sha256:02368a6aac62764a761f5f88ab8c0f4bd35601dedfe8e92975699de7d859d6d8
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      bash
    Args:
      -c
      /defaults/start_provisioning.sh; tail -f /dev/null
    State:          Running
      Started:      Wed, 27 Sep 2023 10:58:31 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /app from provisioning-pv-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gxc9f (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  provisioning-pv-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  provisioning-pv-claim
    ReadOnly:   false
  kube-api-access-gxc9f:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         pushgateway-statefulset-0
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 27 Sep 2023 10:58:30 -0400
Labels:       app=pushgateway-server
              controller-revision-hash=pushgateway-statefulset-75544d5988
              statefulset.kubernetes.io/pod-name=pushgateway-statefulset-0
Annotations:  cni.projectcalico.org/containerID: cf24638e5c1fcc1c21143033b805de298aaf3dddf49f4003b966325d935482af
              cni.projectcalico.org/podIP: 192.168.163.137/32
              cni.projectcalico.org/podIPs: 192.168.163.137/32
Status:       Running
IP:           192.168.163.137
IPs:
  IP:           192.168.163.137
Controlled By:  StatefulSet/pushgateway-statefulset
Containers:
  pushgateway:
    Container ID:   containerd://9ce9faef48e9a1795234789d7c2931b388bb598675bfd47b128fb8ae3764b48f
    Image:          prom/pushgateway:latest
    Image ID:       docker.io/prom/pushgateway@sha256:979a69ab4a4016c89f2b1c53dacaf6190cd676c9d55f7659aabdd208ba48b7c7
    Port:           9091/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Wed, 27 Sep 2023 10:58:32 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vc7jl (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-vc7jl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;


Name:         redis-statefulset-0
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.30.249.85
Start Time:   Wed, 27 Sep 2023 10:58:53 -0400
Labels:       app=redis-app
              controller-revision-hash=redis-statefulset-7dbc765fbb
              statefulset.kubernetes.io/pod-name=redis-statefulset-0
Annotations:  cni.projectcalico.org/containerID: 39c0166fc65b3903149a79efa62faf0463cc15a3270f99fb08b563a9b3d4bbe7
              cni.projectcalico.org/podIP: 192.168.163.170/32
              cni.projectcalico.org/podIPs: 192.168.163.170/32
Status:       Running
IP:           192.168.163.170
IPs:
  IP:           192.168.163.170
Controlled By:  StatefulSet/redis-statefulset
Containers:
  redis:
    Container ID:  containerd://55b0f0508bffe93e312c1efed9a75b8b5b5a6a2f2de9b859d0c9a2d57f4bbf34
    Image:         redis:6.0.8
    Image ID:      docker.io/library/redis@sha256:21db12e5ab3cc343e9376d655e8eabbdbe5516801373e95a8a9e66010c5b8819
    Port:          6379/TCP
    Host Port:     0/TCP
    Args:
      redis-server
      --appendonly
      yes
    State:          Running
      Started:      Wed, 27 Sep 2023 10:58:54 -0400
    Ready:          True
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:
      /data/ from redis-pv-data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-r4rkk (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  redis-pv-data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  redis-pv-claim
    ReadOnly:   false
  kube-api-access-r4rkk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       &lt;nil&gt;
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=as-dl360-r10-01.netprizm.local
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      &lt;none&gt;
root@as-dl380-r10-12:~/cluster#</msg>
<msg timestamp="20231107 17:35:06.981" level="INFO">${podslog} = root@as-dl380-r10-12:~/cluster# kubectl describe pods -n np2-qa
Name:         np2-qa-helm-af-78b8598d95-2jnzm
Namespace:    np2-qa
Priority:     0
Node:         as-dl360-r10-01.netprizm.local/10.3...</msg>
<status status="PASS" starttime="20231107 17:34:58.973" endtime="20231107 17:35:06.981"/>
</kw>
<kw name="Catenate" library="BuiltIn">
<var>${corelogfile}</var>
<arg>${EXECDIR}/${Env}coreversion.txt</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:35:06.982" level="INFO">${corelogfile} = /home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qacoreversion.txt</msg>
<status status="PASS" starttime="20231107 17:35:06.981" endtime="20231107 17:35:06.982"/>
</kw>
<kw name="Create File" library="OperatingSystem">
<arg>${corelogfile}</arg>
<arg>${podslog}</arg>
<arg>encoding=UTF-8</arg>
<doc>Creates a file with the given content and encoding.</doc>
<msg timestamp="20231107 17:35:06.983" level="INFO" html="true">Created file '&lt;a href="file:///home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qacoreversion.txt"&gt;/home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qacoreversion.txt&lt;/a&gt;'.</msg>
<status status="PASS" starttime="20231107 17:35:06.982" endtime="20231107 17:35:06.983"/>
</kw>
<kw name="Getversioncore" library="ParseInJsonV1">
<var>${coremoduledict}</var>
<arg>${corelogfile}</arg>
<msg timestamp="20231107 17:35:06.985" level="INFO">Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/amf:23.2.6.1

/amf: 23.2.6.1

{'AMF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf:23.2.6.1

/ausf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/ausf:23.2.6.1

/ausf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf:23.2.6.1

/nrf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/nrf:23.2.6.1

/nrf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/pcf:23.2.6.1

/pcf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf:23.2.6.1

/smf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/smf:23.2.6.1

/smf: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udm:23.2.6.1

/udm: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1', 'UDM_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr:23.2.6.1

/udr: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1', 'UDM_Version': '23.2.6.1', 'UDR_Version': '23.2.6.1'}
    Image:         registry.vzsme.com:5050/sfdevops/network/5g/core/amantya/ops-helm/udr:23.2.6.1

/udr: 23.2.6.1

{'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1', 'UDM_Version': '23.2.6.1', 'UDR_Version': '23.2.6.1'}
core modele dict :  {'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1', 'UDM_Version': '23.2.6.1', 'UDR_Version': '23.2.6.1'}</msg>
<msg timestamp="20231107 17:35:06.985" level="INFO">${coremoduledict} = {'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1', 'UDM_Version': '23.2.6.1', 'UDR_Version': '23.2.6.1'}</msg>
<status status="PASS" starttime="20231107 17:35:06.983" endtime="20231107 17:35:06.985"/>
</kw>
<kw name="Log To Console" library="BuiltIn">
<arg>${coremoduledict}</arg>
<doc>Logs the given message to the console.</doc>
<status status="PASS" starttime="20231107 17:35:06.985" endtime="20231107 17:35:06.985"/>
</kw>
<msg timestamp="20231107 17:35:06.985" level="INFO">${Core_data} = {'AMF_Version': '23.2.6.1', 'AUSF_Version': '23.2.6.1', 'NRF_Version': '23.2.6.1', 'PCF_Version': '23.2.6.1', 'SMF_Version': '23.2.6.1', 'UDM_Version': '23.2.6.1', 'UDR_Version': '23.2.6.1'}</msg>
<status status="PASS" starttime="20231107 17:34:49.067" endtime="20231107 17:35:06.985"/>
</kw>
<kw name="Catenate" library="BuiltIn">
<var>${jsonfilename}</var>
<arg>${EXECDIR}/${Env}VersionsOutput.json</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:35:06.985" level="INFO">${jsonfilename} = /home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qaVersionsOutput.json</msg>
<status status="PASS" starttime="20231107 17:35:06.985" endtime="20231107 17:35:06.985"/>
</kw>
<kw name="Convert From Txt To JSON" library="resource">
<arg>${Env}</arg>
<arg>${SUT_IP}</arg>
<arg>${User_Name}</arg>
<arg>${clilogfile}</arg>
<arg>${jsonfilename}</arg>
<arg>${Core_data}</arg>
<doc>This Keyword is using for Parse data and convert from txt to JSON. Create JSON file with name "VersionsOutput.JSON" in current directory.
It is a Userdefined Keyword.
It takes Five argument 1.Env Name., 2.HostName, 3.UserName, 4.Input Log File Path, 5.Output Json file Path</doc>
<kw name="Catenate" library="BuiltIn">
<var>${LogFilePathAfterRemovingExtraData}</var>
<arg>${EXECDIR}/${EnvName}FilteredCliLog.txt</arg>
<doc>Catenates the given items together and returns the resulted string.</doc>
<msg timestamp="20231107 17:35:06.986" level="INFO">${LogFilePathAfterRemovingExtraData} = /home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/qaFilteredCliLog.txt</msg>
<status status="PASS" starttime="20231107 17:35:06.986" endtime="20231107 17:35:06.986"/>
</kw>
<if>
<branch type="IF" condition="${coremoduledict} == {}">
<kw name="Log To Console" library="BuiltIn">
<arg>Actual data : ${coremoduledict} No data found from "Kubectl describe pods" command</arg>
<doc>Logs the given message to the console.</doc>
<status status="NOT RUN" starttime="20231107 17:35:06.986" endtime="20231107 17:35:06.986"/>
</kw>
<kw name="Fail" library="BuiltIn">
<arg>*HTML*</arg>
<arg>Actual data : ${coremoduledict} No data found from "Kubectl describe pods" command</arg>
<doc>Fails the test with the given message and optionally alters its tags.</doc>
<status status="NOT RUN" starttime="20231107 17:35:06.986" endtime="20231107 17:35:06.986"/>
</kw>
<status status="NOT RUN" starttime="20231107 17:35:06.986" endtime="20231107 17:35:06.986"/>
</branch>
<branch type="ELSE">
<kw name="Parse Injson" library="ParseInJsonV1">
<var>${PassFailStatus}</var>
<arg>${EnvName}</arg>
<arg>${HostName}</arg>
<arg>${UserName}</arg>
<arg>${config_file_path}</arg>
<arg>${input_text_file_path}</arg>
<arg>${coremoduledict}</arg>
<arg>${LogFilePathAfterRemovingExtraData}</arg>
<arg>${output_Json_file_path}</arg>
<msg timestamp="20231107 17:35:06.989" level="INFO">Config file path:  /home/amantya/Videos/athena/Version_Comparison/ModulesVersionCompForEnvs/config.yaml
['CLI_TOOL', '10.30.249.96', '23.08.16.1', 'Command', 'Line', 'Interface', 'Tool']
['UI_SERVICE', '10.30.249.96:9025', '23.08.28.1', 'UI', 'Service', 'for', 'Netprizm']
['CCL_SERVICE', 'localhost:9020', '23.08.21', 'CCL', 'Service']
['LMM_SERVICE', 'localhost:8081', '2.6.0', 'LMM', 'Service']
['UEM_SERVICE', 'localhost:9026', '23.07.11', 'UE', 'Mapper', 'service', 'for', 'Netprizm']
['NAF_SERVICE', 'localhost:9019', '23.09.11', 'NAF', 'Service', 'for', 'Netprizm']
['EM_SERVICE', 'localhost:9028', '23.08.28.1', 'Execution', 'Manager', 'Service', 'for', 'Netprizm']
['NAF_SERVICE', 'localhost:9019', '23.09.11', 'NAF', 'Service', 'for', 'Netprizm']
['SD_SERVICE', '10.30.249.96:9027', '23.08.23', 'Simulation', 'Data', 'service', 'for', 'Netprizm']
['AM_SERVICE', 'ab-ams.vzsme.com:9092', '23.07.05', 'Account', 'Management', 'Service', 'for', 'Netprizm']
['1', 'CU_01', '10.30.249.86:9022', '23.08.15', '02.05.13']
['2', 'PUBSUB_BROKER', '10.30.249.96:9033', '23.08.13', 'v23.7.2.1']
['3', 'DU_11_RANP_PUBS…', '10.30.249.87:9023', '23.08.13', 'v23.7.2.1']
['4', 'DU_12_RANP_PUBS…', '10.30.249.88:9023', '23.08.13', 'v23.7.2.1']
['5', 'CHV_PUBSUB', '10.30.249.96:9031', '23.08.13', 'v23.7.2.1']
['6', 'UE_01_UEMP_PUBS…', '10.30.249.91:9023', '23.08.13', 'v23.7.2.1']
['7', 'UE_02_UEMP_PUBS…', '10.30.249.92:9023', '23.08.13', 'v23.7.2.1']
['8', 'UE_03_UEMP_PUBS…', '10.30.249.93:9023', '23.08.13', 'v23.7.2.1']
['9', 'UE_04_UEMP_PUBS…', '10.30.249.94:9023', '23.08.13', 'v23.7.2.1']
['10', 'UE_05_UEMP_PUBS…', '10.30.249.89:9023', '23.08.13', 'v23.7.2.1']
['11', 'UE_06_UEMP_PUBS…', '10.30.249.90:9023', '23.08.13', 'v23.7.2.1']
['12', 'UE_01_UEMP_CSM', '10.30.249.91:9023', '23.08.13', 'v23.7.2.1']
['13', 'UE_02_UEMP_CSM', '10.30.249.92:9023', '23.08.13', 'v23.7.2.1']
['14', 'UE_03_UEMP_CSM', '10.30.249.93:9023', '23.08.13', 'v23.7.2.1']
['15', 'UE_04_UEMP_CSM', '10.30.249.94:9023', '23.08.13', 'v23.7.2.1']
['16', 'UE_05_UEMP_CSM', '10.30.249.89:9023', '23.08.13', 'v23.7.2.1']
['17', 'UE_06_UEMP_CSM', '10.30.249.90:9023', '23.08.13', 'v23.7.2.1']
['18', 'CHV_CSM', '10.30.249.96:9031', '23.08.13', 'v23.7.2.1']
['19', 'DU_11_RANP_CSM', '10.30.249.87:9023', '23.08.13', 'v23.7.2.1']
['20', 'DU_12_RANP_CSM', '10.30.249.88:9023', '23.08.13', 'v23.7.2.1']
['21', 'CHV', '10.30.249.96:9031', '23.08.13', 'RF_23.8.1.1']
['22', 'DU_11_RANP', '10.30.249.87:9023', '23.08.13', 'v23.7.2.1']
['23', 'DU_12_RANP', '10.30.249.88:9023', '23.08.13', 'v23.7.2.1']
['24', 'UE_01_UEMP', '10.30.249.91:9023', '23.08.13', 'v23.7.2.1']
['25', 'UE_02_UEMP', '10.30.249.92:9023', '23.08.13', 'v23.7.2.1']
['26', 'UE_03_UEMP', '10.30.249.93:9023', '23.08.13', 'v23.7.2.1']
['27', 'UE_04_UEMP', '10.30.249.94:9023', '23.08.13', 'v23.7.2.1']
['28', 'UE_05_UEMP', '10.30.249.89:9023', '23.08.13', 'v23.7.2.1']
['29', 'UE_06_UEMP', '10.30.249.90:9023', '23.08.13', 'v23.7.2.1']
['30', 'DU_11', '10.30.249.87:9021', '23.08.15.1', '02.05.13']
['31', 'DU_12', '10.30.249.88:9021', '23.08.15.1', '02.05.13']
['32', 'UE_01', '10.30.249.91:9021', '23.08.28', '03.01.05.01']
['33', 'UE_02', '10.30.249.92:9021', '23.08.28', '03.01.05.01']
['34', 'UE_03', '10.30.249.93:9021', '23.08.28', '03.01.05.01']
['35', 'UE_04', '10.30.249.94:9021', '23.08.28', '03.01.05.01']
['36', 'UE_05', '10.30.249.89:9021', '23.08.28', '03.01.05.01']
['37', 'UE_06', '10.30.249.90:9021', '23.08.28', '03.01.05.01']
['38', 'VATEST', '10.30.249.96:9031', '23.08.13', 'RF_23.8.1.1']
{
  "modules": {
    "Cores": [
      {
        "CORE_1_Amantya": {
          "OBR_HOST": "10.30.249.221",
          "OBR_USERNAME": "vzqa",
          "AMF_Version": "23.2.6.1",
          "AUSF_Version": "23.2.6.1",
          "NRF_Version": "23.2.6.1",
          "PCF_Version": "23.2.6.1",
          "SMF_Version": "23.2.6.1",
          "UDM_Version": "23.2.6.1",
          "UDR_Version": "23.2.6.1"
        }
      }
    ],
    "CLIs": [
      {
        "qa": {
          "OBR_HOST": "10.30.249.96",
          "OBR_USERNAME": "vzqa",
          "CLI_TOOL": {
            "app_version": "23.08.16.1"
          },
          "UI_SERVICE": {
            "app_version": "23.08.28.1"
          },
          "CCL_SERVICE": {
            "app_version": "23.08.21"
          },
          "LMM_SERVICE": {
            "app_version": "2.6.0"
          },
          "UEM_SERVICE": {
            "app_version": "23.07.11"
          },
          "NAF_SERVICE": {
            "app_version": "23.09.11"
          },
          "EM_SERVICE": {
            "app_version": "23.08.28.1"
          },
          "SD_SERVICE": {
            "app_version": "23.08.23"
          },
          "AM_SERVICE": {
            "app_version": "23.07.05"
          },
          "CU_01": {
            "OBR_HOST": "10.30.249.86",
            "app_version": "02.05.13",
            "ctlr_version": "23.08.15"
          },
          "PUBSUB_BROKER": {
            "OBR_HOST": "10.30.249.96",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "DU_11_RANP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.87",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "DU_12_RANP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.88",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "CHV_PUBSUB": {
            "OBR_HOST": "10.30.249.96",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_01_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.91",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_02_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.92",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_03_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.93",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_04_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.94",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_05_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.89",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_06_UEMP_PUBS\u2026": {
            "OBR_HOST": "10.30.249.90",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_01_UEMP_CSM": {
            "OBR_HOST": "10.30.249.91",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_02_UEMP_CSM": {
            "OBR_HOST": "10.30.249.92",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_03_UEMP_CSM": {
            "OBR_HOST": "10.30.249.93",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_04_UEMP_CSM": {
            "OBR_HOST": "10.30.249.94",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_05_UEMP_CSM": {
            "OBR_HOST": "10.30.249.89",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_06_UEMP_CSM": {
            "OBR_HOST": "10.30.249.90",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "CHV_CSM": {
            "OBR_HOST": "10.30.249.96",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "DU_11_RANP_CSM": {
            "OBR_HOST": "10.30.249.87",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "DU_12_RANP_CSM": {
            "OBR_HOST": "10.30.249.88",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "CHV": {
            "OBR_HOST": "10.30.249.96",
            "app_version": "RF_23.8.1.1",
            "ctlr_version": "23.08.13"
          },
          "DU_11_RANP": {
            "OBR_HOST": "10.30.249.87",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "DU_12_RANP": {
            "OBR_HOST": "10.30.249.88",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_01_UEMP": {
            "OBR_HOST": "10.30.249.91",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_02_UEMP": {
            "OBR_HOST": "10.30.249.92",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_03_UEMP": {
            "OBR_HOST": "10.30.249.93",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_04_UEMP": {
            "OBR_HOST": "10.30.249.94",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_05_UEMP": {
            "OBR_HOST": "10.30.249.89",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "UE_06_UEMP": {
            "OBR_HOST": "10.30.249.90",
            "app_version": "v23.7.2.1",
            "ctlr_version": "23.08.13"
          },
          "DU_11": {
            "OBR_HOST": "10.30.249.87",
            "app_version": "02.05.13",
            "ctlr_version": "23.08.15.1"
          },
          "DU_12": {
            "OBR_HOST": "10.30.249.88",
            "app_version": "02.05.13",
            "ctlr_version": "23.08.15.1"
          },
          "UE_01": {
            "OBR_HOST": "10.30.249.91",
            "app_version": "03.01.05.01",
            "ctlr_version": "23.08.28"
          },
          "UE_02": {
            "OBR_HOST": "10.30.249.92",
            "app_version": "03.01.05.01",
            "ctlr_version": "23.08.28"
          },
          "UE_03": {
            "OBR_HOST": "10.30.249.93",
            "app_version": "03.01.05.01",
            "ctlr_version": "23.08.28"
          },
          "UE_04": {
            "OBR_HOST": "10.30.249.94",
            "app_version": "03.01.05.01",
            "ctlr_version": "23.08.28"
          },
          "UE_05": {
            "OBR_HOST": "10.30.249.89",
            "app_version": "03.01.05.01",
            "ctlr_version": "23.08.28"
          },
          "UE_06": {
            "OBR_HOST": "10.30.249.90",
            "app_version": "03.01.05.01",
            "ctlr_version": "23.08.28"
          },
          "VATEST": {
            "OBR_HOST": "10.30.249.96",
            "app_version": "RF_23.8.1.1",
            "ctlr_version": "23.08.13"
          }
        }
      }
    ]
  }
}</msg>
<msg timestamp="20231107 17:35:06.989" level="INFO">${PassFailStatus} = 1</msg>
<status status="PASS" starttime="20231107 17:35:06.986" endtime="20231107 17:35:06.989"/>
</kw>
<kw name="Run Keyword And Return Status" library="BuiltIn">
<var>${ValidatePassFailStatus}</var>
<arg>Should Contain</arg>
<arg>${PassFailStatus}</arg>
<arg>1</arg>
<doc>Runs the given keyword with given arguments and returns the status as a Boolean value.</doc>
<kw name="Should Contain" library="BuiltIn">
<arg>${PassFailStatus}</arg>
<arg>1</arg>
<doc>Fails if ``container`` does not contain ``item`` one or more times.</doc>
<status status="PASS" starttime="20231107 17:35:06.990" endtime="20231107 17:35:06.990"/>
</kw>
<msg timestamp="20231107 17:35:06.990" level="INFO">${ValidatePassFailStatus} = True</msg>
<status status="PASS" starttime="20231107 17:35:06.989" endtime="20231107 17:35:06.990"/>
</kw>
<if>
<branch type="IF" condition="${ValidatePassFailStatus}==False">
<kw name="Fail" library="BuiltIn">
<arg>*HTML*${PassFailStatus}</arg>
<doc>Fails the test with the given message and optionally alters its tags.</doc>
<status status="NOT RUN" starttime="20231107 17:35:06.990" endtime="20231107 17:35:06.990"/>
</kw>
<status status="NOT RUN" starttime="20231107 17:35:06.990" endtime="20231107 17:35:06.990"/>
</branch>
<status status="PASS" starttime="20231107 17:35:06.990" endtime="20231107 17:35:06.990"/>
</if>
<status status="PASS" starttime="20231107 17:35:06.986" endtime="20231107 17:35:06.990"/>
</branch>
<status status="PASS" starttime="20231107 17:35:06.986" endtime="20231107 17:35:06.990"/>
</if>
<status status="PASS" starttime="20231107 17:35:06.985" endtime="20231107 17:35:06.990"/>
</kw>
<kw name="Append To List" library="Collections">
<arg>${jsonfilenamelist}</arg>
<arg>${jsonfilename}</arg>
<doc>Adds ``values`` to the end of ``list``.</doc>
<status status="PASS" starttime="20231107 17:35:06.990" endtime="20231107 17:35:06.990"/>
</kw>
<status status="PASS" starttime="20231107 17:34:29.383" endtime="20231107 17:35:06.990"/>
</iter>
<status status="PASS" starttime="20231107 17:33:45.216" endtime="20231107 17:35:06.990"/>
</for>
<kw name="Log To Console" library="BuiltIn">
<arg>@{jsonfilenamelist}</arg>
<doc>Logs the given message to the console.</doc>
<status status="PASS" starttime="20231107 17:35:06.990" endtime="20231107 17:35:06.990"/>
</kw>
<kw name="Getdatafromjsonfile" library="setup">
<arg>${jsonfilenamelist}</arg>
<msg timestamp="20231107 17:35:06.999" level="INFO">NRF_Version 23.2.6.1
NRF_Version 23.2.6.1
new data :  ['NRF_Version', '23.2.6.1', '23.2.6.1']
SMF_Version 23.2.6.1
SMF_Version 23.2.6.1
new data :  ['SMF_Version', '23.2.6.1', '23.2.6.1']
UDR_Version 23.2.6.1
UDR_Version 23.2.6.1
new data :  ['UDR_Version', '23.2.6.1', '23.2.6.1']
PCF_Version 23.2.6.1
PCF_Version 23.2.6.1
new data :  ['PCF_Version', '23.2.6.1', '23.2.6.1']
AMF_Version 23.2.6.1
AMF_Version 23.2.6.1
new data :  ['AMF_Version', '23.2.6.1', '23.2.6.1']
UDM_Version 23.2.6.1
UDM_Version 23.2.6.1
new data :  ['UDM_Version', '23.2.6.1', '23.2.6.1']
AUSF_Version 23.2.6.1
AUSF_Version 23.2.6.1
new data :  ['AUSF_Version', '23.2.6.1', '23.2.6.1']
UE_14_UEMP_CSM qa not found
UE_14_UEMP qa not found
DU_11_RANP_PUBS… qa2 not found
DU_12_RANP_PUBS… qa2 not found
UE_13_UEMP qa not found
UE_09_UEMP_CSM qa not found
UE_10 qa not found
UE_13_UEMP_PUBS… qa not found
UE_16_UEMP_CSM qa not found
DU11_RANP_PUBSUB qa not found
UE_07_UEMP_CSM qa not found
UE_15_UEMP_PUBS… qa not found
UE_08_UEMP_PUBS… qa not found
UE_07_UEMP qa not found
UE_11 qa not found
UE_07 qa not found
UE_13 qa not found
UE_11_UEMP_CSM qa not found
UE_14 qa not found
UE_12 qa not found
UE_15_UEMP_CSM qa not found
UE_10_UEMP_PUBS… qa not found
UE_16_UEMP_PUBS… qa not found
UE_13_UEMP_CSM qa not found
UE_08_UEMP qa not found
UE_09_UEMP qa not found
UE_09_UEMP_PUBS… qa not found
UE_11_UEMP_PUBS… qa not found
UE_12_UEMP qa not found
UE_16_UEMP qa not found
UE_14_UEMP_PUBS… qa not found
UE_10_UEMP_CSM qa not found
UE_09 qa not found
UE_07_UEMP_PUBS… qa not found
UE_12_UEMP_CSM qa not found
UE_12_UEMP_PUBS… qa not found
UE_11_UEMP qa not found
UE_10_UEMP qa not found
DU12_RANP_PUBSUB qa not found
UE_16 qa not found
UE_08 qa not found
UE_15_UEMP qa not found
UE_15 qa not found
UE_08_UEMP_CSM qa not found

 Searching CHV in qa2 Environment
CHV  Found in qa2
data list []
Updated list :  ['CHV', 'RF_23.9.1.0']

 Searching CHV in qa Environment
CHV  Found in qa
data list ['CHV', 'RF_23.9.1.0']
Updated list :  ['CHV', 'RF_23.9.1.0', 'RF_23.8.1.1']

 Searching DU_11_RANP in qa2 Environment
DU_11_RANP  Found in qa2
data list []
Updated list :  ['DU_11_RANP', 'v23.9.1.1']

 Searching DU_11_RANP in qa Environment
DU_11_RANP  Found in qa
data list ['DU_11_RANP', 'v23.9.1.1']
Updated list :  ['DU_11_RANP', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_05_UEMP_CSM in qa2 Environment
UE_05_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_05_UEMP_CSM', 'v23.9.1.1']

 Searching UE_05_UEMP_CSM in qa Environment
UE_05_UEMP_CSM  Found in qa
data list ['UE_05_UEMP_CSM', 'v23.9.1.1']
Updated list :  ['UE_05_UEMP_CSM', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_14_UEMP_CSM in qa2 Environment
UE_14_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_14_UEMP_CSM', 'v23.9.1.1']

 Searching UE_14_UEMP_CSM in qa Environment
UE_14_UEMP_CSM Not Found in given env......
Updated list :  ['UE_14_UEMP_CSM', 'v23.9.1.1', 'module not found in qa']

 Searching UE_14_UEMP in qa2 Environment
UE_14_UEMP  Found in qa2
data list []
Updated list :  ['UE_14_UEMP', 'v23.9.1.1']

 Searching UE_14_UEMP in qa Environment
UE_14_UEMP Not Found in given env......
Updated list :  ['UE_14_UEMP', 'v23.9.1.1', 'module not found in qa']

 Searching EM_SERVICE in qa2 Environment
EM_SERVICE  Found in qa2
data list []
Updated list :  ['EM_SERVICE', '23.09.29.2']

 Searching EM_SERVICE in qa Environment
EM_SERVICE  Found in qa
data list ['EM_SERVICE', '23.09.29.2']
Updated list :  ['EM_SERVICE', '23.09.29.2', '23.08.28.1']

 Searching DU_11_RANP_PUBS… in qa2 Environment
DU_11_RANP_PUBS… Not Found in given env......
Updated list :  ['DU_11_RANP_PUBS…', 'module not found in qa2']

 Searching DU_11_RANP_PUBS… in qa Environment
DU_11_RANP_PUBS…  Found in qa
data list ['DU_11_RANP_PUBS…', 'module not found in qa2']
Updated list :  ['DU_11_RANP_PUBS…', 'module not found in qa2', 'v23.7.2.1']

 Searching UE_06 in qa2 Environment
UE_06  Found in qa2
data list []
Updated list :  ['UE_06', '04.01.02']

 Searching UE_06 in qa Environment
UE_06  Found in qa
data list ['UE_06', '04.01.02']
Updated list :  ['UE_06', '04.01.02', '03.01.05.01']

 Searching UE_05_UEMP_PUBS… in qa2 Environment
UE_05_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_05_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_05_UEMP_PUBS… in qa Environment
UE_05_UEMP_PUBS…  Found in qa
data list ['UE_05_UEMP_PUBS…', 'v23.9.1.1']
Updated list :  ['UE_05_UEMP_PUBS…', 'v23.9.1.1', 'v23.7.2.1']

 Searching DU_11 in qa2 Environment
DU_11  Found in qa2
data list []
Updated list :  ['DU_11', '02.05.13']

 Searching DU_11 in qa Environment
DU_11  Found in qa
data list ['DU_11', '02.05.13']
Updated list :  ['DU_11', '02.05.13', '02.05.13']

 Searching DU_12_RANP_PUBS… in qa2 Environment
DU_12_RANP_PUBS… Not Found in given env......
Updated list :  ['DU_12_RANP_PUBS…', 'module not found in qa2']

 Searching DU_12_RANP_PUBS… in qa Environment
DU_12_RANP_PUBS…  Found in qa
data list ['DU_12_RANP_PUBS…', 'module not found in qa2']
Updated list :  ['DU_12_RANP_PUBS…', 'module not found in qa2', 'v23.7.2.1']

 Searching UE_13_UEMP in qa2 Environment
UE_13_UEMP  Found in qa2
data list []
Updated list :  ['UE_13_UEMP', 'v23.9.1.1']

 Searching UE_13_UEMP in qa Environment
UE_13_UEMP Not Found in given env......
Updated list :  ['UE_13_UEMP', 'v23.9.1.1', 'module not found in qa']

 Searching VATEST in qa2 Environment
VATEST  Found in qa2
data list []
Updated list :  ['VATEST', 'RF_23.9.1.0']

 Searching VATEST in qa Environment
VATEST  Found in qa
data list ['VATEST', 'RF_23.9.1.0']
Updated list :  ['VATEST', 'RF_23.9.1.0', 'RF_23.8.1.1']

 Searching AM_SERVICE in qa2 Environment
AM_SERVICE  Found in qa2
data list []
Updated list :  ['AM_SERVICE', '23.07.05']

 Searching AM_SERVICE in qa Environment
AM_SERVICE  Found in qa
data list ['AM_SERVICE', '23.07.05']
Updated list :  ['AM_SERVICE', '23.07.05', '23.07.05']

 Searching UE_04_UEMP in qa2 Environment
UE_04_UEMP  Found in qa2
data list []
Updated list :  ['UE_04_UEMP', 'v23.9.1.1']

 Searching UE_04_UEMP in qa Environment
UE_04_UEMP  Found in qa
data list ['UE_04_UEMP', 'v23.9.1.1']
Updated list :  ['UE_04_UEMP', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_06_UEMP_PUBS… in qa2 Environment
UE_06_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_06_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_06_UEMP_PUBS… in qa Environment
UE_06_UEMP_PUBS…  Found in qa
data list ['UE_06_UEMP_PUBS…', 'v23.9.1.1']
Updated list :  ['UE_06_UEMP_PUBS…', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_09_UEMP_CSM in qa2 Environment
UE_09_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_09_UEMP_CSM', 'v23.9.1.1']

 Searching UE_09_UEMP_CSM in qa Environment
UE_09_UEMP_CSM Not Found in given env......
Updated list :  ['UE_09_UEMP_CSM', 'v23.9.1.1', 'module not found in qa']

 Searching UE_10 in qa2 Environment
UE_10  Found in qa2
data list []
Updated list :  ['UE_10', '04.01.03']

 Searching UE_10 in qa Environment
UE_10 Not Found in given env......
Updated list :  ['UE_10', '04.01.03', 'module not found in qa']

 Searching DU_12_RANP in qa2 Environment
DU_12_RANP  Found in qa2
data list []
Updated list :  ['DU_12_RANP', 'v23.9.1.1']

 Searching DU_12_RANP in qa Environment
DU_12_RANP  Found in qa
data list ['DU_12_RANP', 'v23.9.1.1']
Updated list :  ['DU_12_RANP', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_05 in qa2 Environment
UE_05  Found in qa2
data list []
Updated list :  ['UE_05', '04.01.02']

 Searching UE_05 in qa Environment
UE_05  Found in qa
data list ['UE_05', '04.01.02']
Updated list :  ['UE_05', '04.01.02', '03.01.05.01']

 Searching UE_13_UEMP_PUBS… in qa2 Environment
UE_13_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_13_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_13_UEMP_PUBS… in qa Environment
UE_13_UEMP_PUBS… Not Found in given env......
Updated list :  ['UE_13_UEMP_PUBS…', 'v23.9.1.1', 'module not found in qa']

 Searching UE_06_UEMP_CSM in qa2 Environment
UE_06_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_06_UEMP_CSM', 'v23.9.1.1']

 Searching UE_06_UEMP_CSM in qa Environment
UE_06_UEMP_CSM  Found in qa
data list ['UE_06_UEMP_CSM', 'v23.9.1.1']
Updated list :  ['UE_06_UEMP_CSM', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_05_UEMP in qa2 Environment
UE_05_UEMP  Found in qa2
data list []
Updated list :  ['UE_05_UEMP', 'v23.9.1.1']

 Searching UE_05_UEMP in qa Environment
UE_05_UEMP  Found in qa
data list ['UE_05_UEMP', 'v23.9.1.1']
Updated list :  ['UE_05_UEMP', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_16_UEMP_CSM in qa2 Environment
UE_16_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_16_UEMP_CSM', 'v23.9.1.1']

 Searching UE_16_UEMP_CSM in qa Environment
UE_16_UEMP_CSM Not Found in given env......
Updated list :  ['UE_16_UEMP_CSM', 'v23.9.1.1', 'module not found in qa']

 Searching DU_12_RANP_CSM in qa2 Environment
DU_12_RANP_CSM  Found in qa2
data list []
Updated list :  ['DU_12_RANP_CSM', 'v23.9.1.1']

 Searching DU_12_RANP_CSM in qa Environment
DU_12_RANP_CSM  Found in qa
data list ['DU_12_RANP_CSM', 'v23.9.1.1']
Updated list :  ['DU_12_RANP_CSM', 'v23.9.1.1', 'v23.7.2.1']

 Searching PUBSUB_BROKER in qa2 Environment
PUBSUB_BROKER  Found in qa2
data list []
Updated list :  ['PUBSUB_BROKER', 'v23.9.1.1']

 Searching PUBSUB_BROKER in qa Environment
PUBSUB_BROKER  Found in qa
data list ['PUBSUB_BROKER', 'v23.9.1.1']
Updated list :  ['PUBSUB_BROKER', 'v23.9.1.1', 'v23.7.2.1']

 Searching UI_SERVICE in qa2 Environment
UI_SERVICE  Found in qa2
data list []
Updated list :  ['UI_SERVICE', '23.09.27.2']

 Searching UI_SERVICE in qa Environment
UI_SERVICE  Found in qa
data list ['UI_SERVICE', '23.09.27.2']
Updated list :  ['UI_SERVICE', '23.09.27.2', '23.08.28.1']

 Searching DU11_RANP_PUBSUB in qa2 Environment
DU11_RANP_PUBSUB  Found in qa2
data list []
Updated list :  ['DU11_RANP_PUBSUB', 'v23.9.1.1']

 Searching DU11_RANP_PUBSUB in qa Environment
DU11_RANP_PUBSUB Not Found in given env......
Updated list :  ['DU11_RANP_PUBSUB', 'v23.9.1.1', 'module not found in qa']

 Searching CHV_PUBSUB in qa2 Environment
CHV_PUBSUB  Found in qa2
data list []
Updated list :  ['CHV_PUBSUB', 'v23.9.1.1']

 Searching CHV_PUBSUB in qa Environment
CHV_PUBSUB  Found in qa
data list ['CHV_PUBSUB', 'v23.9.1.1']
Updated list :  ['CHV_PUBSUB', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_02 in qa2 Environment
UE_02  Found in qa2
data list []
Updated list :  ['UE_02', '04.01.02']

 Searching UE_02 in qa Environment
UE_02  Found in qa
data list ['UE_02', '04.01.02']
Updated list :  ['UE_02', '04.01.02', '03.01.05.01']

 Searching UE_07_UEMP_CSM in qa2 Environment
UE_07_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_07_UEMP_CSM', 'v23.9.1.1']

 Searching UE_07_UEMP_CSM in qa Environment
UE_07_UEMP_CSM Not Found in given env......
Updated list :  ['UE_07_UEMP_CSM', 'v23.9.1.1', 'module not found in qa']

 Searching UE_15_UEMP_PUBS… in qa2 Environment
UE_15_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_15_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_15_UEMP_PUBS… in qa Environment
UE_15_UEMP_PUBS… Not Found in given env......
Updated list :  ['UE_15_UEMP_PUBS…', 'v23.9.1.1', 'module not found in qa']

 Searching UE_04 in qa2 Environment
UE_04  Found in qa2
data list []
Updated list :  ['UE_04', '04.01.02']

 Searching UE_04 in qa Environment
UE_04  Found in qa
data list ['UE_04', '04.01.02']
Updated list :  ['UE_04', '04.01.02', '03.01.05.01']

 Searching UE_08_UEMP_PUBS… in qa2 Environment
UE_08_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_08_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_08_UEMP_PUBS… in qa Environment
UE_08_UEMP_PUBS… Not Found in given env......
Updated list :  ['UE_08_UEMP_PUBS…', 'v23.9.1.1', 'module not found in qa']

 Searching UE_03 in qa2 Environment
UE_03  Found in qa2
data list []
Updated list :  ['UE_03', '04.01.02']

 Searching UE_03 in qa Environment
UE_03  Found in qa
data list ['UE_03', '04.01.02']
Updated list :  ['UE_03', '04.01.02', '03.01.05.01']

 Searching UE_01_UEMP_PUBS… in qa2 Environment
UE_01_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_01_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_01_UEMP_PUBS… in qa Environment
UE_01_UEMP_PUBS…  Found in qa
data list ['UE_01_UEMP_PUBS…', 'v23.9.1.1']
Updated list :  ['UE_01_UEMP_PUBS…', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_07_UEMP in qa2 Environment
UE_07_UEMP  Found in qa2
data list []
Updated list :  ['UE_07_UEMP', 'v23.9.1.1']

 Searching UE_07_UEMP in qa Environment
UE_07_UEMP Not Found in given env......
Updated list :  ['UE_07_UEMP', 'v23.9.1.1', 'module not found in qa']

 Searching UE_11 in qa2 Environment
UE_11  Found in qa2
data list []
Updated list :  ['UE_11', '04.01.02']

 Searching UE_11 in qa Environment
UE_11 Not Found in given env......
Updated list :  ['UE_11', '04.01.02', 'module not found in qa']

 Searching UE_07 in qa2 Environment
UE_07  Found in qa2
data list []
Updated list :  ['UE_07', '04.01.02']

 Searching UE_07 in qa Environment
UE_07 Not Found in given env......
Updated list :  ['UE_07', '04.01.02', 'module not found in qa']

 Searching UE_13 in qa2 Environment
UE_13  Found in qa2
data list []
Updated list :  ['UE_13', '04.01.02']

 Searching UE_13 in qa Environment
UE_13 Not Found in given env......
Updated list :  ['UE_13', '04.01.02', 'module not found in qa']

 Searching UE_04_UEMP_CSM in qa2 Environment
UE_04_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_04_UEMP_CSM', 'v23.9.1.1']

 Searching UE_04_UEMP_CSM in qa Environment
UE_04_UEMP_CSM  Found in qa
data list ['UE_04_UEMP_CSM', 'v23.9.1.1']
Updated list :  ['UE_04_UEMP_CSM', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_11_UEMP_CSM in qa2 Environment
UE_11_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_11_UEMP_CSM', 'v23.9.1.1']

 Searching UE_11_UEMP_CSM in qa Environment
UE_11_UEMP_CSM Not Found in given env......
Updated list :  ['UE_11_UEMP_CSM', 'v23.9.1.1', 'module not found in qa']

 Searching UE_03_UEMP_CSM in qa2 Environment
UE_03_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_03_UEMP_CSM', 'v23.9.1.1']

 Searching UE_03_UEMP_CSM in qa Environment
UE_03_UEMP_CSM  Found in qa
data list ['UE_03_UEMP_CSM', 'v23.9.1.1']
Updated list :  ['UE_03_UEMP_CSM', 'v23.9.1.1', 'v23.7.2.1']

 Searching NAF_SERVICE in qa2 Environment
NAF_SERVICE  Found in qa2
data list []
Updated list :  ['NAF_SERVICE', '23.09.29']

 Searching NAF_SERVICE in qa Environment
NAF_SERVICE  Found in qa
data list ['NAF_SERVICE', '23.09.29']
Updated list :  ['NAF_SERVICE', '23.09.29', '23.09.11']

 Searching UE_14 in qa2 Environment
UE_14  Found in qa2
data list []
Updated list :  ['UE_14', '04.01.02']

 Searching UE_14 in qa Environment
UE_14 Not Found in given env......
Updated list :  ['UE_14', '04.01.02', 'module not found in qa']

 Searching UE_12 in qa2 Environment
UE_12  Found in qa2
data list []
Updated list :  ['UE_12', '04.01.02']

 Searching UE_12 in qa Environment
UE_12 Not Found in given env......
Updated list :  ['UE_12', '04.01.02', 'module not found in qa']

 Searching CHV_CSM in qa2 Environment
CHV_CSM  Found in qa2
data list []
Updated list :  ['CHV_CSM', 'v23.9.1.1']

 Searching CHV_CSM in qa Environment
CHV_CSM  Found in qa
data list ['CHV_CSM', 'v23.9.1.1']
Updated list :  ['CHV_CSM', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_04_UEMP_PUBS… in qa2 Environment
UE_04_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_04_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_04_UEMP_PUBS… in qa Environment
UE_04_UEMP_PUBS…  Found in qa
data list ['UE_04_UEMP_PUBS…', 'v23.9.1.1']
Updated list :  ['UE_04_UEMP_PUBS…', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_15_UEMP_CSM in qa2 Environment
UE_15_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_15_UEMP_CSM', 'v23.9.1.1']

 Searching UE_15_UEMP_CSM in qa Environment
UE_15_UEMP_CSM Not Found in given env......
Updated list :  ['UE_15_UEMP_CSM', 'v23.9.1.1', 'module not found in qa']

 Searching UE_10_UEMP_PUBS… in qa2 Environment
UE_10_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_10_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_10_UEMP_PUBS… in qa Environment
UE_10_UEMP_PUBS… Not Found in given env......
Updated list :  ['UE_10_UEMP_PUBS…', 'v23.9.1.1', 'module not found in qa']

 Searching DU_12 in qa2 Environment
DU_12  Found in qa2
data list []
Updated list :  ['DU_12', '02.05.13']

 Searching DU_12 in qa Environment
DU_12  Found in qa
data list ['DU_12', '02.05.13']
Updated list :  ['DU_12', '02.05.13', '02.05.13']

 Searching UE_03_UEMP_PUBS… in qa2 Environment
UE_03_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_03_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_03_UEMP_PUBS… in qa Environment
UE_03_UEMP_PUBS…  Found in qa
data list ['UE_03_UEMP_PUBS…', 'v23.9.1.1']
Updated list :  ['UE_03_UEMP_PUBS…', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_16_UEMP_PUBS… in qa2 Environment
UE_16_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_16_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_16_UEMP_PUBS… in qa Environment
UE_16_UEMP_PUBS… Not Found in given env......
Updated list :  ['UE_16_UEMP_PUBS…', 'v23.9.1.1', 'module not found in qa']

 Searching UE_13_UEMP_CSM in qa2 Environment
UE_13_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_13_UEMP_CSM', 'v23.9.1.1']

 Searching UE_13_UEMP_CSM in qa Environment
UE_13_UEMP_CSM Not Found in given env......
Updated list :  ['UE_13_UEMP_CSM', 'v23.9.1.1', 'module not found in qa']

 Searching UE_08_UEMP in qa2 Environment
UE_08_UEMP  Found in qa2
data list []
Updated list :  ['UE_08_UEMP', 'v23.9.1.1']

 Searching UE_08_UEMP in qa Environment
UE_08_UEMP Not Found in given env......
Updated list :  ['UE_08_UEMP', 'v23.9.1.1', 'module not found in qa']

 Searching UE_09_UEMP in qa2 Environment
UE_09_UEMP  Found in qa2
data list []
Updated list :  ['UE_09_UEMP', 'v23.9.1.1']

 Searching UE_09_UEMP in qa Environment
UE_09_UEMP Not Found in given env......
Updated list :  ['UE_09_UEMP', 'v23.9.1.1', 'module not found in qa']

 Searching UE_09_UEMP_PUBS… in qa2 Environment
UE_09_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_09_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_09_UEMP_PUBS… in qa Environment
UE_09_UEMP_PUBS… Not Found in given env......
Updated list :  ['UE_09_UEMP_PUBS…', 'v23.9.1.1', 'module not found in qa']

 Searching CU_01 in qa2 Environment
CU_01  Found in qa2
data list []
Updated list :  ['CU_01', '02.05.13.1']

 Searching CU_01 in qa Environment
CU_01  Found in qa
data list ['CU_01', '02.05.13.1']
Updated list :  ['CU_01', '02.05.13.1', '02.05.13']

 Searching UE_11_UEMP_PUBS… in qa2 Environment
UE_11_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_11_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_11_UEMP_PUBS… in qa Environment
UE_11_UEMP_PUBS… Not Found in given env......
Updated list :  ['UE_11_UEMP_PUBS…', 'v23.9.1.1', 'module not found in qa']

 Searching CLI_TOOL in qa2 Environment
CLI_TOOL  Found in qa2
data list []
Updated list :  ['CLI_TOOL', '23.09.29.1']

 Searching CLI_TOOL in qa Environment
CLI_TOOL  Found in qa
data list ['CLI_TOOL', '23.09.29.1']
Updated list :  ['CLI_TOOL', '23.09.29.1', '23.08.16.1']

 Searching UE_12_UEMP in qa2 Environment
UE_12_UEMP  Found in qa2
data list []
Updated list :  ['UE_12_UEMP', 'v23.9.1.1']

 Searching UE_12_UEMP in qa Environment
UE_12_UEMP Not Found in given env......
Updated list :  ['UE_12_UEMP', 'v23.9.1.1', 'module not found in qa']

 Searching LMM_SERVICE in qa2 Environment
LMM_SERVICE  Found in qa2
data list []
Updated list :  ['LMM_SERVICE', '23.2.6.3']

 Searching LMM_SERVICE in qa Environment
LMM_SERVICE  Found in qa
data list ['LMM_SERVICE', '23.2.6.3']
Updated list :  ['LMM_SERVICE', '23.2.6.3', '2.6.0']

 Searching UEM_SERVICE in qa2 Environment
UEM_SERVICE  Found in qa2
data list []
Updated list :  ['UEM_SERVICE', '23.07.11']

 Searching UEM_SERVICE in qa Environment
UEM_SERVICE  Found in qa
data list ['UEM_SERVICE', '23.07.11']
Updated list :  ['UEM_SERVICE', '23.07.11', '23.07.11']

 Searching UE_16_UEMP in qa2 Environment
UE_16_UEMP  Found in qa2
data list []
Updated list :  ['UE_16_UEMP', 'v23.9.1.1']

 Searching UE_16_UEMP in qa Environment
UE_16_UEMP Not Found in given env......
Updated list :  ['UE_16_UEMP', 'v23.9.1.1', 'module not found in qa']

 Searching UE_14_UEMP_PUBS… in qa2 Environment
UE_14_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_14_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_14_UEMP_PUBS… in qa Environment
UE_14_UEMP_PUBS… Not Found in given env......
Updated list :  ['UE_14_UEMP_PUBS…', 'v23.9.1.1', 'module not found in qa']

 Searching UE_01_UEMP in qa2 Environment
UE_01_UEMP  Found in qa2
data list []
Updated list :  ['UE_01_UEMP', 'v23.9.1.1']

 Searching UE_01_UEMP in qa Environment
UE_01_UEMP  Found in qa
data list ['UE_01_UEMP', 'v23.9.1.1']
Updated list :  ['UE_01_UEMP', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_10_UEMP_CSM in qa2 Environment
UE_10_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_10_UEMP_CSM', 'v23.9.1.1']

 Searching UE_10_UEMP_CSM in qa Environment
UE_10_UEMP_CSM Not Found in given env......
Updated list :  ['UE_10_UEMP_CSM', 'v23.9.1.1', 'module not found in qa']

 Searching UE_09 in qa2 Environment
UE_09  Found in qa2
data list []
Updated list :  ['UE_09', '04.01.02']

 Searching UE_09 in qa Environment
UE_09 Not Found in given env......
Updated list :  ['UE_09', '04.01.02', 'module not found in qa']

 Searching CCL_SERVICE in qa2 Environment
CCL_SERVICE  Found in qa2
data list []
Updated list :  ['CCL_SERVICE', '23.09.20.2']

 Searching CCL_SERVICE in qa Environment
CCL_SERVICE  Found in qa
data list ['CCL_SERVICE', '23.09.20.2']
Updated list :  ['CCL_SERVICE', '23.09.20.2', '23.08.21']

 Searching DU_11_RANP_CSM in qa2 Environment
DU_11_RANP_CSM  Found in qa2
data list []
Updated list :  ['DU_11_RANP_CSM', 'v23.9.1.1']

 Searching DU_11_RANP_CSM in qa Environment
DU_11_RANP_CSM  Found in qa
data list ['DU_11_RANP_CSM', 'v23.9.1.1']
Updated list :  ['DU_11_RANP_CSM', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_02_UEMP_PUBS… in qa2 Environment
UE_02_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_02_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_02_UEMP_PUBS… in qa Environment
UE_02_UEMP_PUBS…  Found in qa
data list ['UE_02_UEMP_PUBS…', 'v23.9.1.1']
Updated list :  ['UE_02_UEMP_PUBS…', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_07_UEMP_PUBS… in qa2 Environment
UE_07_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_07_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_07_UEMP_PUBS… in qa Environment
UE_07_UEMP_PUBS… Not Found in given env......
Updated list :  ['UE_07_UEMP_PUBS…', 'v23.9.1.1', 'module not found in qa']

 Searching UE_12_UEMP_CSM in qa2 Environment
UE_12_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_12_UEMP_CSM', 'v23.9.1.1']

 Searching UE_12_UEMP_CSM in qa Environment
UE_12_UEMP_CSM Not Found in given env......
Updated list :  ['UE_12_UEMP_CSM', 'v23.9.1.1', 'module not found in qa']

 Searching UE_12_UEMP_PUBS… in qa2 Environment
UE_12_UEMP_PUBS…  Found in qa2
data list []
Updated list :  ['UE_12_UEMP_PUBS…', 'v23.9.1.1']

 Searching UE_12_UEMP_PUBS… in qa Environment
UE_12_UEMP_PUBS… Not Found in given env......
Updated list :  ['UE_12_UEMP_PUBS…', 'v23.9.1.1', 'module not found in qa']

 Searching UE_11_UEMP in qa2 Environment
UE_11_UEMP  Found in qa2
data list []
Updated list :  ['UE_11_UEMP', 'v23.9.1.1']

 Searching UE_11_UEMP in qa Environment
UE_11_UEMP Not Found in given env......
Updated list :  ['UE_11_UEMP', 'v23.9.1.1', 'module not found in qa']

 Searching UE_01_UEMP_CSM in qa2 Environment
UE_01_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_01_UEMP_CSM', 'v23.9.1.1']

 Searching UE_01_UEMP_CSM in qa Environment
UE_01_UEMP_CSM  Found in qa
data list ['UE_01_UEMP_CSM', 'v23.9.1.1']
Updated list :  ['UE_01_UEMP_CSM', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_10_UEMP in qa2 Environment
UE_10_UEMP  Found in qa2
data list []
Updated list :  ['UE_10_UEMP', 'v23.9.1.1']

 Searching UE_10_UEMP in qa Environment
UE_10_UEMP Not Found in given env......
Updated list :  ['UE_10_UEMP', 'v23.9.1.1', 'module not found in qa']

 Searching DU12_RANP_PUBSUB in qa2 Environment
DU12_RANP_PUBSUB  Found in qa2
data list []
Updated list :  ['DU12_RANP_PUBSUB', 'v23.9.1.1']

 Searching DU12_RANP_PUBSUB in qa Environment
DU12_RANP_PUBSUB Not Found in given env......
Updated list :  ['DU12_RANP_PUBSUB', 'v23.9.1.1', 'module not found in qa']

 Searching SD_SERVICE in qa2 Environment
SD_SERVICE  Found in qa2
data list []
Updated list :  ['SD_SERVICE', '23.09.21']

 Searching SD_SERVICE in qa Environment
SD_SERVICE  Found in qa
data list ['SD_SERVICE', '23.09.21']
Updated list :  ['SD_SERVICE', '23.09.21', '23.08.23']

 Searching UE_16 in qa2 Environment
UE_16  Found in qa2
data list []
Updated list :  ['UE_16', '04.01.02']

 Searching UE_16 in qa Environment
UE_16 Not Found in given env......
Updated list :  ['UE_16', '04.01.02', 'module not found in qa']

 Searching UE_03_UEMP in qa2 Environment
UE_03_UEMP  Found in qa2
data list []
Updated list :  ['UE_03_UEMP', 'v23.9.1.1']

 Searching UE_03_UEMP in qa Environment
UE_03_UEMP  Found in qa
data list ['UE_03_UEMP', 'v23.9.1.1']
Updated list :  ['UE_03_UEMP', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_06_UEMP in qa2 Environment
UE_06_UEMP  Found in qa2
data list []
Updated list :  ['UE_06_UEMP', 'v23.9.1.1']

 Searching UE_06_UEMP in qa Environment
UE_06_UEMP  Found in qa
data list ['UE_06_UEMP', 'v23.9.1.1']
Updated list :  ['UE_06_UEMP', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_08 in qa2 Environment
UE_08  Found in qa2
data list []
Updated list :  ['UE_08', '04.01.02']

 Searching UE_08 in qa Environment
UE_08 Not Found in given env......
Updated list :  ['UE_08', '04.01.02', 'module not found in qa']

 Searching UE_15_UEMP in qa2 Environment
UE_15_UEMP  Found in qa2
data list []
Updated list :  ['UE_15_UEMP', 'v23.9.1.1']

 Searching UE_15_UEMP in qa Environment
UE_15_UEMP Not Found in given env......
Updated list :  ['UE_15_UEMP', 'v23.9.1.1', 'module not found in qa']

 Searching UE_02_UEMP in qa2 Environment
UE_02_UEMP  Found in qa2
data list []
Updated list :  ['UE_02_UEMP', 'v23.9.1.1']

 Searching UE_02_UEMP in qa Environment
UE_02_UEMP  Found in qa
data list ['UE_02_UEMP', 'v23.9.1.1']
Updated list :  ['UE_02_UEMP', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_15 in qa2 Environment
UE_15  Found in qa2
data list []
Updated list :  ['UE_15', '04.01.02']

 Searching UE_15 in qa Environment
UE_15 Not Found in given env......
Updated list :  ['UE_15', '04.01.02', 'module not found in qa']

 Searching UE_02_UEMP_CSM in qa2 Environment
UE_02_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_02_UEMP_CSM', 'v23.9.1.1']

 Searching UE_02_UEMP_CSM in qa Environment
UE_02_UEMP_CSM  Found in qa
data list ['UE_02_UEMP_CSM', 'v23.9.1.1']
Updated list :  ['UE_02_UEMP_CSM', 'v23.9.1.1', 'v23.7.2.1']

 Searching UE_08_UEMP_CSM in qa2 Environment
UE_08_UEMP_CSM  Found in qa2
data list []
Updated list :  ['UE_08_UEMP_CSM', 'v23.9.1.1']

 Searching UE_08_UEMP_CSM in qa Environment
UE_08_UEMP_CSM Not Found in given env......
Updated list :  ['UE_08_UEMP_CSM', 'v23.9.1.1', 'module not found in qa']

 Searching UE_01 in qa2 Environment
UE_01  Found in qa2
data list []
Updated list :  ['UE_01', '04.01.02']

 Searching UE_01 in qa Environment
UE_01  Found in qa
data list ['UE_01', '04.01.02']
Updated list :  ['UE_01', '04.01.02', '03.01.05.01']
+------------------+----------------------+----------------------+
| Modules          | qa2                  | qa                   |
+------------------+----------------------+----------------------+
| NRF_Version      | 23.2.6.1             | 23.2.6.1             |
| SMF_Version      | 23.2.6.1             | 23.2.6.1             |
| UDR_Version      | 23.2.6.1             | 23.2.6.1             |
| PCF_Version      | 23.2.6.1             | 23.2.6.1             |
| AMF_Version      | 23.2.6.1             | 23.2.6.1             |
| UDM_Version      | 23.2.6.1             | 23.2.6.1             |
| AUSF_Version     | 23.2.6.1             | 23.2.6.1             |
| CHV              | RF_23.9.1.0          | RF_23.8.1.1          |
| DU_11_RANP       | v23.9.1.1            | v23.7.2.1            |
| UE_05_UEMP_CSM   | v23.9.1.1            | v23.7.2.1            |
| UE_14_UEMP_CSM   | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_14_UEMP       | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| EM_SERVICE       | 23.09.29.2           | 23.08.28.1           |
| DU_11_RANP_PUBS… | module not found in  | v23.7.2.1            |
|                  | qa2                  |                      |
| UE_06            | 04.01.02             | 03.01.05.01          |
| UE_05_UEMP_PUBS… | v23.9.1.1            | v23.7.2.1            |
| DU_11            | 02.05.13             | 02.05.13             |
| DU_12_RANP_PUBS… | module not found in  | v23.7.2.1            |
|                  | qa2                  |                      |
| UE_13_UEMP       | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| VATEST           | RF_23.9.1.0          | RF_23.8.1.1          |
| AM_SERVICE       | 23.07.05             | 23.07.05             |
| UE_04_UEMP       | v23.9.1.1            | v23.7.2.1            |
| UE_06_UEMP_PUBS… | v23.9.1.1            | v23.7.2.1            |
| UE_09_UEMP_CSM   | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_10            | 04.01.03             | module not found in  |
|                  |                      | qa                   |
| DU_12_RANP       | v23.9.1.1            | v23.7.2.1            |
| UE_05            | 04.01.02             | 03.01.05.01          |
| UE_13_UEMP_PUBS… | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_06_UEMP_CSM   | v23.9.1.1            | v23.7.2.1            |
| UE_05_UEMP       | v23.9.1.1            | v23.7.2.1            |
| UE_16_UEMP_CSM   | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| DU_12_RANP_CSM   | v23.9.1.1            | v23.7.2.1            |
| PUBSUB_BROKER    | v23.9.1.1            | v23.7.2.1            |
| UI_SERVICE       | 23.09.27.2           | 23.08.28.1           |
| DU11_RANP_PUBSUB | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| CHV_PUBSUB       | v23.9.1.1            | v23.7.2.1            |
| UE_02            | 04.01.02             | 03.01.05.01          |
| UE_07_UEMP_CSM   | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_15_UEMP_PUBS… | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_04            | 04.01.02             | 03.01.05.01          |
| UE_08_UEMP_PUBS… | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_03            | 04.01.02             | 03.01.05.01          |
| UE_01_UEMP_PUBS… | v23.9.1.1            | v23.7.2.1            |
| UE_07_UEMP       | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_11            | 04.01.02             | module not found in  |
|                  |                      | qa                   |
| UE_07            | 04.01.02             | module not found in  |
|                  |                      | qa                   |
| UE_13            | 04.01.02             | module not found in  |
|                  |                      | qa                   |
| UE_04_UEMP_CSM   | v23.9.1.1            | v23.7.2.1            |
| UE_11_UEMP_CSM   | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_03_UEMP_CSM   | v23.9.1.1            | v23.7.2.1            |
| NAF_SERVICE      | 23.09.29             | 23.09.11             |
| UE_14            | 04.01.02             | module not found in  |
|                  |                      | qa                   |
| UE_12            | 04.01.02             | module not found in  |
|                  |                      | qa                   |
| CHV_CSM          | v23.9.1.1            | v23.7.2.1            |
| UE_04_UEMP_PUBS… | v23.9.1.1            | v23.7.2.1            |
| UE_15_UEMP_CSM   | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_10_UEMP_PUBS… | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| DU_12            | 02.05.13             | 02.05.13             |
| UE_03_UEMP_PUBS… | v23.9.1.1            | v23.7.2.1            |
| UE_16_UEMP_PUBS… | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_13_UEMP_CSM   | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_08_UEMP       | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_09_UEMP       | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_09_UEMP_PUBS… | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| CU_01            | 02.05.13.1           | 02.05.13             |
| UE_11_UEMP_PUBS… | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| CLI_TOOL         | 23.09.29.1           | 23.08.16.1           |
| UE_12_UEMP       | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| LMM_SERVICE      | 23.2.6.3             | 2.6.0                |
| UEM_SERVICE      | 23.07.11             | 23.07.11             |
| UE_16_UEMP       | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_14_UEMP_PUBS… | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_01_UEMP       | v23.9.1.1            | v23.7.2.1            |
| UE_10_UEMP_CSM   | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_09            | 04.01.02             | module not found in  |
|                  |                      | qa                   |
| CCL_SERVICE      | 23.09.20.2           | 23.08.21             |
| DU_11_RANP_CSM   | v23.9.1.1            | v23.7.2.1            |
| UE_02_UEMP_PUBS… | v23.9.1.1            | v23.7.2.1            |
| UE_07_UEMP_PUBS… | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_12_UEMP_CSM   | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_12_UEMP_PUBS… | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_11_UEMP       | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_01_UEMP_CSM   | v23.9.1.1            | v23.7.2.1            |
| UE_10_UEMP       | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| DU12_RANP_PUBSUB | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| SD_SERVICE       | 23.09.21             | 23.08.23             |
| UE_16            | 04.01.02             | module not found in  |
|                  |                      | qa                   |
| UE_03_UEMP       | v23.9.1.1            | v23.7.2.1            |
| UE_06_UEMP       | v23.9.1.1            | v23.7.2.1            |
| UE_08            | 04.01.02             | module not found in  |
|                  |                      | qa                   |
| UE_15_UEMP       | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_02_UEMP       | v23.9.1.1            | v23.7.2.1            |
| UE_15            | 04.01.02             | module not found in  |
|                  |                      | qa                   |
| UE_02_UEMP_CSM   | v23.9.1.1            | v23.7.2.1            |
| UE_08_UEMP_CSM   | v23.9.1.1            | module not found in  |
|                  |                      | qa                   |
| UE_01            | 04.01.02             | 03.01.05.01          |
+------------------+----------------------+----------------------+</msg>
<status status="PASS" starttime="20231107 17:35:06.991" endtime="20231107 17:35:07.000"/>
</kw>
<status status="PASS" starttime="20231107 17:33:45.215" endtime="20231107 17:35:07.000"/>
</kw>
<status status="PASS" starttime="20231107 17:33:45.215" endtime="20231107 17:35:07.000"/>
</test>
<doc>Modules Version In Json Format</doc>
<status status="PASS" starttime="20231107 17:33:45.094" endtime="20231107 17:35:07.000"/>
</suite>
<statistics>
<total>
<stat pass="1" fail="0" skip="0">All Tests</stat>
</total>
<tag>
</tag>
<suite>
<stat pass="1" fail="0" skip="0" id="s1" name="VersionComparison">VersionComparison</stat>
</suite>
</statistics>
<errors>
</errors>
</robot>
